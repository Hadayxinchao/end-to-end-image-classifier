# Default Hyperparameters

# Optimizer settings
learning_rate: 0.001
weight_decay: 1e-4
momentum: 0.9
optimizer: adam  # adam or sgd

# Training settings
num_epochs: 50
batch_size: 64

# Learning rate scheduler
use_scheduler: true
scheduler_type: step  # step, cosine, or plateau
step_size: 10
gamma: 0.1
min_lr: 1e-6

# Regularization
dropout: 0.5
label_smoothing: 0.0

# Gradient clipping
clip_grad_norm: 1.0
