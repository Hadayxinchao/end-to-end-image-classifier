{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"End-to-End Image Classifier with MLOps","text":"<p>Welcome to the documentation for the End-to-End Image Classifier project! This project demonstrates MLOps best practices for building, training, and deploying image classification models.</p>"},{"location":"#project-overview","title":"\ud83c\udfaf Project Overview","text":"<p>This project implements a complete MLOps pipeline for image classification, demonstrating:</p> <ul> <li>Standardized Project Structure: Clean, maintainable codebase following industry standards</li> <li>Configuration Management: Flexible experiment configuration with Hydra</li> <li>Data Versioning: Track datasets with DVC</li> <li>Automated Testing: Comprehensive test suite with pytest</li> <li>CI/CD: Automated pipelines with GitHub Actions</li> <li>Continuous ML: Automated model training and reporting with CML</li> <li>Containerization: Docker support for reproducible deployments</li> <li>Documentation: Complete documentation with MkDocs</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Get started in 3 simple steps:</p> <pre><code># 1. Clone the repository\ngit clone https://github.com/Hadayxinchao/end-to-end-image-classifier.git\ncd end-to-end-image-classifier\n\n# 2. Install dependencies\npip install -r requirements.txt\n\n# 3. Train your first model\npython src/training/train.py\n</code></pre>"},{"location":"#key-features","title":"\ud83d\udcda Key Features","text":""},{"location":"#smart-configuration-management","title":"Smart Configuration Management","text":"<p>Use Hydra to manage all configurations without touching code:</p> <pre><code># Override learning rate\npython src/training/train.py hyperparameters.learning_rate=0.001\n\n# Use different model\npython src/training/train.py model=resnet\n\n# Switch dataset\npython src/training/train.py data=mnist\n</code></pre>"},{"location":"#data-version-control","title":"Data Version Control","text":"<p>Never lose track of your datasets:</p> <pre><code># Track data with DVC\ndvc add data/raw\n\n# Push to remote storage\ndvc push\n\n# Pull data from any commit\ngit checkout &lt;commit&gt;\ndvc pull\n</code></pre>"},{"location":"#automated-testing","title":"Automated Testing","text":"<p>Ensure code quality with comprehensive tests:</p> <pre><code># Run all tests\npytest tests/\n\n# Run with coverage\npytest --cov=src tests/\n\n# Run only fast tests\npytest -m \"not slow\"\n</code></pre>"},{"location":"#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>Every push triggers:</p> <ul> <li>\u2705 Code linting and formatting checks</li> <li>\u2705 Unit and integration tests</li> <li>\u2705 Model training with performance reporting</li> <li>\u2705 Automated deployment (optional)</li> </ul>"},{"location":"#docker-support","title":"Docker Support","text":"<p>Run anywhere with Docker:</p> <pre><code># Build image\ndocker build -t image-classifier .\n\n# Run training\ndocker run image-classifier\n\n# Run inference\ndocker run image-classifier python src/models/predict.py --help\n</code></pre>"},{"location":"#documentation-structure","title":"\ud83d\udcd6 Documentation Structure","text":"<ul> <li>Getting Started: Installation and setup</li> <li>User Guide: Learn how to use the project</li> <li>MLOps: CI/CD and automation details</li> <li>API Reference: Code documentation</li> <li>Development: Contributing guidelines</li> </ul>"},{"location":"#project-architecture","title":"\ud83c\udfd7\ufe0f Project Architecture","text":"<pre><code>graph TD\n    A[\"GitHub Repository\"] --&gt; B[\"Source Code\"]\n    A --&gt; C[\"Tests&lt;br/&gt;(pytest)\"]\n    A --&gt; D[\"Configs&lt;br/&gt;(Hydra)\"]\n    B --&gt; E[\"GitHub Actions&lt;br/&gt;(CI/CD)\"]\n    C --&gt; E\n    D --&gt; E\n    E --&gt; F[\"Lint &amp; Test\"]\n    E --&gt; G[\"Train Model\"]\n    E --&gt; H[\"Generate Reports\"]\n    F --&gt; I[\"DVC Remote&lt;br/&gt;(Data Storage)\"]\n    G --&gt; I\n    H --&gt; I\n    I --&gt; J[\"Docker Image&lt;br/&gt;(Deployment)\"]\n\n    style A fill:#4A90E2,stroke:#2E5C8A,color:#fff\n    style E fill:#7B68EE,stroke:#5A4B96,color:#fff\n    style I fill:#50C878,stroke:#3A8B56,color:#fff\n    style J fill:#FF6B6B,stroke:#C44444,color:#fff</code></pre>"},{"location":"#learning-resources","title":"\ud83c\udf93 Learning Resources","text":""},{"location":"#for-beginners","title":"For Beginners","text":"<ol> <li>Start with Installation</li> <li>Follow the Quick Start Guide</li> <li>Learn about Configuration</li> </ol>"},{"location":"#for-mlops-engineers","title":"For MLOps Engineers","text":"<ol> <li>Understand the CI/CD Pipeline</li> <li>Setup Data Versioning</li> <li>Learn Docker Deployment</li> </ol>"},{"location":"#for-contributors","title":"For Contributors","text":"<ol> <li>Read Contributing Guidelines</li> <li>Review Testing Practices</li> <li>Follow Code Style Guide</li> </ol>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for details.</p>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<ul> <li>Cookiecutter Data Science for project structure inspiration</li> <li>MLOps community for best practices</li> <li>PyTorch team for the deep learning framework</li> </ul>"},{"location":"#contact","title":"\ud83d\udce7 Contact","text":"<p>For questions or feedback:</p> <ul> <li>GitHub Issues: Report an issue</li> <li>Email: your.email@example.com</li> </ul> <p>Ready to get started? \u2192 Installation Guide</p>"},{"location":"about/license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2025 Hadayxinchao</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"about/license/#additional-licenses","title":"Additional Licenses","text":"<p>This project includes or depends on:</p>"},{"location":"about/license/#pytorch","title":"PyTorch","text":"<ul> <li>License: BSD</li> <li>Website: https://pytorch.org/</li> </ul>"},{"location":"about/license/#torchvision","title":"torchvision","text":"<ul> <li>License: BSD</li> <li>Website: https://github.com/pytorch/vision</li> </ul>"},{"location":"about/license/#hydra","title":"Hydra","text":"<ul> <li>License: Apache 2.0</li> <li>Website: https://hydra.cc/</li> </ul>"},{"location":"about/license/#dvc","title":"DVC","text":"<ul> <li>License: Apache 2.0</li> <li>Website: https://dvc.org/</li> </ul>"},{"location":"about/license/#pytest","title":"pytest","text":"<ul> <li>License: MIT</li> <li>Website: https://pytest.org/</li> </ul>"},{"location":"about/license/#mkdocs","title":"MkDocs","text":"<ul> <li>License: BSD</li> <li>Website: https://www.mkdocs.org/</li> </ul>"},{"location":"about/license/#material-for-mkdocs","title":"Material for MkDocs","text":"<ul> <li>License: MIT</li> <li>Website: https://squidfunk.github.io/mkdocs-material/</li> </ul>"},{"location":"about/license/#scikit-learn","title":"scikit-learn","text":"<ul> <li>License: BSD 3-Clause</li> <li>Website: https://scikit-learn.org/</li> </ul>"},{"location":"about/license/#numpy","title":"NumPy","text":"<ul> <li>License: BSD</li> <li>Website: https://numpy.org/</li> </ul>"},{"location":"about/license/#pandas","title":"Pandas","text":"<ul> <li>License: BSD 3-Clause</li> <li>Website: https://pandas.pydata.org/</li> </ul>"},{"location":"about/license/#matplotlib","title":"Matplotlib","text":"<ul> <li>License: PSF</li> <li>Website: https://matplotlib.org/</li> </ul>"},{"location":"about/license/#seaborn","title":"Seaborn","text":"<ul> <li>License: BSD 3-Clause</li> <li>Website: https://seaborn.pydata.org/</li> </ul> <p>See individual package licenses for more details.</p>"},{"location":"about/license/#third-party-resources","title":"Third-Party Resources","text":""},{"location":"about/license/#datasets","title":"Datasets","text":"<ul> <li>CIFAR-10: Published under CC BY 4.0</li> <li>MNIST: Published under CC BY 4.0</li> </ul>"},{"location":"about/license/#models","title":"Models","text":"<p>Built using published architectures and best practices from the deep learning community.</p>"},{"location":"about/license/#your-contributions","title":"Your Contributions","text":"<p>By contributing to this project, you agree that your contributions will be licensed under the same MIT License.</p> <p>For any questions about licensing, please open an issue on GitHub.</p>"},{"location":"about/overview/","title":"Project Overview","text":"<p>End-to-end image classification project following MLOps best practices.</p>"},{"location":"about/overview/#project-goals","title":"Project Goals","text":"<p>This project demonstrates a production-ready machine learning pipeline with:</p> <ol> <li>Reproducibility: Fixed seeds, versioned data, documented configurations</li> <li>Scalability: Modular architecture supporting multiple models and datasets</li> <li>Maintainability: Clean code, comprehensive tests, full documentation</li> <li>Automation: CI/CD pipelines, automated testing, model versioning</li> <li>Best Practices: DVC data versioning, Hydra config management, Docker containerization</li> </ol>"},{"location":"about/overview/#whats-included","title":"What's Included","text":""},{"location":"about/overview/#1-core-components","title":"1. Core Components","text":"<ul> <li>Data Management: Automatic CIFAR-10 &amp; MNIST loading with preprocessing</li> <li>Model Architectures: SimpleCNN and ResNet for image classification</li> <li>Training Pipeline: Configurable training with multiple optimizers and schedulers</li> <li>Inference: Batch and single-image prediction utilities</li> <li>Evaluation: Comprehensive metrics and visualization tools</li> </ul>"},{"location":"about/overview/#2-mlops-infrastructure","title":"2. MLOps Infrastructure","text":"<ul> <li>Configuration Management: Hydra for reproducible experiments</li> <li>Data Versioning: DVC integration for dataset tracking</li> <li>Unit Testing: 30+ pytest tests covering data, models, and training</li> <li>CI/CD: GitHub Actions workflows for automated testing</li> <li>Containerization: Docker setup for reproducible environments</li> <li>Documentation: MkDocs with comprehensive guides and API reference</li> </ul>"},{"location":"about/overview/#3-code-quality","title":"3. Code Quality","text":"<ul> <li>Code Formatting: Black for consistent style</li> <li>Import Sorting: isort for organized imports</li> <li>Linting: flake8 for code quality checks</li> <li>Type Checking: mypy for static type verification</li> </ul>"},{"location":"about/overview/#quick-start","title":"Quick Start","text":""},{"location":"about/overview/#installation","title":"Installation","text":"<pre><code># Clone repository\ngit clone &lt;repository-url&gt;\ncd end-to-end-image-classifier\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"about/overview/#training","title":"Training","text":"<pre><code># Train on CIFAR-10 (default)\npython src/training/train.py\n\n# Train on MNIST\npython src/training/train.py data=mnist\n\n# Custom configuration\npython src/training/train.py \\\n    hyperparameters.learning_rate=0.001 \\\n    hyperparameters.num_epochs=100\n</code></pre>"},{"location":"about/overview/#making-predictions","title":"Making Predictions","text":"<pre><code>from src.models.model import get_model\nfrom src.models.predict import predict_batch\nfrom src.data.make_dataset import load_cifar10\nimport torch\n\n# Load model and data\nmodel = get_model(\"simple_cnn\", num_classes=10, image_size=32)\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n_, _, test_loader = load_cifar10()\n\n# Make predictions\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\npredictions, labels = predict_batch(model, test_loader, device)\n</code></pre>"},{"location":"about/overview/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 src/                      # Source code\n\u2502   \u251c\u2500\u2500 data/                # Data loading and preprocessing\n\u2502   \u251c\u2500\u2500 models/              # Model architectures\n\u2502   \u251c\u2500\u2500 training/            # Training scripts\n\u2502   \u2514\u2500\u2500 utils/               # Utility functions\n\u251c\u2500\u2500 tests/                    # Unit and integration tests\n\u251c\u2500\u2500 configs/                  # Hydra configuration files\n\u251c\u2500\u2500 data/                     # Data directory (gitignored)\n\u251c\u2500\u2500 models/                   # Model checkpoints (gitignored)\n\u251c\u2500\u2500 reports/                  # Training reports and visualizations\n\u251c\u2500\u2500 docs/                     # Documentation (MkDocs)\n\u251c\u2500\u2500 .github/workflows/        # CI/CD pipelines\n\u251c\u2500\u2500 Dockerfile                # Container setup\n\u251c\u2500\u2500 requirements.txt          # Python dependencies\n\u251c\u2500\u2500 pyproject.toml            # Project configuration\n\u2514\u2500\u2500 mkdocs.yml               # Documentation configuration\n</code></pre>"},{"location":"about/overview/#technology-stack","title":"Technology Stack","text":"<ul> <li>Framework: PyTorch 2.0+</li> <li>Configuration: Hydra 1.3+</li> <li>Data Versioning: DVC 3.0+</li> <li>Testing: pytest 7.4+</li> <li>CI/CD: GitHub Actions</li> <li>Container: Docker</li> <li>Documentation: MkDocs Material</li> <li>Code Quality: Black, isort, flake8, mypy</li> </ul>"},{"location":"about/overview/#performance","title":"Performance","text":""},{"location":"about/overview/#model-accuracy","title":"Model Accuracy","text":"<ul> <li>CIFAR-10: ~85% (SimpleCNN), ~90%+ (ResNet)</li> <li>MNIST: ~98%+ (both architectures)</li> </ul>"},{"location":"about/overview/#training-time-cifar-10-simplecnn","title":"Training Time (CIFAR-10, SimpleCNN)","text":"<ul> <li>GPU: ~2-3 minutes for 50 epochs</li> <li>CPU: ~15-20 minutes for 50 epochs</li> </ul>"},{"location":"about/overview/#key-features","title":"Key Features","text":""},{"location":"about/overview/#hydra-configuration-management","title":"Hydra Configuration Management","text":"<p>Easily modify training via command line or YAML configs:</p> <pre><code>python src/training/train.py \\\n    data=cifar10 \\\n    model=resnet \\\n    hyperparameters.learning_rate=0.001 \\\n    hyperparameters.batch_size=64\n</code></pre>"},{"location":"about/overview/#reproducibility","title":"Reproducibility","text":"<ul> <li>Fixed random seeds for data and model</li> <li>Version control for code and configurations</li> <li>Data versioning with DVC</li> <li>Containerization for consistent environments</li> </ul>"},{"location":"about/overview/#modular-architecture","title":"Modular Architecture","text":"<ul> <li>Easy to add new models in <code>src/models/</code></li> <li>Simple to support new datasets in <code>src/data/</code></li> <li>Extensible training pipeline in <code>src/training/</code></li> </ul>"},{"location":"about/overview/#comprehensive-testing","title":"Comprehensive Testing","text":"<ul> <li>Data loading tests</li> <li>Model architecture tests</li> <li>Training pipeline tests</li> <li>90%+ code coverage</li> </ul>"},{"location":"about/overview/#continuous-integration","title":"Continuous Integration","text":"<p>Automated testing on every commit: - Multi-Python version support (3.9, 3.10, 3.11, 3.12, 3.13) - Linting and type checking - Unit test execution</p>"},{"location":"about/overview/#getting-started","title":"Getting Started","text":"<p>See Getting Started for installation instructions.</p>"},{"location":"about/overview/#documentation","title":"Documentation","text":"<p>Full documentation available at: - Installation &amp; Setup - Quick Start Guide - Configuration Guide - Training Guide - Inference Guide - API Reference</p>"},{"location":"about/overview/#contributing","title":"Contributing","text":"<p>We welcome contributions! See Contributing Guide for details.</p>"},{"location":"about/overview/#license","title":"License","text":"<p>This project is licensed under the MIT License. See License for details.</p>"},{"location":"about/overview/#authors","title":"Authors","text":"<ul> <li>Created as part of MLOps best practices demonstration</li> <li>Maintaining: Open to community contributions</li> </ul>"},{"location":"about/overview/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>PyTorch team for the excellent deep learning framework</li> <li>Hydra team for configuration management</li> <li>DVC team for data versioning</li> <li>GitHub for CI/CD infrastructure</li> </ul>"},{"location":"about/overview/#citation","title":"Citation","text":"<p>If you use this project in your research, please cite:</p> <pre><code>@misc{image-classifier-mlops,\n  title={End-to-End Image Classifier with MLOps},\n  author={Hoang Ha},\n  year={2025},\n  url={https://github.com/Hadayxinchao/end-to-end-image-classifier}\n}\n</code></pre>"},{"location":"about/overview/#contact","title":"Contact","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Email: [your-email]</li> </ul>"},{"location":"about/overview/#roadmap","title":"Roadmap","text":"<ul> <li> Add more model architectures (EfficientNet, Vision Transformer)</li> <li> Support additional datasets (ImageNet, COCO)</li> <li> MLflow integration for experiment tracking</li> <li> Model serving with FastAPI</li> <li> Batch inference optimization</li> <li> GPU optimization (mixed precision training)</li> <li> Model quantization</li> <li> Federated learning support</li> </ul>"},{"location":"about/overview/#related-resources","title":"Related Resources","text":"<ul> <li>PyTorch Documentation</li> <li>Hydra Documentation</li> <li>DVC Documentation</li> <li>MLOps Guide</li> <li>GitHub Actions Docs</li> </ul>"},{"location":"api/data/","title":"API Reference - Data Module","text":"<p>Auto-generated API documentation for the data module.</p>"},{"location":"api/data/#loading-datasets","title":"Loading Datasets","text":""},{"location":"api/data/#cifar-10-dataset","title":"CIFAR-10 Dataset","text":"<p>Load CIFAR-10 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Directory to store/load data</p> <code>'./data/raw'</code> <code>batch_size</code> <code>int</code> <p>Batch size for dataloaders</p> <code>64</code> <code>val_split</code> <code>float</code> <p>Fraction of training data to use for validation</p> <code>0.1</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading</p> <code>2</code> <p>Returns:</p> Type Description <code>Tuple[DataLoader, DataLoader, DataLoader]</code> <p>Tuple of (train_loader, val_loader, test_loader)</p> Source code in <code>src/data/make_dataset.py</code> <pre><code>def load_cifar10(data_dir: str = \"./data/raw\", batch_size: int = 64, \n                 val_split: float = 0.1, num_workers: int = 2) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"\n    Load CIFAR-10 dataset.\n\n    Args:\n        data_dir: Directory to store/load data\n        batch_size: Batch size for dataloaders\n        val_split: Fraction of training data to use for validation\n        num_workers: Number of workers for data loading\n\n    Returns:\n        Tuple of (train_loader, val_loader, test_loader)\n    \"\"\"\n    data_path = Path(data_dir)\n    data_path.mkdir(parents=True, exist_ok=True)\n\n    # Get transforms\n    train_transform, test_transform = get_transforms()\n\n    # Load datasets\n    train_dataset = datasets.CIFAR10(\n        root=data_dir, train=True, download=True, transform=train_transform\n    )\n\n    test_dataset = datasets.CIFAR10(\n        root=data_dir, train=False, download=True, transform=test_transform\n    )\n\n    # Split training into train and validation\n    val_size = int(len(train_dataset) * val_split)\n    train_size = len(train_dataset) - val_size\n\n    train_dataset, val_dataset = random_split(\n        train_dataset, [train_size, val_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n\n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, \n        num_workers=num_workers, pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"api/data/#mnist-dataset","title":"MNIST Dataset","text":"<p>Load MNIST dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Directory to store/load data</p> <code>'./data/raw'</code> <code>batch_size</code> <code>int</code> <p>Batch size for dataloaders</p> <code>64</code> <code>val_split</code> <code>float</code> <p>Fraction of training data to use for validation</p> <code>0.1</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading</p> <code>2</code> <p>Returns:</p> Type Description <code>Tuple[DataLoader, DataLoader, DataLoader]</code> <p>Tuple of (train_loader, val_loader, test_loader)</p> Source code in <code>src/data/make_dataset.py</code> <pre><code>def load_mnist(data_dir: str = \"./data/raw\", batch_size: int = 64,\n               val_split: float = 0.1, num_workers: int = 2) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"\n    Load MNIST dataset.\n\n    Args:\n        data_dir: Directory to store/load data\n        batch_size: Batch size for dataloaders\n        val_split: Fraction of training data to use for validation\n        num_workers: Number of workers for data loading\n\n    Returns:\n        Tuple of (train_loader, val_loader, test_loader)\n    \"\"\"\n    data_path = Path(data_dir)\n    data_path.mkdir(parents=True, exist_ok=True)\n\n    # MNIST-specific transforms\n    train_transform = transforms.Compose([\n        transforms.Resize((28, 28)),\n        transforms.RandomRotation(10),\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    test_transform = transforms.Compose([\n        transforms.Resize((28, 28)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    # Load datasets\n    train_dataset = datasets.MNIST(\n        root=data_dir, train=True, download=True, transform=train_transform\n    )\n\n    test_dataset = datasets.MNIST(\n        root=data_dir, train=False, download=True, transform=test_transform\n    )\n\n    # Split training into train and validation\n    val_size = int(len(train_dataset) * val_split)\n    train_size = len(train_dataset) - val_size\n\n    train_dataset, val_dataset = random_split(\n        train_dataset, [train_size, val_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n\n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"api/data/#data-utilities","title":"Data Utilities","text":""},{"location":"api/data/#transform-functions","title":"Transform Functions","text":"<p>Data preprocessing and augmentation utilities:</p> <p>Get train and test transforms.</p> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>int</code> <p>Size to resize images to</p> <code>32</code> <code>mean</code> <code>Tuple</code> <p>Mean for normalization</p> <code>(0.5, 0.5, 0.5)</code> <code>std</code> <code>Tuple</code> <p>Standard deviation for normalization</p> <code>(0.5, 0.5, 0.5)</code> <p>Returns:</p> Type Description <code>Tuple[Compose, Compose]</code> <p>Tuple of (train_transform, test_transform)</p> Source code in <code>src/data/make_dataset.py</code> <pre><code>def get_transforms(image_size: int = 32, mean: Tuple = (0.5, 0.5, 0.5), \n                   std: Tuple = (0.5, 0.5, 0.5)) -&gt; Tuple[transforms.Compose, transforms.Compose]:\n    \"\"\"\n    Get train and test transforms.\n\n    Args:\n        image_size: Size to resize images to\n        mean: Mean for normalization\n        std: Standard deviation for normalization\n\n    Returns:\n        Tuple of (train_transform, test_transform)\n    \"\"\"\n    train_transform = transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n\n    test_transform = transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n\n    return train_transform, test_transform\n</code></pre>"},{"location":"api/data/#class-reference","title":"Class Reference","text":""},{"location":"api/data/#dataloader-configuration","title":"DataLoader Configuration","text":"<pre><code>from torch.utils.data import DataLoader\n\n# Default parameters\nDataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True\n)\n</code></pre>"},{"location":"api/data/#module-contents","title":"Module Contents","text":"<pre><code>src/data/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 make_dataset.py\n\u2502   \u251c\u2500\u2500 load_cifar10()\n\u2502   \u251c\u2500\u2500 load_mnist()\n\u2502   \u2514\u2500\u2500 get_transforms()\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"api/data/#example-usage","title":"Example Usage","text":"<pre><code>from src.data.make_dataset import load_cifar10\n\n# Load datasets\ntrain_loader, val_loader, test_loader = load_cifar10(\n    data_dir=\"data/raw\",\n    batch_size=32,\n    val_split=0.2,\n    num_workers=4\n)\n\n# Iterate through batches\nfor images, labels in train_loader:\n    print(f\"Batch shape: {images.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n    break\n</code></pre>"},{"location":"api/models/","title":"API Reference - Models Module","text":"<p>Auto-generated API documentation for the models module.</p>"},{"location":"api/models/#model-factory","title":"Model Factory","text":""},{"location":"api/models/#get-model","title":"Get Model","text":"<p>Factory function to get model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model ('simple_cnn' or 'resnet')</p> <code>'simple_cnn'</code> <code>num_classes</code> <code>int</code> <p>Number of output classes</p> <code>10</code> <code>input_channels</code> <code>int</code> <p>Number of input channels</p> <code>3</code> <code>image_size</code> <code>int</code> <p>Input image size</p> <code>32</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for model</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>PyTorch model</p> Source code in <code>src/models/model.py</code> <pre><code>def get_model(model_name: str = \"simple_cnn\", num_classes: int = 10, \n              input_channels: int = 3, image_size: int = 32, **kwargs: Any) -&gt; nn.Module:\n    \"\"\"\n    Factory function to get model by name.\n\n    Args:\n        model_name: Name of the model ('simple_cnn' or 'resnet')\n        num_classes: Number of output classes\n        input_channels: Number of input channels\n        image_size: Input image size\n        **kwargs: Additional arguments for model\n\n    Returns:\n        PyTorch model\n    \"\"\"\n    if model_name == \"simple_cnn\":\n        return SimpleCNN(num_classes=num_classes, input_channels=input_channels, \n                        image_size=image_size, **kwargs)\n    elif model_name == \"resnet\":\n        return ResNet(num_classes=num_classes, input_channels=input_channels)\n    else:\n        raise ValueError(f\"Unknown model name: {model_name}\")\n</code></pre>"},{"location":"api/models/#model-classes","title":"Model Classes","text":""},{"location":"api/models/#simplecnn","title":"SimpleCNN","text":"<p>               Bases: <code>Module</code></p> <p>Simple CNN for image classification.</p> Source code in <code>src/models/model.py</code> <pre><code>class SimpleCNN(nn.Module):\n    \"\"\"Simple CNN for image classification.\"\"\"\n\n    def __init__(self, num_classes: int = 10, input_channels: int = 3, dropout: float = 0.5, image_size: int = 32):\n        \"\"\"\n        Initialize the CNN model.\n\n        Args:\n            num_classes: Number of output classes\n            input_channels: Number of input channels (3 for RGB, 1 for grayscale)\n            dropout: Dropout probability\n            image_size: Input image size (height and width)\n        \"\"\"\n        super(SimpleCNN, self).__init__()\n\n        self.num_classes = num_classes\n        self.input_channels = input_channels\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n\n        # Pooling and dropout\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(dropout)\n\n        # Calculate size after conv layers\n        # After 3 pooling layers (each divides by 2): image_size / 8\n        conv_output_size = image_size // 8\n        fc_input_size = 128 * conv_output_size * conv_output_size\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(fc_input_size, 256)\n        self.fc2 = nn.Linear(256, num_classes)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            x: Input tensor of shape (batch_size, channels, height, width)\n\n        Returns:\n            Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        # Conv block 1\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.pool(x)  # 32x32 -&gt; 16x16 (for CIFAR-10)\n\n        # Conv block 2\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = self.pool(x)  # 16x16 -&gt; 8x8\n\n        # Conv block 3\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = F.relu(x)\n        x = self.pool(x)  # 8x8 -&gt; 4x4\n\n        # Flatten\n        x = x.view(x.size(0), -1)\n\n        # FC layers\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n</code></pre>"},{"location":"api/models/#src.models.model.SimpleCNN.__init__","title":"<code>__init__(num_classes=10, input_channels=3, dropout=0.5, image_size=32)</code>","text":"<p>Initialize the CNN model.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of output classes</p> <code>10</code> <code>input_channels</code> <code>int</code> <p>Number of input channels (3 for RGB, 1 for grayscale)</p> <code>3</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.5</code> <code>image_size</code> <code>int</code> <p>Input image size (height and width)</p> <code>32</code> Source code in <code>src/models/model.py</code> <pre><code>def __init__(self, num_classes: int = 10, input_channels: int = 3, dropout: float = 0.5, image_size: int = 32):\n    \"\"\"\n    Initialize the CNN model.\n\n    Args:\n        num_classes: Number of output classes\n        input_channels: Number of input channels (3 for RGB, 1 for grayscale)\n        dropout: Dropout probability\n        image_size: Input image size (height and width)\n    \"\"\"\n    super(SimpleCNN, self).__init__()\n\n    self.num_classes = num_classes\n    self.input_channels = input_channels\n\n    # Convolutional layers\n    self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n    self.bn1 = nn.BatchNorm2d(32)\n\n    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n    self.bn2 = nn.BatchNorm2d(64)\n\n    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n    self.bn3 = nn.BatchNorm2d(128)\n\n    # Pooling and dropout\n    self.pool = nn.MaxPool2d(2, 2)\n    self.dropout = nn.Dropout(dropout)\n\n    # Calculate size after conv layers\n    # After 3 pooling layers (each divides by 2): image_size / 8\n    conv_output_size = image_size // 8\n    fc_input_size = 128 * conv_output_size * conv_output_size\n\n    # Fully connected layers\n    self.fc1 = nn.Linear(fc_input_size, 256)\n    self.fc2 = nn.Linear(256, num_classes)\n</code></pre>"},{"location":"api/models/#src.models.model.SimpleCNN.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, channels, height, width)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, num_classes)</p> Source code in <code>src/models/model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Args:\n        x: Input tensor of shape (batch_size, channels, height, width)\n\n    Returns:\n        Output tensor of shape (batch_size, num_classes)\n    \"\"\"\n    # Conv block 1\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = F.relu(x)\n    x = self.pool(x)  # 32x32 -&gt; 16x16 (for CIFAR-10)\n\n    # Conv block 2\n    x = self.conv2(x)\n    x = self.bn2(x)\n    x = F.relu(x)\n    x = self.pool(x)  # 16x16 -&gt; 8x8\n\n    # Conv block 3\n    x = self.conv3(x)\n    x = self.bn3(x)\n    x = F.relu(x)\n    x = self.pool(x)  # 8x8 -&gt; 4x4\n\n    # Flatten\n    x = x.view(x.size(0), -1)\n\n    # FC layers\n    x = self.fc1(x)\n    x = F.relu(x)\n    x = self.dropout(x)\n    x = self.fc2(x)\n\n    return x\n</code></pre>"},{"location":"api/models/#resnet","title":"ResNet","text":"<p>               Bases: <code>Module</code></p> <p>Simple ResNet-style architecture.</p> Source code in <code>src/models/model.py</code> <pre><code>class ResNet(nn.Module):\n    \"\"\"Simple ResNet-style architecture.\"\"\"\n\n    def __init__(self, num_classes: int = 10, input_channels: int = 3):\n        \"\"\"\n        Initialize ResNet model.\n\n        Args:\n            num_classes: Number of output classes\n            input_channels: Number of input channels\n        \"\"\"\n        super(ResNet, self).__init__()\n\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, \n                              stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        self.layer1 = self._make_layer(64, 2, stride=1)\n        self.layer2 = self._make_layer(128, 2, stride=2)\n        self.layer3 = self._make_layer(256, 2, stride=2)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)\n\n    def _make_layer(self, out_channels: int, num_blocks: int, stride: int):\n        \"\"\"Create a layer with multiple residual blocks.\"\"\"\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\"\"\"\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avg_pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n</code></pre>"},{"location":"api/models/#src.models.model.ResNet.__init__","title":"<code>__init__(num_classes=10, input_channels=3)</code>","text":"<p>Initialize ResNet model.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of output classes</p> <code>10</code> <code>input_channels</code> <code>int</code> <p>Number of input channels</p> <code>3</code> Source code in <code>src/models/model.py</code> <pre><code>def __init__(self, num_classes: int = 10, input_channels: int = 3):\n    \"\"\"\n    Initialize ResNet model.\n\n    Args:\n        num_classes: Number of output classes\n        input_channels: Number of input channels\n    \"\"\"\n    super(ResNet, self).__init__()\n\n    self.in_channels = 64\n\n    self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, \n                          stride=1, padding=1, bias=False)\n    self.bn1 = nn.BatchNorm2d(64)\n\n    self.layer1 = self._make_layer(64, 2, stride=1)\n    self.layer2 = self._make_layer(128, 2, stride=2)\n    self.layer3 = self._make_layer(256, 2, stride=2)\n\n    self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n    self.fc = nn.Linear(256, num_classes)\n</code></pre>"},{"location":"api/models/#src.models.model.ResNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> Source code in <code>src/models/model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\"\"\"\n    out = F.relu(self.bn1(self.conv1(x)))\n    out = self.layer1(out)\n    out = self.layer2(out)\n    out = self.layer3(out)\n    out = self.avg_pool(out)\n    out = out.view(out.size(0), -1)\n    out = self.fc(out)\n    return out\n</code></pre>"},{"location":"api/models/#inference","title":"Inference","text":""},{"location":"api/models/#batch-prediction","title":"Batch Prediction","text":"<p>Make predictions on a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Trained model</p> required <code>dataloader</code> <code>DataLoader</code> <p>DataLoader containing images</p> required <code>device</code> <code>str</code> <p>Device to run inference on</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple of (predictions, true_labels)</p> Source code in <code>src/models/predict.py</code> <pre><code>def predict_batch(model: nn.Module, dataloader: DataLoader, device: str = 'cpu') -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Make predictions on a batch of images.\n\n    Args:\n        model: Trained model\n        dataloader: DataLoader containing images\n        device: Device to run inference on\n\n    Returns:\n        Tuple of (predictions, true_labels)\n    \"\"\"\n    model.eval()\n    all_predictions = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images = images.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n\n            all_predictions.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return np.array(all_predictions), np.array(all_labels)\n</code></pre>"},{"location":"api/models/#module-contents","title":"Module Contents","text":"<pre><code>src/models/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 model.py\n\u2502   \u251c\u2500\u2500 get_model()\n\u2502   \u251c\u2500\u2500 SimpleCNN\n\u2502   \u2514\u2500\u2500 ResNet\n\u251c\u2500\u2500 predict.py\n\u2502   \u251c\u2500\u2500 predict_batch()\n\u2502   \u2514\u2500\u2500 predict_single()\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"api/models/#model-architecture","title":"Model Architecture","text":""},{"location":"api/models/#simplecnn_1","title":"SimpleCNN","text":"<pre><code>SimpleCNN(\n    num_classes=10,\n    input_channels=3,\n    dropout=0.5,\n    image_size=32\n)\n</code></pre> <p>Structure: - Conv Layer 1: 3 \u2192 64 channels - Conv Layer 2: 64 \u2192 128 channels - Conv Layer 3: 128 \u2192 128 channels - Fully Connected: 12844 \u2192 256 \u2192 num_classes</p>"},{"location":"api/models/#resnet_1","title":"ResNet","text":"<p>Standard ResNet architecture adapted for CIFAR-10/MNIST with: - Residual blocks - Batch normalization - Global average pooling</p>"},{"location":"api/models/#example-usage","title":"Example Usage","text":"<pre><code>import torch\nfrom src.models.model import get_model\n\n# Create model\nmodel = get_model(\n    model_name=\"simple_cnn\",\n    num_classes=10,\n    input_channels=3,\n    image_size=32,\n    dropout=0.5\n)\n\n# Forward pass\nbatch = torch.randn(32, 3, 32, 32)\noutput = model(batch)\nprint(output.shape)  # [32, 10]\n\n# Load checkpoint\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Predictions\nwith torch.no_grad():\n    predictions = model(batch)\n    probs = torch.softmax(predictions, dim=1)\n    class_ids = predictions.argmax(dim=1)\n</code></pre>"},{"location":"api/models/#model-properties","title":"Model Properties","text":"<ul> <li>Total Parameters: Varies by architecture</li> <li>Input Size: Configurable (32x32 for CIFAR-10, 28x28 for MNIST)</li> <li>Output Size: Number of classes (10)</li> <li>Device: CPU/CUDA compatible</li> </ul>"},{"location":"api/training/","title":"API Reference - Training Module","text":"<p>Auto-generated API documentation for the training module.</p>"},{"location":"api/training/#training-functions","title":"Training Functions","text":""},{"location":"api/training/#main-training-loop","title":"Main Training Loop","text":"<p>Main training function.</p> Source code in <code>src/training/train.py</code> <pre><code>@hydra.main(version_base=None, config_path=\"../../configs\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    \"\"\"Main training function.\"\"\"\n\n    # Print configuration\n    print(\"=\" * 80)\n    print(\"Configuration:\")\n    print(OmegaConf.to_yaml(cfg))\n    print(\"=\" * 80)\n\n    # Set seed\n    set_seed(cfg.seed)\n\n    # Get device\n    device = get_device(cfg.device)\n    print(f\"\\nUsing device: {device}\")\n\n    # Load data\n    print(f\"\\nLoading {cfg.data.name} dataset...\")\n    if cfg.data.name == \"cifar10\":\n        train_loader, val_loader, test_loader = load_cifar10(\n            data_dir=cfg.data.data_dir,\n            batch_size=cfg.hyperparameters.batch_size,\n            val_split=cfg.data.val_split,\n            num_workers=cfg.num_workers\n        )\n    elif cfg.data.name == \"mnist\":\n        train_loader, val_loader, test_loader = load_mnist(\n            data_dir=cfg.data.data_dir,\n            batch_size=cfg.hyperparameters.batch_size,\n            val_split=cfg.data.val_split,\n            num_workers=cfg.num_workers\n        )\n    else:\n        raise ValueError(f\"Unknown dataset: {cfg.data.name}\")\n\n    print(f\"Train batches: {len(train_loader)}\")\n    print(f\"Val batches: {len(val_loader)}\")\n    print(f\"Test batches: {len(test_loader)}\")\n\n    # Create model\n    print(f\"\\nCreating {cfg.model.name} model...\")\n    model = get_model(\n        model_name=cfg.model.name,\n        num_classes=cfg.data.num_classes,\n        input_channels=cfg.data.input_channels,\n        image_size=cfg.data.image_size,\n        dropout=cfg.hyperparameters.dropout\n    )\n    model = model.to(device)\n\n    # Count parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n\n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss(label_smoothing=cfg.hyperparameters.label_smoothing)\n    optimizer = get_optimizer(model, cfg)\n    scheduler = get_scheduler(optimizer, cfg)\n\n    # Training history\n    history = {\n        'train_loss': [],\n        'train_acc': [],\n        'val_loss': [],\n        'val_acc': []\n    }\n\n    best_val_acc = 0.0\n    patience_counter = 0\n\n    # Training loop\n    print(f\"\\nStarting training for {cfg.hyperparameters.num_epochs} epochs...\")\n    print(\"=\" * 80)\n\n    for epoch in range(cfg.hyperparameters.num_epochs):\n        print(f\"\\nEpoch {epoch + 1}/{cfg.hyperparameters.num_epochs}\")\n        print(\"-\" * 80)\n\n        # Train\n        train_loss, train_acc = train_epoch(\n            model, train_loader, criterion, optimizer, device,\n            clip_grad_norm=cfg.hyperparameters.clip_grad_norm\n        )\n\n        # Validate\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n\n        # Update learning rate\n        if scheduler is not None:\n            if isinstance(scheduler, ReduceLROnPlateau):\n                scheduler.step(val_loss)\n            else:\n                scheduler.step()\n\n        # Save history\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n\n        # Print epoch summary\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"\\nEpoch Summary:\")\n        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n        print(f\"  Learning Rate: {current_lr:.6f}\")\n\n        # Save best model\n        if val_acc &gt; best_val_acc:\n            best_val_acc = val_acc\n            patience_counter = 0\n\n            if cfg.save_best_only:\n                model_save_path = Path(cfg.model_save_dir) / f\"{cfg.model.name}_best.pth\"\n                model_save_path.parent.mkdir(parents=True, exist_ok=True)\n\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'val_acc': val_acc,\n                    'config': OmegaConf.to_container(cfg)\n                }, model_save_path)\n\n                print(f\"  \u2713 Best model saved! (Val Acc: {val_acc:.2f}%)\")\n        else:\n            patience_counter += 1\n\n        # Early stopping\n        if patience_counter &gt;= cfg.early_stopping_patience:\n            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n            break\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Training completed!\")\n    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n\n    # Load best model for evaluation\n    best_model_path = Path(cfg.model_save_dir) / f\"{cfg.model.name}_best.pth\"\n    if best_model_path.exists():\n        checkpoint = torch.load(best_model_path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"\\nLoaded best model from epoch {checkpoint['epoch'] + 1}\")\n\n    # Evaluate on test set\n    print(\"\\nEvaluating on test set...\")\n    test_loss, test_acc = validate(model, test_loader, criterion, device)\n    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n\n    # Generate predictions and metrics\n    print(\"\\nGenerating predictions and metrics...\")\n    predictions, true_labels = predict_batch(model, test_loader, device)\n\n    # Calculate metrics\n    metrics = calculate_metrics(predictions, true_labels)\n    print(\"\\nTest Metrics:\")\n    for metric_name, metric_value in metrics.items():\n        print(f\"  {metric_name}: {metric_value:.4f}\")\n\n    # Save reports\n    report_dir = Path(cfg.report_dir)\n    report_dir.mkdir(parents=True, exist_ok=True)\n\n    # Confusion matrix\n    plot_confusion_matrix(\n        predictions, true_labels, cfg.data.classes,\n        save_path=str(report_dir / \"confusion_matrix.png\")\n    )\n\n    # Classification report\n    save_classification_report(\n        predictions, true_labels, cfg.data.classes,\n        save_path=str(report_dir / \"classification_report.txt\")\n    )\n\n    # Training history plot\n    plot_training_history(\n        history,\n        save_path=str(report_dir / \"training_history.png\")\n    )\n\n    print(f\"\\nReports saved to {report_dir}\")\n    print(\"\\n\" + \"=\" * 80)\n    print(\"All done! \ud83c\udf89\")\n</code></pre>"},{"location":"api/training/#train-epoch","title":"Train Epoch","text":"<p>Train for one epoch.</p> Source code in <code>src/training/train.py</code> <pre><code>def train_epoch(model, train_loader, criterion, optimizer, device, clip_grad_norm=None):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n\n    losses = AverageMeter()\n    correct = 0\n    total = 0\n\n    pbar = tqdm(train_loader, desc=\"Training\")\n    for images, labels in pbar:\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Gradient clipping\n        if clip_grad_norm is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n\n        optimizer.step()\n\n        # Update metrics\n        losses.update(loss.item(), images.size(0))\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n        # Update progress bar\n        pbar.set_postfix({\n            'loss': f'{losses.avg:.4f}',\n            'acc': f'{100.*correct/total:.2f}%'\n        })\n\n    return losses.avg, 100. * correct / total\n</code></pre>"},{"location":"api/training/#validation","title":"Validation","text":"<p>Validate the model.</p> Source code in <code>src/training/train.py</code> <pre><code>def validate(model, val_loader, criterion, device):\n    \"\"\"Validate the model.\"\"\"\n    model.eval()\n\n    losses = AverageMeter()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        pbar = tqdm(val_loader, desc=\"Validation\")\n        for images, labels in pbar:\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            losses.update(loss.item(), images.size(0))\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n            pbar.set_postfix({\n                'loss': f'{losses.avg:.4f}',\n                'acc': f'{100.*correct/total:.2f}%'\n            })\n\n    return losses.avg, 100. * correct / total\n</code></pre>"},{"location":"api/training/#utilities","title":"Utilities","text":""},{"location":"api/training/#set-seed","title":"Set Seed","text":"<p>Set random seed for reproducibility.</p> Source code in <code>src/training/train.py</code> <pre><code>def set_seed(seed: int):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"api/training/#get-device","title":"Get Device","text":"<p>Get device based on configuration.</p> Source code in <code>src/training/train.py</code> <pre><code>def get_device(device_config: str) -&gt; torch.device:\n    \"\"\"Get device based on configuration.\"\"\"\n    if device_config == \"auto\":\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        return torch.device(device_config)\n</code></pre>"},{"location":"api/training/#get-optimizer","title":"Get Optimizer","text":"<p>Get optimizer based on configuration.</p> Source code in <code>src/training/train.py</code> <pre><code>def get_optimizer(model, cfg):\n    \"\"\"Get optimizer based on configuration.\"\"\"\n    if cfg.hyperparameters.optimizer.lower() == \"adam\":\n        return optim.Adam(\n            model.parameters(),\n            lr=cfg.hyperparameters.learning_rate,\n            weight_decay=cfg.hyperparameters.weight_decay\n        )\n    elif cfg.hyperparameters.optimizer.lower() == \"sgd\":\n        return optim.SGD(\n            model.parameters(),\n            lr=cfg.hyperparameters.learning_rate,\n            momentum=cfg.hyperparameters.momentum,\n            weight_decay=cfg.hyperparameters.weight_decay\n        )\n    else:\n        raise ValueError(f\"Unknown optimizer: {cfg.hyperparameters.optimizer}\")\n</code></pre>"},{"location":"api/training/#get-scheduler","title":"Get Scheduler","text":"<p>Get learning rate scheduler based on configuration.</p> Source code in <code>src/training/train.py</code> <pre><code>def get_scheduler(optimizer, cfg):\n    \"\"\"Get learning rate scheduler based on configuration.\"\"\"\n    if not cfg.hyperparameters.use_scheduler:\n        return None\n\n    scheduler_type = cfg.hyperparameters.scheduler_type.lower()\n\n    if scheduler_type == \"step\":\n        return StepLR(\n            optimizer,\n            step_size=cfg.hyperparameters.step_size,\n            gamma=cfg.hyperparameters.gamma\n        )\n    elif scheduler_type == \"cosine\":\n        return CosineAnnealingLR(\n            optimizer,\n            T_max=cfg.hyperparameters.num_epochs,\n            eta_min=cfg.hyperparameters.min_lr\n        )\n    elif scheduler_type == \"plateau\":\n        return ReduceLROnPlateau(\n            optimizer,\n            mode='min',\n            factor=cfg.hyperparameters.gamma,\n            patience=5,\n            min_lr=cfg.hyperparameters.min_lr\n        )\n    else:\n        raise ValueError(f\"Unknown scheduler type: {scheduler_type}\")\n</code></pre>"},{"location":"api/training/#module-contents","title":"Module Contents","text":"<pre><code>src/training/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 train.py\n\u2502   \u251c\u2500\u2500 main()\n\u2502   \u251c\u2500\u2500 train_epoch()\n\u2502   \u251c\u2500\u2500 validate()\n\u2502   \u251c\u2500\u2500 set_seed()\n\u2502   \u251c\u2500\u2500 get_device()\n\u2502   \u251c\u2500\u2500 get_optimizer()\n\u2502   \u2514\u2500\u2500 get_scheduler()\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"api/training/#training-configuration","title":"Training Configuration","text":""},{"location":"api/training/#hydra-configuration","title":"Hydra Configuration","text":"<p>Configure training via YAML files in <code>configs/</code>:</p> <pre><code># Main config\nhyperparameters:\n  optimizer: adam\n  learning_rate: 0.001\n  batch_size: 32\n  num_epochs: 50\n  dropout: 0.5\n  label_smoothing: 0.1\n  clip_grad_norm: 1.0\n</code></pre>"},{"location":"api/training/#supported-optimizers","title":"Supported Optimizers","text":"<ul> <li>Adam: Adaptive learning rates</li> <li>SGD: Stochastic gradient descent</li> </ul>"},{"location":"api/training/#supported-schedulers","title":"Supported Schedulers","text":"<ul> <li>Step: Decay learning rate by gamma every step_size epochs</li> <li>Cosine: Cosine annealing schedule</li> <li>Plateau: Reduce learning rate on metric plateau</li> </ul>"},{"location":"api/training/#example-usage","title":"Example Usage","text":"<pre><code>from src.training.train import (\n    train_epoch, validate, set_seed,\n    get_device, get_optimizer\n)\nimport torch\nimport torch.nn as nn\n\n# Setup\nset_seed(42)\ndevice = get_device(\"auto\")\nmodel = ...  # Create model\ncriterion = nn.CrossEntropyLoss()\noptimizer = get_optimizer(model, config)\n\n# Train one epoch\ntrain_loss, train_acc = train_epoch(\n    model, train_loader, criterion,\n    optimizer, device\n)\n\n# Validate\nval_loss, val_acc = validate(\n    model, val_loader, criterion, device\n)\n\nprint(f\"Train: loss={train_loss:.4f}, acc={train_acc:.2f}%\")\nprint(f\"Val: loss={val_loss:.4f}, acc={val_acc:.2f}%\")\n</code></pre>"},{"location":"api/training/#training-outputs","title":"Training Outputs","text":"<p>During training, the script saves: - Best model: <code>models/{model_name}_best.pth</code> - Reports: <code>reports/</code> directory   - <code>confusion_matrix.png</code>   - <code>classification_report.txt</code>   - <code>training_history.png</code></p>"},{"location":"api/training/#checkpoint-format","title":"Checkpoint Format","text":"<pre><code>checkpoint = {\n    'epoch': int,\n    'model_state_dict': dict,\n    'optimizer_state_dict': dict,\n    'val_acc': float,\n    'config': dict\n}\n</code></pre> <p>Load checkpoint:</p> <pre><code>checkpoint = torch.load('models/simple_cnn_best.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nepoch = checkpoint['epoch']\n</code></pre>"},{"location":"api/utils/","title":"API Reference - Utils Module","text":"<p>Auto-generated API documentation for the utilities module.</p>"},{"location":"api/utils/#metrics","title":"Metrics","text":""},{"location":"api/utils/#calculate-metrics","title":"Calculate Metrics","text":"<p>Calculate classification metrics.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Predicted labels</p> required <code>labels</code> <code>ndarray</code> <p>True labels</p> required <code>average</code> <code>str</code> <p>Averaging strategy for multi-class metrics</p> <code>'macro'</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing metrics</p> Source code in <code>src/utils/metrics.py</code> <pre><code>def calculate_metrics(predictions: np.ndarray, labels: np.ndarray, \n                     average: str = 'macro') -&gt; dict:\n    \"\"\"\n    Calculate classification metrics.\n\n    Args:\n        predictions: Predicted labels\n        labels: True labels\n        average: Averaging strategy for multi-class metrics\n\n    Returns:\n        Dictionary containing metrics\n    \"\"\"\n    metrics = {\n        'accuracy': accuracy_score(labels, predictions),\n        'precision': precision_score(labels, predictions, average=average, zero_division=0),\n        'recall': recall_score(labels, predictions, average=average, zero_division=0),\n        'f1_score': f1_score(labels, predictions, average=average, zero_division=0)\n    }\n\n    return metrics\n</code></pre>"},{"location":"api/utils/#confusion-matrix","title":"Confusion Matrix","text":"<p>Plot confusion matrix.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Predicted labels</p> required <code>labels</code> <code>ndarray</code> <p>True labels</p> required <code>class_names</code> <code>List[str]</code> <p>List of class names</p> required <code>save_path</code> <code>str</code> <p>Path to save figure</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size</p> <code>(10, 8)</code> Source code in <code>src/utils/metrics.py</code> <pre><code>def plot_confusion_matrix(predictions: np.ndarray, labels: np.ndarray,\n                          class_names: List[str], save_path: str = None,\n                          figsize: Tuple[int, int] = (10, 8)):\n    \"\"\"\n    Plot confusion matrix.\n\n    Args:\n        predictions: Predicted labels\n        labels: True labels\n        class_names: List of class names\n        save_path: Path to save figure\n        figsize: Figure size\n    \"\"\"\n    cm = confusion_matrix(labels, predictions)\n\n    plt.figure(figsize=figsize)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n\n    if save_path:\n        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\"Confusion matrix saved to {save_path}\")\n\n    plt.close()\n</code></pre>"},{"location":"api/utils/#classification-report","title":"Classification Report","text":"<p>Save classification report to file.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Predicted labels</p> required <code>labels</code> <code>ndarray</code> <p>True labels</p> required <code>class_names</code> <code>List[str]</code> <p>List of class names</p> required <code>save_path</code> <code>str</code> <p>Path to save report</p> required Source code in <code>src/utils/metrics.py</code> <pre><code>def save_classification_report(predictions: np.ndarray, labels: np.ndarray,\n                               class_names: List[str], save_path: str):\n    \"\"\"\n    Save classification report to file.\n\n    Args:\n        predictions: Predicted labels\n        labels: True labels\n        class_names: List of class names\n        save_path: Path to save report\n    \"\"\"\n    report = classification_report(labels, predictions, \n                                   target_names=class_names, digits=4)\n\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n\n    with open(save_path, 'w') as f:\n        f.write(\"Classification Report\\n\")\n        f.write(\"=\" * 80 + \"\\n\\n\")\n        f.write(report)\n        f.write(\"\\n\\n\")\n\n        # Add overall metrics\n        metrics = calculate_metrics(predictions, labels)\n        f.write(\"Overall Metrics\\n\")\n        f.write(\"-\" * 80 + \"\\n\")\n        for metric_name, metric_value in metrics.items():\n            f.write(f\"{metric_name}: {metric_value:.4f}\\n\")\n\n    print(f\"Classification report saved to {save_path}\")\n</code></pre>"},{"location":"api/utils/#training-history-plot","title":"Training History Plot","text":"<p>Plot training history (loss and accuracy).</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>dict</code> <p>Dictionary containing 'train_loss', 'val_loss', 'train_acc', 'val_acc'</p> required <code>save_path</code> <code>str</code> <p>Path to save figure</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size</p> <code>(12, 4)</code> Source code in <code>src/utils/metrics.py</code> <pre><code>def plot_training_history(history: dict, save_path: str = None,\n                          figsize: Tuple[int, int] = (12, 4)):\n    \"\"\"\n    Plot training history (loss and accuracy).\n\n    Args:\n        history: Dictionary containing 'train_loss', 'val_loss', 'train_acc', 'val_acc'\n        save_path: Path to save figure\n        figsize: Figure size\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=figsize)\n\n    # Plot loss\n    axes[0].plot(history['train_loss'], label='Train Loss')\n    if 'val_loss' in history:\n        axes[0].plot(history['val_loss'], label='Val Loss')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    # Plot accuracy\n    axes[1].plot(history['train_acc'], label='Train Accuracy')\n    if 'val_acc' in history:\n        axes[1].plot(history['val_acc'], label='Val Accuracy')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n\n    if save_path:\n        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\"Training history plot saved to {save_path}\")\n\n    plt.close()\n</code></pre>"},{"location":"api/utils/#utilities","title":"Utilities","text":""},{"location":"api/utils/#average-meter","title":"Average Meter","text":"<p>Computes and stores the average and current value.</p> Source code in <code>src/utils/metrics.py</code> <pre><code>class AverageMeter:\n    \"\"\"Computes and stores the average and current value.\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset all statistics.\"\"\"\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val: float, n: int = 1) -&gt; None:\n        \"\"\"\n        Update statistics.\n\n        Args:\n            val: Value to add\n            n: Number of items\n        \"\"\"\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count if self.count != 0 else 0\n</code></pre>"},{"location":"api/utils/#src.utils.metrics.AverageMeter.reset","title":"<code>reset()</code>","text":"<p>Reset all statistics.</p> Source code in <code>src/utils/metrics.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all statistics.\"\"\"\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n</code></pre>"},{"location":"api/utils/#src.utils.metrics.AverageMeter.update","title":"<code>update(val, n=1)</code>","text":"<p>Update statistics.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>float</code> <p>Value to add</p> required <code>n</code> <code>int</code> <p>Number of items</p> <code>1</code> Source code in <code>src/utils/metrics.py</code> <pre><code>def update(self, val: float, n: int = 1) -&gt; None:\n    \"\"\"\n    Update statistics.\n\n    Args:\n        val: Value to add\n        n: Number of items\n    \"\"\"\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count if self.count != 0 else 0\n</code></pre>"},{"location":"api/utils/#module-contents","title":"Module Contents","text":"<pre><code>src/utils/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 metrics.py\n\u2502   \u251c\u2500\u2500 AverageMeter\n\u2502   \u251c\u2500\u2500 calculate_metrics()\n\u2502   \u251c\u2500\u2500 plot_confusion_matrix()\n\u2502   \u251c\u2500\u2500 save_classification_report()\n\u2502   \u2514\u2500\u2500 plot_training_history()\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"api/utils/#metrics-calculation","title":"Metrics Calculation","text":"<p>Supported metrics: - Accuracy: Overall correctness - Precision: True positives / (true positives + false positives) - Recall: True positives / (true positives + false negatives) - F1 Score: Harmonic mean of precision and recall</p>"},{"location":"api/utils/#example-usage","title":"Example Usage","text":"<pre><code>from src.utils.metrics import (\n    AverageMeter, calculate_metrics,\n    plot_confusion_matrix\n)\nimport numpy as np\n\n# Track average loss\nloss_meter = AverageMeter()\nfor batch_loss in losses:\n    loss_meter.update(batch_loss, batch_size)\n\nprint(f\"Average loss: {loss_meter.avg:.4f}\")\n\n# Calculate metrics\npredictions = np.array([...])  # Predicted labels\nlabels = np.array([...])  # True labels\n\nmetrics = calculate_metrics(predictions, labels)\nprint(f\"Accuracy: {metrics['accuracy']:.4f}\")\nprint(f\"Precision: {metrics['precision']:.4f}\")\nprint(f\"Recall: {metrics['recall']:.4f}\")\nprint(f\"F1 Score: {metrics['f1_score']:.4f}\")\n\n# Generate visualizations\nplot_confusion_matrix(\n    predictions, labels,\n    class_names=['class_0', 'class_1', ...],\n    save_path=\"reports/confusion_matrix.png\"\n)\n</code></pre>"},{"location":"api/utils/#averagemeter-class","title":"AverageMeter Class","text":"<p>Track running average of a metric:</p> <pre><code>meter = AverageMeter()\nmeter.update(value, n)  # n = batch size\nprint(meter.avg)  # Get average\n</code></pre>"},{"location":"api/utils/#visualization-functions","title":"Visualization Functions","text":""},{"location":"api/utils/#confusion-matrix_1","title":"Confusion Matrix","text":"<p>Heatmap showing prediction vs true labels.</p>"},{"location":"api/utils/#classification-report_1","title":"Classification Report","text":"<p>Text report with precision, recall, F1-score per class.</p>"},{"location":"api/utils/#training-history","title":"Training History","text":"<p>Line plots of loss and accuracy over epochs.</p>"},{"location":"development/code-style/","title":"Code Style","text":"<p>Code style guidelines and standards for this project.</p>"},{"location":"development/code-style/#python-style-guide","title":"Python Style Guide","text":"<p>We follow PEP 8 with tools for enforcement:</p> <ul> <li>Black: Code formatting</li> <li>isort: Import sorting</li> <li>flake8: Linting</li> <li>mypy: Type checking</li> </ul>"},{"location":"development/code-style/#running-style-checks","title":"Running Style Checks","text":""},{"location":"development/code-style/#format-code-with-black","title":"Format Code with Black","text":"<pre><code># Format all Python files\nblack src/ tests/\n\n# Format specific file\nblack src/training/train.py\n\n# Check without modifying\nblack --check src/\n</code></pre>"},{"location":"development/code-style/#sort-imports-with-isort","title":"Sort Imports with isort","text":"<pre><code># Sort imports\nisort src/ tests/\n\n# Check without modifying\nisort --check-only src/\n</code></pre>"},{"location":"development/code-style/#lint-with-flake8","title":"Lint with flake8","text":"<pre><code># Run linter\nflake8 src/ tests/\n\n# Show statistics\nflake8 --statistics src/\n</code></pre>"},{"location":"development/code-style/#type-check-with-mypy","title":"Type Check with mypy","text":"<pre><code># Run type checker\nmypy src/\n\n# Strict mode\nmypy --strict src/\n</code></pre>"},{"location":"development/code-style/#code-style-rules","title":"Code Style Rules","text":""},{"location":"development/code-style/#naming-conventions","title":"Naming Conventions","text":"<pre><code># Constants: UPPER_CASE\nLEARNING_RATE = 0.001\nMAX_EPOCHS = 100\n\n# Functions and variables: snake_case\ndef train_model(data_loader, model):\n    pass\n\n# Classes: PascalCase\nclass SimpleCNN(torch.nn.Module):\n    pass\n\n# Private methods: _leading_underscore\ndef _preprocess_data(data):\n    pass\n</code></pre>"},{"location":"development/code-style/#line-length","title":"Line Length","text":"<p>Maximum 88 characters (Black default):</p> <pre><code># Good - fits within 88 chars\nresult = some_function(arg1, arg2, arg3)\n\n# Bad - exceeds 88 chars\nresult = some_very_long_function_name(argument_one, argument_two, argument_three)\n\n# Good - break long lines\nresult = some_very_long_function_name(\n    argument_one,\n    argument_two,\n    argument_three\n)\n</code></pre>"},{"location":"development/code-style/#imports","title":"Imports","text":"<pre><code># Good - grouped and sorted\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nfrom src.models.model import SimpleCNN\nfrom src.utils.metrics import calculate_metrics\n\n# Bad - unsorted, mixed order\nfrom src.models.model import SimpleCNN\nimport sys\nimport torch\nfrom src.utils.metrics import calculate_metrics\nimport os\n</code></pre>"},{"location":"development/code-style/#docstring-format","title":"Docstring Format","text":"<p>Use Google-style docstrings:</p> <pre><code>def train_model(\n    model: torch.nn.Module,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    num_epochs: int = 50,\n    learning_rate: float = 0.001\n) -&gt; Dict[str, List[float]]:\n    \"\"\"Train a neural network model.\n\n    Long description of what this function does,\n    including important details and context.\n\n    Args:\n        model: PyTorch model to train\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        num_epochs: Number of training epochs. Defaults to 50.\n        learning_rate: Learning rate for optimizer. Defaults to 0.001.\n\n    Returns:\n        Dictionary containing training history with keys:\n        - 'train_loss': List of training losses\n        - 'val_loss': List of validation losses\n        - 'train_acc': List of training accuracies\n        - 'val_acc': List of validation accuracies\n\n    Raises:\n        ValueError: If num_epochs &lt;= 0\n        TypeError: If model is not a torch.nn.Module\n\n    Example:\n        &gt;&gt;&gt; model = SimpleCNN()\n        &gt;&gt;&gt; history = train_model(model, train_loader, val_loader)\n        &gt;&gt;&gt; print(history['val_acc'][-1])\n\n    Note:\n        This function modifies the model in-place.\n\n    See Also:\n        validate() - Function for model validation\n    \"\"\"\n    pass\n</code></pre>"},{"location":"development/code-style/#type-hints","title":"Type Hints","text":"<p>Always use type hints:</p> <pre><code># Good\ndef process_batch(\n    images: torch.Tensor,\n    labels: torch.Tensor,\n    device: torch.device\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    images = images.to(device)\n    labels = labels.to(device)\n    return images, labels\n\n# Bad\ndef process_batch(images, labels, device):\n    images = images.to(device)\n    labels = labels.to(device)\n    return images, labels\n\n# Optional types\nfrom typing import Optional\n\ndef train_model(\n    model: torch.nn.Module,\n    checkpoint_path: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    if checkpoint_path is not None:\n        load_checkpoint(model, checkpoint_path)\n    return {}\n</code></pre>"},{"location":"development/code-style/#string-formatting","title":"String Formatting","text":"<p>Use f-strings:</p> <pre><code># Good\nname = \"CIFAR-10\"\nprint(f\"Training on {name} dataset\")\n\n# OK but less readable\nprint(\"Training on {} dataset\".format(name))\n\n# Avoid\nprint(\"Training on \" + name + \" dataset\")\n</code></pre>"},{"location":"development/code-style/#whitespace","title":"Whitespace","text":"<pre><code># Good - proper spacing\ndef calculate_metrics(predictions: np.ndarray, labels: np.ndarray) -&gt; float:\n    accuracy = (predictions == labels).sum() / len(labels)\n    return accuracy\n\n# Bad - inconsistent spacing\ndef calculate_metrics(predictions:np.ndarray,labels:np.ndarray)-&gt;float:\n    accuracy=(predictions==labels).sum()/len(labels)\n    return accuracy\n</code></pre>"},{"location":"development/code-style/#listdict-comprehensions","title":"List/Dict Comprehensions","text":"<p>Use comprehensions for readability:</p> <pre><code># Good\nsquares = [x ** 2 for x in range(10)]\neven_numbers = {x: x**2 for x in range(10) if x % 2 == 0}\n\n# Acceptable but less readable\nsquares = []\nfor x in range(10):\n    squares.append(x ** 2)\n</code></pre>"},{"location":"development/code-style/#comments","title":"Comments","text":"<pre><code># Good - explains WHY, not WHAT\n# Use exponential moving average for smoother loss tracking\nsmoothed_loss = 0.99 * smoothed_loss + 0.01 * current_loss\n\n# Bad - just repeats the code\n# Multiply smoothed_loss by 0.99 and add current_loss multiplied by 0.01\nsmoothed_loss = 0.99 * smoothed_loss + 0.01 * current_loss\n\n# Good - explain non-obvious logic\n# We use 32x32 for CIFAR-10 to match standard benchmark size\nimage_size = 32\n</code></pre>"},{"location":"development/code-style/#file-organization","title":"File Organization","text":""},{"location":"development/code-style/#module-structure","title":"Module Structure","text":"<pre><code># src/models/model.py\n\n\"\"\"CNN architectures for image classification.\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom typing import Optional\n\n# Constants first\nDEFAULT_IMAGE_SIZE = 32\nDEFAULT_NUM_CLASSES = 10\n\n# Then functions\ndef get_model(model_name: str, **kwargs) -&gt; nn.Module:\n    \"\"\"Factory function for creating models.\"\"\"\n    pass\n\n# Then classes\nclass SimpleCNN(nn.Module):\n    \"\"\"Simple CNN for image classification.\"\"\"\n    pass\n\nclass ResNet(nn.Module):\n    \"\"\"ResNet architecture.\"\"\"\n    pass\n</code></pre>"},{"location":"development/code-style/#linting-configuration","title":"Linting Configuration","text":""},{"location":"development/code-style/#flake8-configuration","title":"flake8 Configuration","text":"<pre><code># setup.cfg or .flake8\n[flake8]\nmax-line-length = 88\nexclude = .git,__pycache__,venv\nignore = E203,W503\n</code></pre>"},{"location":"development/code-style/#isort-configuration","title":"isort Configuration","text":"<pre><code># pyproject.toml\n[tool.isort]\nprofile = \"black\"\nline_length = 88\nmulti_line_mode = 3\ninclude_trailing_comma = true\n</code></pre>"},{"location":"development/code-style/#mypy-configuration","title":"mypy Configuration","text":"<pre><code># mypy.ini\n[mypy]\npython_version = 3.9\nwarn_return_any = True\nwarn_unused_configs = True\nignore_missing_imports = True\n</code></pre>"},{"location":"development/code-style/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Automatically format code before commit:</p> <pre><code># Install\npip install pre-commit\n\n# Setup\npre-commit install\n\n# Run manually\npre-commit run --all-files\n</code></pre>"},{"location":"development/code-style/#ide-integration","title":"IDE Integration","text":""},{"location":"development/code-style/#vs-code","title":"VS Code","text":"<p>Install extensions: - Python - Pylance - Black Formatter - isort</p> <p>Add to <code>.vscode/settings.json</code>:</p> <pre><code>{\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.mypyEnabled\": true,\n    \"[python]\": {\n        \"editor.formatOnSave\": true,\n        \"editor.codeActionsOnSave\": {\n            \"source.organizeImports\": true\n        }\n    }\n}\n</code></pre>"},{"location":"development/code-style/#checklist-before-commit","title":"Checklist Before Commit","text":"<ul> <li> Code formatted with Black</li> <li> Imports sorted with isort</li> <li> No flake8 warnings</li> <li> Type hints added</li> <li> Docstrings present</li> <li> Tests pass</li> <li> No commented-out code</li> </ul>"},{"location":"development/contributing/","title":"Contributing","text":"<p>Guidelines for contributing to this project.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#fork-and-clone","title":"Fork and Clone","text":"<pre><code># Fork on GitHub, then\ngit clone https://github.com/Hadayxinchao/end-to-end-image-classifier.git\ncd end-to-end-image-classifier\ngit remote add upstream https://github.com/original/end-to-end-image-classifier.git\n</code></pre>"},{"location":"development/contributing/#setup-development-environment","title":"Setup Development Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\npip install -e .  # Install in development mode\n</code></pre>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#create-feature-branch","title":"Create Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#make-changes","title":"Make Changes","text":"<p>Follow the code style guidelines (see Code Style).</p>"},{"location":"development/contributing/#write-tests","title":"Write Tests","text":"<p>Add tests in <code>tests/</code> directory:</p> <pre><code># tests/test_my_feature.py\nimport pytest\nfrom src.module import function\n\ndef test_function():\n    assert function(input) == expected_output\n</code></pre>"},{"location":"development/contributing/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test\npytest tests/test_my_feature.py\n\n# With coverage\npytest --cov=src tests/\n</code></pre>"},{"location":"development/contributing/#commit-changes","title":"Commit Changes","text":"<pre><code>git add .\ngit commit -m \"Add feature: description\"\n</code></pre>"},{"location":"development/contributing/#push-and-create-pr","title":"Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"development/contributing/#code-standards","title":"Code Standards","text":""},{"location":"development/contributing/#python-style","title":"Python Style","text":"<p>Follow PEP 8 and use: - Black: Code formatting - isort: Import sorting - flake8: Linting - mypy: Type checking</p> <pre><code># Format code\nblack src/ tests/\n\n# Sort imports\nisort src/ tests/\n\n# Lint\nflake8 src/ tests/\n\n# Type check\nmypy src/\n</code></pre>"},{"location":"development/contributing/#type-hints","title":"Type Hints","text":"<p>Use type hints for all functions:</p> <pre><code>from typing import List, Tuple\nimport numpy as np\n\ndef process_batch(\n    images: np.ndarray,\n    labels: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Process a batch of images and labels.\n\n    Args:\n        images: Image array of shape (B, C, H, W)\n        labels: Label array of shape (B,)\n\n    Returns:\n        Processed images and labels\n    \"\"\"\n    return images, labels\n</code></pre>"},{"location":"development/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def train_model(\n    train_loader,\n    val_loader,\n    num_epochs: int = 50,\n    learning_rate: float = 0.001\n) -&gt; dict:\n    \"\"\"Train a model.\n\n    Args:\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        num_epochs: Number of training epochs\n        learning_rate: Learning rate for optimizer\n\n    Returns:\n        Dictionary containing training history\n\n    Raises:\n        ValueError: If num_epochs &lt;= 0\n\n    Example:\n        &gt;&gt;&gt; history = train_model(train_loader, val_loader)\n        &gt;&gt;&gt; print(history['accuracy'])\n    \"\"\"\n    if num_epochs &lt;= 0:\n        raise ValueError(\"num_epochs must be positive\")\n\n    # Implementation\n    return {}\n</code></pre>"},{"location":"development/contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"development/contributing/#test-structure","title":"Test Structure","text":"<pre><code># tests/test_models.py\nimport pytest\nimport torch\nfrom src.models.model import SimpleCNN, get_model\n\nclass TestSimpleCNN:\n    \"\"\"Test SimpleCNN model.\"\"\"\n\n    @pytest.fixture\n    def model(self):\n        \"\"\"Create test model.\"\"\"\n        return SimpleCNN(num_classes=10, image_size=32)\n\n    def test_initialization(self, model):\n        \"\"\"Test model initialization.\"\"\"\n        assert model is not None\n\n    def test_forward_pass(self, model):\n        \"\"\"Test forward pass.\"\"\"\n        x = torch.randn(2, 3, 32, 32)\n        y = model(x)\n        assert y.shape == (2, 10)\n\n    @pytest.mark.parametrize(\"image_size\", [28, 32, 64])\n    def test_different_sizes(self, image_size):\n        \"\"\"Test model with different input sizes.\"\"\"\n        model = SimpleCNN(num_classes=10, image_size=image_size)\n        x = torch.randn(2, 3, image_size, image_size)\n        y = model(x)\n        assert y.shape == (2, 10)\n</code></pre>"},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test class\npytest tests/test_models.py::TestSimpleCNN\n\n# Run specific test\npytest tests/test_models.py::TestSimpleCNN::test_forward_pass\n\n# Run with coverage report\npytest --cov=src --cov-report=html tests/\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#update-documentation","title":"Update Documentation","text":"<ol> <li>Update relevant <code>.md</code> file in <code>docs/</code></li> <li>Follow Markdown format</li> <li>Include code examples where applicable</li> </ol>"},{"location":"development/contributing/#build-documentation-locally","title":"Build Documentation Locally","text":"<pre><code>pip install mkdocs mkdocs-material\nmkdocs serve\n</code></pre> <p>Then visit <code>http://localhost:8000</code></p>"},{"location":"development/contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"development/contributing/#bug-reports","title":"Bug Reports","text":"<p>Create an issue with: - Title: Clear, concise description - Description: What happened, what should happen - Reproduction: Steps to reproduce - Environment: Python version, OS, packages</p>"},{"location":"development/contributing/#feature-requests","title":"Feature Requests","text":"<p>Describe: - Use case: Why it's needed - Proposed solution: How to implement - Alternatives: Other approaches considered</p>"},{"location":"development/contributing/#pr-review-process","title":"PR Review Process","text":"<ol> <li>Check automated tests: Must pass all CI/CD checks</li> <li>Code review: 2 approvals required</li> <li>Documentation: Updated if needed</li> <li>Merge: Squash and merge to main</li> </ol>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Discussions: GitHub Discussions</li> <li>Issues: GitHub Issues</li> <li>Documentation: Read the docs site</li> <li>Email: Contact maintainers</li> </ul>"},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>No harassment or discrimination</li> <li>Assume good faith</li> <li>Give and accept constructive feedback</li> </ul> <p>Thank you for contributing! \ud83c\udf89</p>"},{"location":"development/testing/","title":"Testing","text":"<p>Comprehensive guide to testing in this project.</p>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 test_data.py         # Data loading tests\n\u251c\u2500\u2500 test_model.py        # Model architecture tests\n\u251c\u2500\u2500 test_training.py     # Training pipeline tests\n\u2514\u2500\u2500 fixtures/            # Test fixtures and mocks\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#run-all-tests","title":"Run All Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"development/testing/#run-specific-test-file","title":"Run Specific Test File","text":"<pre><code>pytest tests/test_model.py\n</code></pre>"},{"location":"development/testing/#run-specific-test-class","title":"Run Specific Test Class","text":"<pre><code>pytest tests/test_model.py::TestSimpleCNN\n</code></pre>"},{"location":"development/testing/#run-specific-test","title":"Run Specific Test","text":"<pre><code>pytest tests/test_model.py::TestSimpleCNN::test_forward_pass\n</code></pre>"},{"location":"development/testing/#verbose-output","title":"Verbose Output","text":"<pre><code>pytest -v\n</code></pre>"},{"location":"development/testing/#show-print-statements","title":"Show Print Statements","text":"<pre><code>pytest -s\n</code></pre>"},{"location":"development/testing/#stop-on-first-failure","title":"Stop on First Failure","text":"<pre><code>pytest -x\n</code></pre>"},{"location":"development/testing/#code-coverage","title":"Code Coverage","text":""},{"location":"development/testing/#generate-coverage-report","title":"Generate Coverage Report","text":"<pre><code>pytest --cov=src tests/\n</code></pre>"},{"location":"development/testing/#html-coverage-report","title":"HTML Coverage Report","text":"<pre><code>pytest --cov=src --cov-report=html tests/\n</code></pre> <p>View report in <code>htmlcov/index.html</code></p>"},{"location":"development/testing/#coverage-threshold","title":"Coverage Threshold","text":"<p>Configure in <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\naddopts = \"--cov=src --cov-report=term-missing --cov-fail-under=80\"\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Test individual functions:</p> <pre><code># tests/test_utils.py\nfrom src.utils.metrics import AverageMeter\n\ndef test_average_meter():\n    meter = AverageMeter()\n    meter.update(10, 1)\n    meter.update(20, 2)\n    assert meter.avg == 15.0  # (10*1 + 20*2) / 3\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Test module interactions:</p> <pre><code># tests/test_training.py\ndef test_training_pipeline(train_loader, val_loader):\n    model = get_model(\"simple_cnn\", num_classes=10, image_size=32)\n    optimizer = torch.optim.Adam(model.parameters())\n    criterion = torch.nn.CrossEntropyLoss()\n\n    train_loss, train_acc = train_epoch(\n        model, train_loader, criterion, optimizer, device\n    )\n\n    assert train_loss &gt; 0\n    assert 0 &lt;= train_acc &lt;= 100\n</code></pre>"},{"location":"development/testing/#parametrized-tests","title":"Parametrized Tests","text":"<p>Test multiple inputs:</p> <pre><code>import pytest\n\n@pytest.mark.parametrize(\"model_name\", [\"simple_cnn\", \"resnet\"])\n@pytest.mark.parametrize(\"image_size\", [28, 32])\ndef test_models(model_name, image_size):\n    model = get_model(model_name, num_classes=10, image_size=image_size)\n    x = torch.randn(2, 3, image_size, image_size)\n    y = model(x)\n    assert y.shape == (2, 10)\n</code></pre>"},{"location":"development/testing/#test-fixtures","title":"Test Fixtures","text":""},{"location":"development/testing/#common-fixtures","title":"Common Fixtures","text":"<pre><code># tests/conftest.py\nimport pytest\nimport torch\n\n@pytest.fixture\ndef device():\n    return torch.device(\"cpu\")\n\n@pytest.fixture\ndef model():\n    from src.models.model import SimpleCNN\n    return SimpleCNN(num_classes=10, image_size=32)\n\n@pytest.fixture\ndef sample_batch():\n    images = torch.randn(4, 3, 32, 32)\n    labels = torch.tensor([0, 1, 2, 3])\n    return images, labels\n</code></pre>"},{"location":"development/testing/#using-fixtures","title":"Using Fixtures","text":"<pre><code>def test_model_forward(model, sample_batch, device):\n    images, labels = sample_batch\n    images = images.to(device)\n\n    output = model(images)\n    assert output.shape == (4, 10)\n</code></pre>"},{"location":"development/testing/#mocking","title":"Mocking","text":""},{"location":"development/testing/#mock-external-calls","title":"Mock External Calls","text":"<pre><code>from unittest.mock import patch, MagicMock\n\n@patch('src.data.make_dataset.torchvision.datasets.CIFAR10')\ndef test_load_cifar10(mock_cifar10):\n    mock_cifar10.return_value = MagicMock()\n\n    train_loader, _, _ = load_cifar10()\n    mock_cifar10.assert_called_once()\n</code></pre>"},{"location":"development/testing/#performance-tests","title":"Performance Tests","text":""},{"location":"development/testing/#test-execution-time","title":"Test Execution Time","text":"<pre><code>import pytest\nimport time\n\n@pytest.mark.performance\ndef test_training_speed():\n    model = get_model(\"simple_cnn\", num_classes=10, image_size=32)\n    batch = torch.randn(32, 3, 32, 32)\n\n    start = time.time()\n    for _ in range(100):\n        _ = model(batch)\n    elapsed = time.time() - start\n\n    # Should complete 100 batches in &lt; 5 seconds\n    assert elapsed &lt; 5.0\n</code></pre>"},{"location":"development/testing/#skip-slow-tests","title":"Skip Slow Tests","text":"<pre><code>pytest -m \"not performance\"\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions","title":"GitHub Actions","text":"<p>Tests run on every push/PR:</p> <pre><code># .github/workflows/tests.yaml\n- name: Run tests\n  run: pytest --cov=src tests/\n</code></pre>"},{"location":"development/testing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<pre><code># Install pre-commit\npip install pre-commit\n\n# Setup hooks\npre-commit install\n\n# Run manually\npre-commit run --all-files\n</code></pre>"},{"location":"development/testing/#debugging-tests","title":"Debugging Tests","text":""},{"location":"development/testing/#use-pytest-debugger","title":"Use pytest Debugger","text":"<pre><code>pytest --pdb\n</code></pre> <p>This drops into debugger on failure.</p>"},{"location":"development/testing/#use-print-statements","title":"Use Print Statements","text":"<pre><code>pytest -s  # Show print output\n</code></pre>"},{"location":"development/testing/#verbose-output_1","title":"Verbose Output","text":"<pre><code>pytest -vv  # Extra verbose\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":"<ol> <li> <p>One assertion per test (when possible)    <pre><code># Good\ndef test_model_output_shape():\n    output = model(x)\n    assert output.shape == (2, 10)\n\n# Bad\ndef test_model():\n    output = model(x)\n    assert output.shape == (2, 10)\n    assert output.dtype == torch.float32\n</code></pre></p> </li> <li> <p>Use descriptive names <pre><code># Good\ndef test_simple_cnn_forward_pass_with_correct_input_shape()\n\n# Bad\ndef test_model()\n</code></pre></p> </li> <li> <p>Test edge cases <pre><code>def test_metrics_with_empty_predictions():\n    predictions = np.array([])\n    labels = np.array([])\n    # Should handle gracefully\n</code></pre></p> </li> <li> <p>Use fixtures for setup <pre><code># Good\n@pytest.fixture\ndef model():\n    return SimpleCNN()\n\n# Bad\ndef test_something():\n    model = SimpleCNN()  # Setup in test\n</code></pre></p> </li> <li> <p>Mock external dependencies <pre><code># Good - Mock network call\n@patch('requests.get')\ndef test_download(mock_get)\n\n# Bad - Actually calls network\ndef test_download():\n    response = requests.get(url)\n</code></pre></p> </li> </ol>"},{"location":"development/testing/#test-examples","title":"Test Examples","text":""},{"location":"development/testing/#data-loading-test","title":"Data Loading Test","text":"<pre><code>def test_load_cifar10(tmp_path):\n    train_loader, val_loader, test_loader = load_cifar10(\n        data_dir=str(tmp_path),\n        batch_size=32\n    )\n\n    assert len(train_loader) &gt; 0\n    assert len(val_loader) &gt; 0\n\n    images, labels = next(iter(train_loader))\n    assert images.shape == (32, 3, 32, 32)\n    assert labels.shape == (32,)\n</code></pre>"},{"location":"development/testing/#model-test","title":"Model Test","text":"<pre><code>def test_simple_cnn_different_image_sizes():\n    for image_size in [28, 32, 64]:\n        model = SimpleCNN(num_classes=10, image_size=image_size)\n        x = torch.randn(4, 3, image_size, image_size)\n        y = model(x)\n        assert y.shape == (4, 10)\n</code></pre>"},{"location":"development/testing/#training-test","title":"Training Test","text":"<pre><code>def test_training_improves_loss(train_loader, model):\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n    initial_loss = None\n    final_loss = None\n\n    for epoch in range(2):\n        loss, _ = train_epoch(\n            model, train_loader, criterion, optimizer, device\n        )\n        if epoch == 0:\n            initial_loss = loss\n        else:\n            final_loss = loss\n\n    # Loss should decrease after training\n    assert final_loss &lt; initial_loss\n</code></pre> <p>Enjoy testing! \ud83e\uddea</p>"},{"location":"getting-started/configuration/","title":"Configuration Management with Hydra","text":"<p>This project uses Hydra for configuration management, making it easy to run experiments without modifying code.</p>"},{"location":"getting-started/configuration/#configuration-structure","title":"Configuration Structure","text":"<pre><code>configs/\n\u251c\u2500\u2500 config.yaml              # Main configuration\n\u251c\u2500\u2500 model/\n\u2502   \u251c\u2500\u2500 simple_cnn.yaml     # Simple CNN config\n\u2502   \u2514\u2500\u2500 resnet.yaml         # ResNet config\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 cifar10.yaml        # CIFAR-10 dataset config\n\u2502   \u2514\u2500\u2500 mnist.yaml          # MNIST dataset config\n\u2514\u2500\u2500 hyperparameters/\n    \u251c\u2500\u2500 default.yaml        # Default hyperparameters\n    \u2514\u2500\u2500 fast.yaml           # Fast training config\n</code></pre>"},{"location":"getting-started/configuration/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/configuration/#override-single-parameter","title":"Override Single Parameter","text":"<pre><code>python src/training/train.py hyperparameters.learning_rate=0.01\n</code></pre>"},{"location":"getting-started/configuration/#override-multiple-parameters","title":"Override Multiple Parameters","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.learning_rate=0.01 \\\n  hyperparameters.batch_size=128 \\\n  hyperparameters.num_epochs=20\n</code></pre>"},{"location":"getting-started/configuration/#use-different-config-group","title":"Use Different Config Group","text":"<pre><code># Use ResNet model\npython src/training/train.py model=resnet\n\n# Use MNIST dataset\npython src/training/train.py data=mnist\n\n# Use fast hyperparameters\npython src/training/train.py hyperparameters=fast\n</code></pre>"},{"location":"getting-started/configuration/#combine-overrides","title":"Combine Overrides","text":"<pre><code>python src/training/train.py \\\n  model=resnet \\\n  data=mnist \\\n  hyperparameters=fast\n</code></pre>"},{"location":"getting-started/configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"getting-started/configuration/#main-config-configyaml","title":"Main Config (<code>config.yaml</code>)","text":"<pre><code>defaults:\n  - model: simple_cnn\n  - data: cifar10\n  - hyperparameters: default\n  - _self_\n\nseed: 42\ndevice: auto\nnum_workers: 2\n\noutput_dir: ./outputs\nmodel_save_dir: ./models\nreport_dir: ./reports\n\nlog_interval: 10\nsave_best_only: true\nearly_stopping_patience: 10\n\nexperiment_name: image_classifier\nrun_name: ${now:%Y-%m-%d_%H-%M-%S}\n</code></pre>"},{"location":"getting-started/configuration/#model-configs","title":"Model Configs","text":"<p>Simple CNN (<code>model/simple_cnn.yaml</code>): <pre><code>name: simple_cnn\nnum_classes: 10\ninput_channels: 3\ndropout: 0.5\n</code></pre></p> <p>ResNet (<code>model/resnet.yaml</code>): <pre><code>name: resnet\nnum_classes: 10\ninput_channels: 3\n</code></pre></p>"},{"location":"getting-started/configuration/#data-configs","title":"Data Configs","text":"<p>CIFAR-10 (<code>data/cifar10.yaml</code>): <pre><code>name: cifar10\ndata_dir: ./data/raw\nbatch_size: 64\nval_split: 0.1\nimage_size: 32\nnum_classes: 10\ninput_channels: 3\n\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\n\nclasses:\n  - airplane\n  - automobile\n  - bird\n  # ... etc\n</code></pre></p>"},{"location":"getting-started/configuration/#hyperparameter-configs","title":"Hyperparameter Configs","text":"<p>Default (<code>hyperparameters/default.yaml</code>): <pre><code>learning_rate: 0.001\nweight_decay: 1e-4\nmomentum: 0.9\noptimizer: adam\n\nnum_epochs: 50\nbatch_size: 64\n\nuse_scheduler: true\nscheduler_type: step\nstep_size: 10\ngamma: 0.1\nmin_lr: 1e-6\n\ndropout: 0.5\nlabel_smoothing: 0.0\nclip_grad_norm: 1.0\n</code></pre></p>"},{"location":"getting-started/configuration/#advanced-usage","title":"Advanced Usage","text":""},{"location":"getting-started/configuration/#create-custom-experiment-config","title":"Create Custom Experiment Config","text":"<p>Create <code>configs/experiment/my_exp.yaml</code>:</p> <pre><code># @package _global_\n\ndefaults:\n  - override /model: resnet\n  - override /data: cifar10\n  - override /hyperparameters: default\n\n# Override specific values\nhyperparameters:\n  learning_rate: 0.005\n  num_epochs: 100\n  batch_size: 128\n  use_scheduler: true\n  scheduler_type: cosine\n\nseed: 1234\nexperiment_name: resnet_cifar10_cosine\n</code></pre> <p>Run it:</p> <pre><code>python src/training/train.py --config-name=experiment/my_exp\n</code></pre>"},{"location":"getting-started/configuration/#access-config-in-code","title":"Access Config in Code","text":"<pre><code>import hydra\nfrom omegaconf import DictConfig\n\n@hydra.main(version_base=None, config_path=\"../configs\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    print(cfg.hyperparameters.learning_rate)\n    print(cfg.model.name)\n    # Access nested configs easily\n</code></pre>"},{"location":"getting-started/configuration/#print-configuration","title":"Print Configuration","text":"<pre><code># Print full config\npython src/training/train.py --cfg job\n\n# Print specific group\npython src/training/train.py --cfg job model\n</code></pre>"},{"location":"getting-started/configuration/#composition","title":"Composition","text":"<p>Create different combinations:</p> <pre><code># Experiment 1: Simple CNN on CIFAR-10\npython src/training/train.py \\\n  model=simple_cnn \\\n  data=cifar10 \\\n  experiment_name=exp1_simple_cifar\n\n# Experiment 2: ResNet on MNIST\npython src/training/train.py \\\n  model=resnet \\\n  data=mnist \\\n  experiment_name=exp2_resnet_mnist\n</code></pre>"},{"location":"getting-started/configuration/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/configuration/#learning-rate-tuning","title":"Learning Rate Tuning","text":"<pre><code># Try different learning rates\npython src/training/train.py hyperparameters.learning_rate=0.0001\npython src/training/train.py hyperparameters.learning_rate=0.001\npython src/training/train.py hyperparameters.learning_rate=0.01\n</code></pre>"},{"location":"getting-started/configuration/#batch-size-tuning","title":"Batch Size Tuning","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.batch_size=32 \\\n  data.batch_size=32\n</code></pre>"},{"location":"getting-started/configuration/#scheduler-experiments","title":"Scheduler Experiments","text":"<pre><code># Step scheduler\npython src/training/train.py \\\n  hyperparameters.scheduler_type=step \\\n  hyperparameters.step_size=15\n\n# Cosine annealing\npython src/training/train.py \\\n  hyperparameters.scheduler_type=cosine\n\n# No scheduler\npython src/training/train.py \\\n  hyperparameters.use_scheduler=false\n</code></pre>"},{"location":"getting-started/configuration/#regularization-tuning","title":"Regularization Tuning","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.dropout=0.3 \\\n  hyperparameters.weight_decay=1e-5 \\\n  hyperparameters.label_smoothing=0.1\n</code></pre>"},{"location":"getting-started/configuration/#multi-run","title":"Multi-Run","text":"<p>Run multiple experiments with different parameters:</p> <pre><code>python src/training/train.py -m \\\n  hyperparameters.learning_rate=0.001,0.01,0.1\n</code></pre> <p>This creates separate runs for each learning rate.</p>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Use environment variables in configs:</p> <pre><code>data_dir: ${oc.env:DATA_DIR,./data/raw}\n</code></pre> <p>Then:</p> <pre><code>export DATA_DIR=/path/to/data\npython src/training/train.py\n</code></pre>"},{"location":"getting-started/configuration/#tips-best-practices","title":"Tips &amp; Best Practices","text":"<ol> <li>Never hardcode values - Use configs for everything</li> <li>Create experiment configs - Document your experiments</li> <li>Use meaningful names - <code>experiment_name: resnet_lr001_bs128</code></li> <li>Version configs with git - Track what worked</li> <li>Use defaults - Override only what you need</li> </ol>"},{"location":"getting-started/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/configuration/#config-not-found","title":"Config Not Found","text":"<p>Make sure you're in the project root:</p> <pre><code>cd /path/to/end-to-end-image-classifier\npython src/training/train.py\n</code></pre>"},{"location":"getting-started/configuration/#override-not-working","title":"Override Not Working","text":"<p>Use correct syntax:</p> <pre><code># \u2705 Correct\npython src/training/train.py hyperparameters.learning_rate=0.01\n\n# \u274c Wrong\npython src/training/train.py --learning_rate=0.01\n</code></pre>"},{"location":"getting-started/configuration/#nested-override","title":"Nested Override","text":"<p>For nested values:</p> <pre><code>python src/training/train.py model.dropout=0.3\n</code></pre>"},{"location":"getting-started/configuration/#learn-more","title":"Learn More","text":"<ul> <li>Hydra Documentation</li> <li>OmegaConf Documentation</li> <li>Configuration Patterns</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will help you set up the Image Classifier project on your local machine.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.8 or higher: Download Python</li> <li>Git: Download Git</li> <li>pip: Python package installer (usually comes with Python)</li> <li>(Optional) Docker: Download Docker</li> </ul>"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/Hadayxinchao/end-to-end-image-classifier.git\ncd end-to-end-image-classifier\n</code></pre>"},{"location":"getting-started/installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<p>It's recommended to use a virtual environment to avoid dependency conflicts.</p> Linux/MacWindows <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <pre><code>python -m venv venv\nvenv\\Scripts\\activate\n</code></pre>"},{"location":"getting-started/installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code># Upgrade pip\npip install --upgrade pip\n\n# Install project dependencies\npip install -r requirements.txt\n\n# Install the package in editable mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#4-verify-installation","title":"4. Verify Installation","text":"<p>Run the tests to ensure everything is working:</p> <pre><code>pytest tests/ -v -m \"not slow\"\n</code></pre> <p>If all tests pass, you're ready to go! \u2705</p>"},{"location":"getting-started/installation/#optional-setup-dvc","title":"Optional: Setup DVC","text":"<p>If you want to use Data Version Control:</p> <pre><code># Initialize DVC\ndvc init\n\n# Add remote storage (example: local storage)\ndvc remote add -d storage /tmp/dvc-storage\n</code></pre> <p>For more details, see the Data Versioning Guide.</p>"},{"location":"getting-started/installation/#optional-setup-docker","title":"Optional: Setup Docker","text":"<p>Build the Docker image:</p> <pre><code>docker build -t image-classifier:latest .\n</code></pre> <p>Test the Docker image:</p> <pre><code>docker run --rm image-classifier:latest python --version\n</code></pre>"},{"location":"getting-started/installation/#development-setup","title":"Development Setup","text":"<p>If you plan to contribute to the project:</p>"},{"location":"getting-started/installation/#install-development-dependencies","title":"Install Development Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>This includes: - <code>pytest</code> - Testing framework - <code>black</code> - Code formatter - <code>flake8</code> - Linter - <code>mypy</code> - Type checker</p>"},{"location":"getting-started/installation/#setup-pre-commit-hooks-optional","title":"Setup Pre-commit Hooks (Optional)","text":"<pre><code>pip install pre-commit\npre-commit install\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#issue-pytorch-installation-fails","title":"Issue: PyTorch Installation Fails","text":"<p>If you have GPU and want CUDA support:</p> <pre><code># CUDA 11.8\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\n# CUDA 12.1\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>For CPU-only: <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n</code></pre></p>"},{"location":"getting-started/installation/#issue-permission-denied","title":"Issue: Permission Denied","text":"<p>On Linux/Mac, you might need to use <code>sudo</code> or fix permissions:</p> <pre><code>sudo chown -R $USER:$USER .\n</code></pre>"},{"location":"getting-started/installation/#issue-module-not-found","title":"Issue: Module Not Found","text":"<p>Make sure you installed the package:</p> <pre><code>pip install -e .\n</code></pre> <p>And that your virtual environment is activated.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you've installed the project, check out:</p> <ul> <li>Quick Start Guide - Train your first model</li> <li>Configuration Guide - Learn about Hydra configs</li> <li>User Guide - Detailed training instructions</li> </ul>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>CPU: Dual-core processor</li> <li>RAM: 4 GB</li> <li>Storage: 2 GB free space</li> <li>OS: Linux, macOS, or Windows 10+</li> </ul>"},{"location":"getting-started/installation/#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>CPU: Quad-core processor or better</li> <li>RAM: 8 GB or more</li> <li>GPU: NVIDIA GPU with CUDA support (for faster training)</li> <li>Storage: 10 GB free space (for datasets and models)</li> <li>OS: Ubuntu 20.04+ or equivalent</li> </ul>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter any issues:</p> <ol> <li>Check the Troubleshooting section</li> <li>Search GitHub Issues</li> <li>Create a new issue with details about your problem</li> </ol>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>Get up and running with the Image Classifier in just a few minutes!</p>"},{"location":"getting-started/quickstart/#train-your-first-model","title":"Train Your First Model","text":""},{"location":"getting-started/quickstart/#using-default-configuration","title":"Using Default Configuration","text":"<p>The simplest way to start:</p> <pre><code>python src/training/train.py\n</code></pre> <p>This will:</p> <ul> <li>Download the CIFAR-10 dataset automatically</li> <li>Train a simple CNN model</li> <li>Save the best model to <code>models/</code></li> <li>Generate reports in <code>reports/</code></li> </ul>"},{"location":"getting-started/quickstart/#training-output","title":"Training Output","text":"<p>You'll see output like this:</p> <pre><code>================================================================================\nConfiguration:\ndata:\n  batch_size: 64\n  name: cifar10\n  num_classes: 10\nhyperparameters:\n  learning_rate: 0.001\n  num_epochs: 50\n...\n================================================================================\n\nUsing device: cuda\n\nLoading cifar10 dataset...\nTrain batches: 704\nVal batches: 79\nTest batches: 157\n\nCreating simple_cnn model...\nTotal parameters: 1,234,567\nTrainable parameters: 1,234,567\n\nStarting training for 50 epochs...\n</code></pre>"},{"location":"getting-started/quickstart/#quick-experiments","title":"Quick Experiments","text":""},{"location":"getting-started/quickstart/#change-learning-rate","title":"Change Learning Rate","text":"<pre><code>python src/training/train.py hyperparameters.learning_rate=0.01\n</code></pre>"},{"location":"getting-started/quickstart/#use-different-model","title":"Use Different Model","text":"<pre><code>python src/training/train.py model=resnet\n</code></pre>"},{"location":"getting-started/quickstart/#train-on-mnist-instead","title":"Train on MNIST Instead","text":"<pre><code>python src/training/train.py data=mnist\n</code></pre>"},{"location":"getting-started/quickstart/#fast-training-for-testing","title":"Fast Training (for testing)","text":"<pre><code>python src/training/train.py hyperparameters=fast\n</code></pre> <p>This uses: - 5 epochs instead of 50 - Larger batch size (128) - No learning rate scheduler</p>"},{"location":"getting-started/quickstart/#view-results","title":"View Results","text":"<p>After training, check the results:</p>"},{"location":"getting-started/quickstart/#training-report","title":"Training Report","text":"<pre><code>cat reports/classification_report.txt\n</code></pre>"},{"location":"getting-started/quickstart/#confusion-matrix","title":"Confusion Matrix","text":"<p>The confusion matrix is saved as an image:</p> <pre><code># Linux/Mac\nxdg-open reports/figures/confusion_matrix.png\n\n# Mac\nopen reports/figures/confusion_matrix.png\n\n# Windows\nstart reports/figures/confusion_matrix.png\n</code></pre>"},{"location":"getting-started/quickstart/#training-history","title":"Training History","text":"<p>View the training curves:</p> <pre><code># Linux/Mac\nxdg-open reports/figures/training_history.png\n</code></pre>"},{"location":"getting-started/quickstart/#make-predictions","title":"Make Predictions","text":""},{"location":"getting-started/quickstart/#on-a-single-image","title":"On a Single Image","text":"<pre><code>python src/models/predict.py \\\n  --model_path models/simple_cnn_best.pth \\\n  --image_path path/to/your/image.jpg \\\n  --dataset cifar10\n</code></pre> <p>Example output:</p> <pre><code>Prediction: cat\nConfidence: 0.9234\n\nAll class probabilities:\n  airplane: 0.0012\n  automobile: 0.0045\n  bird: 0.0234\n  cat: 0.9234\n  deer: 0.0123\n  ...\n</code></pre>"},{"location":"getting-started/quickstart/#run-tests","title":"Run Tests","text":"<p>Make sure everything works:</p> <pre><code># Run fast tests only\npytest tests/ -m \"not slow\"\n\n# Run all tests\npytest tests/\n\n# Run with coverage\npytest --cov=src tests/\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":""},{"location":"getting-started/quickstart/#learn-configuration-management","title":"Learn Configuration Management","text":"<p>Hydra makes it easy to manage experiments. Learn more:</p> <pre><code># See all config options\npython src/training/train.py --help\n\n# Use different config group\npython src/training/train.py --config-name=experiment1\n</code></pre> <p>Read the Configuration Guide for details.</p>"},{"location":"getting-started/quickstart/#experiment-tracking","title":"Experiment Tracking","text":"<p>Create a new experiment config:</p> <pre><code>mkdir -p configs/experiment\n</code></pre> <p>Create <code>configs/experiment/my_experiment.yaml</code>:</p> <pre><code># @package _global_\n\ndefaults:\n  - override /model: resnet\n  - override /hyperparameters: default\n\nhyperparameters:\n  learning_rate: 0.005\n  num_epochs: 30\n  batch_size: 128\n\nexperiment_name: my_first_experiment\n</code></pre> <p>Run it:</p> <pre><code>python src/training/train.py --config-name=experiment/my_experiment\n</code></pre>"},{"location":"getting-started/quickstart/#use-docker","title":"Use Docker","text":"<p>Train in a container:</p> <pre><code># Build image\ndocker build -t image-classifier .\n\n# Run training\ndocker run --rm \\\n  -v $(pwd)/models:/app/models \\\n  -v $(pwd)/reports:/app/reports \\\n  image-classifier\n</code></pre>"},{"location":"getting-started/quickstart/#version-your-data","title":"Version Your Data","text":"<p>Track datasets with DVC:</p> <pre><code># Initialize DVC\ndvc init\n\n# Track data\ndvc add data/raw\n\n# Commit to git\ngit add data/raw.dvc .dvc/config\ngit commit -m \"Track data with DVC\"\n</code></pre>"},{"location":"getting-started/quickstart/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/quickstart/#workflow-1-quick-iteration","title":"Workflow 1: Quick Iteration","text":"<pre><code># Fast training for debugging\npython src/training/train.py \\\n  hyperparameters=fast \\\n  hyperparameters.num_epochs=2\n\n# Check results\ncat reports/classification_report.txt\n</code></pre>"},{"location":"getting-started/quickstart/#workflow-2-production-training","title":"Workflow 2: Production Training","text":"<pre><code># Full training with best settings\npython src/training/train.py \\\n  model=resnet \\\n  hyperparameters.num_epochs=100 \\\n  hyperparameters.learning_rate=0.001 \\\n  hyperparameters.use_scheduler=true\n</code></pre>"},{"location":"getting-started/quickstart/#workflow-3-hyperparameter-search","title":"Workflow 3: Hyperparameter Search","text":"<pre><code># Try different learning rates\nfor lr in 0.0001 0.001 0.01; do\n  python src/training/train.py \\\n    hyperparameters.learning_rate=$lr \\\n    experiment_name=lr_search_$lr\ndone\n</code></pre>"},{"location":"getting-started/quickstart/#tips-tricks","title":"Tips &amp; Tricks","text":""},{"location":"getting-started/quickstart/#monitor-gpu-usage","title":"Monitor GPU Usage","text":"<pre><code># While training, in another terminal:\nwatch -n 1 nvidia-smi\n</code></pre>"},{"location":"getting-started/quickstart/#save-outputs-to-custom-directory","title":"Save Outputs to Custom Directory","text":"<pre><code>python src/training/train.py \\\n  output_dir=./outputs/experiment_1 \\\n  model_save_dir=./models/experiment_1\n</code></pre>"},{"location":"getting-started/quickstart/#use-multiple-gpus","title":"Use Multiple GPUs","text":"<p>PyTorch will automatically use all available GPUs. To restrict:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python src/training/train.py\n</code></pre>"},{"location":"getting-started/quickstart/#resume-training","title":"Resume Training","text":"<p>Currently not implemented, but you can add this feature! See Contributing.</p>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quickstart/#out-of-memory","title":"Out of Memory","text":"<p>Reduce batch size:</p> <pre><code>python src/training/train.py hyperparameters.batch_size=32\n</code></pre>"},{"location":"getting-started/quickstart/#training-too-slow","title":"Training Too Slow","text":"<p>Use fast config or fewer epochs:</p> <pre><code>python src/training/train.py hyperparameters.num_epochs=10\n</code></pre>"},{"location":"getting-started/quickstart/#cant-find-model-file","title":"Can't Find Model File","text":"<p>Check the model save directory:</p> <pre><code>ls -la models/\n</code></pre> <p>Models are saved as <code>{model_name}_best.pth</code>.</p>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Configuration Guide - Master Hydra configuration</li> <li>Training Guide - Advanced training techniques</li> <li>CI/CD Setup - Automate your workflow</li> </ul>"},{"location":"guide/data/","title":"Data Management","text":"<p>This guide explains how to manage datasets and data versioning.</p>"},{"location":"guide/data/#dataset-overview","title":"Dataset Overview","text":"<p>The project supports two datasets:</p>"},{"location":"guide/data/#cifar-10","title":"CIFAR-10","text":"<ul> <li>Resolution: 32\u00d732 RGB images</li> <li>Classes: 10 (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)</li> <li>Samples: 50,000 training, 10,000 test</li> <li>Download: Automatic via torchvision</li> </ul>"},{"location":"guide/data/#mnist","title":"MNIST","text":"<ul> <li>Resolution: 28\u00d728 grayscale images</li> <li>Classes: 10 (digits 0-9)</li> <li>Samples: 60,000 training, 10,000 test</li> <li>Download: Automatic via torchvision</li> </ul>"},{"location":"guide/data/#downloading-datasets","title":"Downloading Datasets","text":""},{"location":"guide/data/#automatic-download","title":"Automatic Download","text":"<p>Data downloads automatically on first run:</p> <pre><code>python src/training/train.py data=cifar10\n</code></pre>"},{"location":"guide/data/#manual-download","title":"Manual Download","text":"<p>To pre-download datasets:</p> <pre><code>from src.data.make_dataset import load_cifar10, load_mnist\n\n# Download CIFAR-10\nload_cifar10(data_dir=\"data/raw\")\n\n# Download MNIST\nload_mnist(data_dir=\"data/raw\")\n</code></pre>"},{"location":"guide/data/#data-directory-structure","title":"Data Directory Structure","text":"<pre><code>data/\n\u251c\u2500\u2500 raw/                 # Original datasets\n\u2502   \u251c\u2500\u2500 cifar-10/\n\u2502   \u2514\u2500\u2500 mnist/\n\u2514\u2500\u2500 processed/           # Processed data (if any)\n</code></pre>"},{"location":"guide/data/#data-configuration","title":"Data Configuration","text":"<p>Configure datasets in <code>configs/data/</code>:</p> <p>CIFAR-10 (<code>configs/data/cifar10.yaml</code>): <pre><code>name: cifar10\nnum_classes: 10\ninput_channels: 3\nimage_size: 32\nval_split: 0.2\nclasses: [airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]\n</code></pre></p> <p>MNIST (<code>configs/data/mnist.yaml</code>): <pre><code>name: mnist\nnum_classes: 10\ninput_channels: 1\nimage_size: 28\nval_split: 0.2\nclasses: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre></p>"},{"location":"guide/data/#data-augmentation","title":"Data Augmentation","text":""},{"location":"guide/data/#training-augmentation","title":"Training Augmentation","text":"<p>Images are augmented during training: - Random horizontal flip - Random rotation (\u00b110 degrees) - Random crop - Normalization with ImageNet statistics</p>"},{"location":"guide/data/#validationtest","title":"Validation/Test","text":"<p>No augmentation for validation/test sets.</p>"},{"location":"guide/data/#custom-augmentation","title":"Custom Augmentation","text":"<p>Modify <code>src/data/make_dataset.py</code>:</p> <pre><code>train_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.RandomErasing(p=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n</code></pre>"},{"location":"guide/data/#data-versioning-with-dvc","title":"Data Versioning with DVC","text":""},{"location":"guide/data/#initialize-dvc","title":"Initialize DVC","text":"<pre><code>dvc init\ngit add .dvc .dvcignore\ngit commit -m \"Initialize DVC\"\n</code></pre>"},{"location":"guide/data/#track-data","title":"Track Data","text":"<pre><code>dvc add data/raw/cifar10\ndvc add data/raw/mnist\ngit add data/raw/cifar10.dvc data/raw/mnist.dvc\ngit commit -m \"Add dataset versions\"\n</code></pre>"},{"location":"guide/data/#remote-storage","title":"Remote Storage","text":"<p>Configure remote storage:</p> <pre><code># Local storage\ndvc remote add -d storage /path/to/storage\n\n# S3\ndvc remote add -d storage s3://bucket-name/path\n\n# Google Drive\ndvc remote add -d storage gdrive://folder-id\n</code></pre>"},{"location":"guide/data/#push-data","title":"Push Data","text":"<pre><code>dvc push\n</code></pre>"},{"location":"guide/data/#pull-data","title":"Pull Data","text":"<pre><code>dvc pull\n</code></pre>"},{"location":"guide/data/#data-statistics","title":"Data Statistics","text":"<p>Calculate dataset statistics:</p> <pre><code>from src.data.make_dataset import load_cifar10\nimport numpy as np\n\ntrain_loader, _, _ = load_cifar10(batch_size=1000)\n\nmeans = []\nstds = []\n\nfor images, _ in train_loader:\n    means.append(images.mean([0, 2, 3]))\n    stds.append(images.std([0, 2, 3]))\n\nmean = np.stack(means).mean(axis=0)\nstd = np.stack(stds).mean(axis=0)\n\nprint(f\"Mean: {mean}\")\nprint(f\"Std: {std}\")\n</code></pre>"},{"location":"guide/data/#custom-dataset","title":"Custom Dataset","text":"<p>To add custom datasets:</p> <ol> <li>Create dataset loader in <code>src/data/make_dataset.py</code></li> <li>Add configuration file in <code>configs/data/</code></li> <li>Update training script to support new dataset</li> </ol> <p>Example:</p> <pre><code>def load_custom_dataset(data_dir, batch_size=32, val_split=0.2):\n    dataset = ImageFolder(data_dir, transform=transform)\n    val_size = int(len(dataset) * val_split)\n    train_size = len(dataset) - val_size\n\n    train_set, val_set = torch.utils.data.random_split(\n        dataset, [train_size, val_size]\n    )\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"guide/data/#data-issues-solutions","title":"Data Issues &amp; Solutions","text":""},{"location":"guide/data/#disk-space","title":"Disk Space","text":"<ul> <li>CIFAR-10: ~170MB</li> <li>MNIST: ~12MB</li> <li>Ensure sufficient disk space before downloading</li> </ul>"},{"location":"guide/data/#network-issues","title":"Network Issues","text":"<ul> <li>Use <code>--offline</code> flag if data already downloaded</li> <li>Check internet connection for automatic downloads</li> </ul>"},{"location":"guide/data/#corrupted-files","title":"Corrupted Files","text":"<ul> <li>Delete cached files and re-download:   <pre><code>rm -rf data/raw\npython src/training/train.py data=cifar10\n</code></pre></li> </ul>"},{"location":"guide/experiments/","title":"Experiments &amp; Tracking","text":"<p>Track and manage machine learning experiments.</p>"},{"location":"guide/experiments/#experiment-configuration","title":"Experiment Configuration","text":""},{"location":"guide/experiments/#using-hydra-multi-run","title":"Using Hydra Multi-run","text":"<p>Run multiple experiments with different configurations:</p> <pre><code># Grid search over learning rates\npython src/training/train.py -m \\\n    hyperparameters.learning_rate=0.0001,0.001,0.01\n\n# Different models and datasets\npython src/training/train.py -m \\\n    model=simple_cnn,resnet \\\n    data=cifar10,mnist\n</code></pre>"},{"location":"guide/experiments/#output-structure","title":"Output Structure","text":"<p>Results are saved in <code>multirun/</code> with timestamps:</p> <pre><code>multirun/\n\u251c\u2500\u2500 2025-12-10/\n\u2502   \u251c\u2500\u2500 01-00-00/outputs/\n\u2502   \u251c\u2500\u2500 02-00-00/outputs/\n\u2502   \u2514\u2500\u2500 03-00-00/outputs/\n</code></pre>"},{"location":"guide/experiments/#comparing-results","title":"Comparing Results","text":""},{"location":"guide/experiments/#manual-comparison","title":"Manual Comparison","text":"<p>Compare metrics from different runs:</p> <pre><code>from pathlib import Path\nimport json\n\nruns_dir = Path(\"multirun/2025-12-10\")\n\nfor run_dir in sorted(runs_dir.iterdir()):\n    config_file = run_dir / \"outputs\" / \".hydra\" / \"config.yaml\"\n    if config_file.exists():\n        print(f\"\\nRun: {run_dir.name}\")\n        with open(config_file) as f:\n            print(f.read())\n</code></pre>"},{"location":"guide/experiments/#experiment-logs","title":"Experiment Logs","text":"<p>Training logs are saved in <code>outputs/</code> with timestamps:</p> <pre><code>outputs/\n\u251c\u2500\u2500 2025-12-10/\n\u2502   \u251c\u2500\u2500 .hydra/\n\u2502   \u2502   \u251c\u2500\u2500 config.yaml\n\u2502   \u2502   \u251c\u2500\u2500 hydra.yaml\n\u2502   \u2502   \u2514\u2500\u2500 launcher.yaml\n\u2502   \u2514\u2500\u2500 .hydra.log\n</code></pre>"},{"location":"guide/experiments/#model-checkpoints","title":"Model Checkpoints","text":"<p>Best models are saved during training:</p> <pre><code>models/\n\u251c\u2500\u2500 simple_cnn_best.pth\n\u251c\u2500\u2500 resnet_best.pth\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guide/experiments/#tracking-with-version-control","title":"Tracking with Version Control","text":""},{"location":"guide/experiments/#commit-experiment","title":"Commit Experiment","text":"<pre><code>git add .\ngit commit -m \"Exp: SimpleCNN on CIFAR-10, LR=0.001, 50% dropout\"\n</code></pre>"},{"location":"guide/experiments/#tag-important-runs","title":"Tag Important Runs","text":"<pre><code>git tag exp-001-baseline\ngit tag exp-002-with-augmentation\n</code></pre>"},{"location":"guide/experiments/#analysis-visualization","title":"Analysis &amp; Visualization","text":""},{"location":"guide/experiments/#plot-training-history","title":"Plot Training History","text":"<pre><code>import matplotlib.pyplot as plt\nfrom pathlib import Path\n\ndef plot_experiment(run_dir):\n    history_file = run_dir / \"reports\" / \"training_history.png\"\n    confusion_file = run_dir / \"reports\" / \"confusion_matrix.png\"\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n    # Load and display images\n    from PIL import Image\n    axes[0].imshow(Image.open(history_file))\n    axes[1].imshow(Image.open(confusion_file))\n\n    plt.show()\n</code></pre>"},{"location":"guide/experiments/#compare-multiple-runs","title":"Compare Multiple Runs","text":"<pre><code>import pandas as pd\nfrom pathlib import Path\n\ndef compare_runs(experiment_dir):\n    results = []\n\n    for run_dir in experiment_dir.iterdir():\n        config = run_dir / \".hydra\" / \"config.yaml\"\n        metrics_file = run_dir / \"reports\" / \"metrics.json\"\n\n        if config.exists() and metrics_file.exists():\n            results.append({\n                'run': run_dir.name,\n                'config': config,\n                'metrics': metrics_file\n            })\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"guide/experiments/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Name experiments clearly <pre><code>git commit -m \"Exp: SimpleCNN with batch norm, LR=0.001\"\n</code></pre></p> </li> <li> <p>Track hyperparameters in config</p> </li> <li>Use Hydra for consistent tracking</li> <li> <p>Save config.yaml with each run</p> </li> <li> <p>Save model and logs</p> </li> <li>All runs save to timestamped directories</li> <li> <p>Easy to reproduce past experiments</p> </li> <li> <p>Document findings</p> </li> <li>Keep experiment notes in README or wiki</li> <li> <p>Record best configurations</p> </li> <li> <p>Version control checkpoints <pre><code>dvc add models/simple_cnn_best.pth\ngit add models/simple_cnn_best.pth.dvc\n</code></pre></p> </li> </ol>"},{"location":"guide/experiments/#integration-with-mlflow-optional","title":"Integration with MLflow (Optional)","text":"<p>To add MLflow tracking:</p> <pre><code>pip install mlflow\n</code></pre> <p>Then modify <code>src/training/train.py</code>:</p> <pre><code>import mlflow\n\nwith mlflow.start_run():\n    mlflow.log_params(OmegaConf.to_container(cfg))\n    # ... training code ...\n    mlflow.log_metrics({'val_acc': val_acc})\n    mlflow.pytorch.log_model(model, \"models\")\n</code></pre> <p>View results:</p> <pre><code>mlflow ui\n</code></pre> <p>Then open <code>http://localhost:5000</code></p>"},{"location":"guide/inference/","title":"Making Predictions","text":"<p>This guide explains how to use trained models for inference.</p>"},{"location":"guide/inference/#batch-prediction","title":"Batch Prediction","text":"<p>Predict on a batch of images:</p> <pre><code>import torch\nfrom pathlib import Path\nfrom src.models.model import get_model\nfrom src.models.predict import predict_batch\nfrom src.data.make_dataset import load_cifar10\n\n# Load model\nmodel = get_model(\n    model_name=\"simple_cnn\",\n    num_classes=10,\n    input_channels=3,\n    image_size=32\n)\n\n# Load checkpoint\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Load data\n_, _, test_loader = load_cifar10(batch_size=32)\n\n# Make predictions\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\npredictions, true_labels = predict_batch(model, test_loader, device)\n</code></pre>"},{"location":"guide/inference/#single-image-prediction","title":"Single Image Prediction","text":"<p>Predict on a single image:</p> <pre><code>import torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom src.models.model import get_model\n\n# Load model\nmodel = get_model(\n    model_name=\"simple_cnn\",\n    num_classes=10,\n    input_channels=3,\n    image_size=32\n)\n\n# Load checkpoint\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Prepare image\ntransform = transforms.Compose([\n    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\nimage = Image.open(\"path/to/image.jpg\")\nimage_tensor = transform(image).unsqueeze(0)\n\n# Predict\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nimage_tensor = image_tensor.to(device)\n\nwith torch.no_grad():\n    output = model(image_tensor)\n    probabilities = torch.softmax(output, dim=1)\n    prediction = output.argmax(dim=1).item()\n    confidence = probabilities[0][prediction].item()\n\nprint(f\"Prediction: {prediction}, Confidence: {confidence:.2%}\")\n</code></pre>"},{"location":"guide/inference/#command-line-inference","title":"Command Line Inference","text":"<p>Use the prediction script:</p> <pre><code>python src/models/predict.py \\\n    --model_path models/simple_cnn_best.pth \\\n    --image_path path/to/image.jpg \\\n    --device cuda\n</code></pre>"},{"location":"guide/inference/#batch-processing","title":"Batch Processing","text":"<p>Process multiple images:</p> <pre><code>import torch\nfrom pathlib import Path\nfrom src.models.model import get_model\nfrom src.models.predict import predict_batch\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\n\n# Prepare data\ntransform = transforms.Compose([\n    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\ndataset = ImageFolder(\"path/to/images\", transform=transform)\ndata_loader = DataLoader(dataset, batch_size=32, num_workers=4)\n\n# Load model\nmodel = get_model(\n    model_name=\"simple_cnn\",\n    num_classes=10,\n    input_channels=3,\n    image_size=32\n)\n\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Predict\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\npredictions, labels = predict_batch(model, data_loader, device)\n</code></pre>"},{"location":"guide/inference/#output-format","title":"Output Format","text":"<p>Predictions are returned as: - predictions: Numpy array of predicted class indices - true_labels: Numpy array of true labels (if available) - probabilities: Softmax probabilities for each class</p>"},{"location":"guide/inference/#class-labels","title":"Class Labels","text":"<p>CIFAR-10 classes: - 0: airplane - 1: automobile - 2: bird - 3: cat - 4: deer - 5: dog - 6: frog - 7: horse - 8: ship - 9: truck</p> <p>MNIST classes: - 0-9: Digits</p>"},{"location":"guide/inference/#performance-metrics","title":"Performance Metrics","text":"<p>Calculate prediction metrics:</p> <pre><code>from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report\n)\n\naccuracy = accuracy_score(true_labels, predictions)\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\nf1 = f1_score(true_labels, predictions, average='macro')\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(true_labels, predictions))\n</code></pre>"},{"location":"guide/training/","title":"Training Models","text":"<p>This guide explains how to train image classification models using this project.</p>"},{"location":"guide/training/#basic-training","title":"Basic Training","text":""},{"location":"guide/training/#cifar-10","title":"CIFAR-10","text":"<p>Train on CIFAR-10 dataset with default configuration:</p> <pre><code>python src/training/train.py data=cifar10\n</code></pre>"},{"location":"guide/training/#mnist","title":"MNIST","text":"<p>Train on MNIST dataset:</p> <pre><code>python src/training/train.py data=mnist\n</code></pre>"},{"location":"guide/training/#configuration-override","title":"Configuration Override","text":"<p>Override hyperparameters via command line:</p> <pre><code># Custom learning rate\npython src/training/train.py hyperparameters.learning_rate=0.001\n\n# Custom batch size\npython src/training/train.py hyperparameters.batch_size=128\n\n# Different optimizer\npython src/training/train.py hyperparameters.optimizer=adam\n\n# Multiple overrides\npython src/training/train.py \\\n  data=cifar10 \\\n  hyperparameters.num_epochs=100 \\\n  hyperparameters.learning_rate=0.001 \\\n  model=resnet\n</code></pre>"},{"location":"guide/training/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":""},{"location":"guide/training/#step-decay","title":"Step Decay","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.use_scheduler=true \\\n  hyperparameters.scheduler_type=step \\\n  hyperparameters.step_size=30 \\\n  hyperparameters.gamma=0.1\n</code></pre>"},{"location":"guide/training/#cosine-annealing","title":"Cosine Annealing","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.use_scheduler=true \\\n  hyperparameters.scheduler_type=cosine \\\n  hyperparameters.min_lr=1e-5\n</code></pre>"},{"location":"guide/training/#reduce-on-plateau","title":"Reduce on Plateau","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.use_scheduler=true \\\n  hyperparameters.scheduler_type=plateau\n</code></pre>"},{"location":"guide/training/#model-selection","title":"Model Selection","text":""},{"location":"guide/training/#simplecnn","title":"SimpleCNN","text":"<pre><code>python src/training/train.py model=simple_cnn\n</code></pre>"},{"location":"guide/training/#resnet","title":"ResNet","text":"<pre><code>python src/training/train.py model=resnet\n</code></pre>"},{"location":"guide/training/#training-output","title":"Training Output","text":"<p>Training generates: - Best model: <code>models/simple_cnn_best.pth</code> - Reports: <code>reports/</code> directory with:   - <code>confusion_matrix.png</code> - Confusion matrix visualization   - <code>classification_report.txt</code> - Detailed metrics   - <code>training_history.png</code> - Loss and accuracy curves</p>"},{"location":"guide/training/#early-stopping","title":"Early Stopping","text":"<p>Configure early stopping patience:</p> <pre><code>python src/training/train.py early_stopping_patience=10\n</code></pre>"},{"location":"guide/training/#device-configuration","title":"Device Configuration","text":""},{"location":"guide/training/#auto-detect-default","title":"Auto-detect (default)","text":"<pre><code>python src/training/train.py device=auto\n</code></pre>"},{"location":"guide/training/#explicit-gpu","title":"Explicit GPU","text":"<pre><code>python src/training/train.py device=cuda:0\n</code></pre>"},{"location":"guide/training/#cpu-only","title":"CPU only","text":"<pre><code>python src/training/train.py device=cpu\n</code></pre>"},{"location":"guide/training/#monitoring-training","title":"Monitoring Training","text":"<p>The training script provides real-time updates: - Loss and accuracy per epoch - Learning rate schedule - Best validation accuracy - Training history plots</p>"},{"location":"guide/training/#tips-for-better-results","title":"Tips for Better Results","text":"<ol> <li>Data augmentation: Adjust in <code>src/data/make_dataset.py</code></li> <li>Batch size: Larger batches for better GPU utilization</li> <li>Learning rate: Start with 0.001 and tune based on results</li> <li>Epochs: Use early stopping to avoid overfitting</li> <li>Optimizer: Adam for adaptive learning, SGD for stable training</li> </ol>"},{"location":"mlops/cicd/","title":"CI/CD Pipeline","text":"<p>This project uses GitHub Actions for Continuous Integration and Continuous Deployment (CI/CD).</p>"},{"location":"mlops/cicd/#overview","title":"Overview","text":"<p>The CI/CD pipeline consists of two main workflows:</p> <ol> <li>Tests &amp; Linting (<code>tests.yaml</code>) - Runs on every push and PR</li> <li>Continuous ML (<code>cml.yaml</code>) - Trains models and generates reports</li> </ol>"},{"location":"mlops/cicd/#workflow-1-tests-and-linting","title":"Workflow 1: Tests and Linting","text":""},{"location":"mlops/cicd/#triggered-on","title":"Triggered On","text":"<ul> <li>Push to <code>main</code> or <code>develop</code> branches</li> <li>Pull requests to <code>main</code> or <code>develop</code></li> </ul>"},{"location":"mlops/cicd/#what-it-does","title":"What It Does","text":"<pre><code>graph LR\n    A[Push/PR] --&gt; B[Setup Python]\n    B --&gt; C[Install Dependencies]\n    C --&gt; D[Lint Code]\n    D --&gt; E[Run Tests]\n    E --&gt; F[Coverage Report]</code></pre>"},{"location":"mlops/cicd/#jobs","title":"Jobs","text":""},{"location":"mlops/cicd/#1-code-quality-checks","title":"1. Code Quality Checks","text":"<p>Flake8 - Syntax and style checking: <pre><code>flake8 src tests --max-line-length=127\n</code></pre></p> <p>Black - Code formatting: <pre><code>black --check src tests\n</code></pre></p> <p>isort - Import sorting: <pre><code>isort --check-only src tests\n</code></pre></p> <p>mypy - Type checking: <pre><code>mypy src --ignore-missing-imports\n</code></pre></p>"},{"location":"mlops/cicd/#2-unit-tests","title":"2. Unit Tests","text":"<p>Runs all tests across multiple Python versions:</p> <ul> <li>Python 3.8</li> <li>Python 3.9</li> <li>Python 3.10</li> </ul> <pre><code>pytest tests/ -v --tb=short -m \"not slow\"\n</code></pre>"},{"location":"mlops/cicd/#3-coverage-report","title":"3. Coverage Report","text":"<p>Generates code coverage and uploads to Codecov:</p> <pre><code>pytest tests/ --cov=src --cov-report=xml\n</code></pre>"},{"location":"mlops/cicd/#configuration-file","title":"Configuration File","text":"<p><code>.github/workflows/tests.yaml</code>:</p> <pre><code>name: CI - Tests and Linting\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main, develop ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\"]\n    # ... (see full file)\n</code></pre>"},{"location":"mlops/cicd/#workflow-2-continuous-ml-cml","title":"Workflow 2: Continuous ML (CML)","text":""},{"location":"mlops/cicd/#triggered-on_1","title":"Triggered On","text":"<ul> <li>Push to <code>main</code> or <code>develop</code></li> <li>Pull requests to <code>main</code></li> </ul>"},{"location":"mlops/cicd/#what-it-does_1","title":"What It Does","text":"<pre><code>graph TD\n    A[Trigger] --&gt; B[Setup Environment]\n    B --&gt; C[Train Model]\n    C --&gt; D[Generate Metrics]\n    D --&gt; E[Create Confusion Matrix]\n    E --&gt; F[Generate Report]\n    F --&gt; G[Post PR Comment]</code></pre>"},{"location":"mlops/cicd/#steps","title":"Steps","text":"<ol> <li>Setup Environment</li> <li>Install Python and dependencies</li> <li> <p>Pull data with DVC (if configured)</p> </li> <li> <p>Train Model <pre><code>python src/training/train.py \\\n  hyperparameters=fast \\\n  hyperparameters.num_epochs=3\n</code></pre></p> </li> <li> <p>Generate Metrics</p> </li> <li>Classification report</li> <li>Confusion matrix</li> <li> <p>Training history plots</p> </li> <li> <p>Create CML Report</p> </li> <li>Compiles all metrics into markdown</li> <li>Posts as PR comment</li> <li>Uploads artifacts</li> </ol>"},{"location":"mlops/cicd/#example-report","title":"Example Report","text":"<p>When you create a PR, CML automatically posts:</p> <p><pre><code># Model Training Report\n\n## Training Configuration\n- Epochs: 3 (fast config for CI)\n- Batch Size: 128\n- Dataset: CIFAR-10\n\n## Metrics\n</code></pre> Classification Report ================================================================================</p> <pre><code>          precision    recall  f1-score   support\n\nairplane     0.7234    0.6821    0.7021      1000\n</code></pre> <p>automobile     0.8123    0.7912    0.8016      1000         bird     0.6234    0.5821    0.6019      1000 ...</p> <pre><code>accuracy                         0.7234     10000\n</code></pre> <p>``` </p>"},{"location":"mlops/cicd/#confusion-matrix","title":"Confusion Matrix","text":""},{"location":"mlops/cicd/#training-history","title":"Training History","text":"<p> ```</p>"},{"location":"mlops/cicd/#configuration-file_1","title":"Configuration File","text":"<p><code>.github/workflows/cml.yaml</code>:</p> <pre><code>name: CML - Continuous Machine Learning\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  train-and-report:\n    runs-on: ubuntu-latest\n    steps:\n    # ... (see full file)\n</code></pre>"},{"location":"mlops/cicd/#setting-up-cicd","title":"Setting Up CI/CD","text":""},{"location":"mlops/cicd/#1-enable-github-actions","title":"1. Enable GitHub Actions","text":"<p>GitHub Actions is enabled by default for public repositories.</p> <p>For private repos: 1. Go to Settings \u2192 Actions \u2192 General 2. Enable \"Allow all actions and reusable workflows\"</p>"},{"location":"mlops/cicd/#2-add-secrets-if-needed","title":"2. Add Secrets (if needed)","text":"<p>For DVC remote storage or other services:</p> <ol> <li>Go to Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Add secrets:</li> <li><code>DVC_REMOTE_URL</code> (if using DVC)</li> <li><code>AWS_ACCESS_KEY_ID</code> (if using S3)</li> <li>etc.</li> </ol>"},{"location":"mlops/cicd/#3-configure-dvc-remote-optional","title":"3. Configure DVC Remote (Optional)","text":"<p>If using DVC:</p> <pre><code># Local setup\ndvc remote add -d storage s3://mybucket/path\n\n# In GitHub Actions, add secret\n# Then modify workflow:\n- name: Configure DVC\n  env:\n    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n  run: dvc pull\n</code></pre>"},{"location":"mlops/cicd/#customization","title":"Customization","text":""},{"location":"mlops/cicd/#modify-test-configuration","title":"Modify Test Configuration","text":"<p>Edit <code>.github/workflows/tests.yaml</code>:</p> <pre><code># Change Python versions\nstrategy:\n  matrix:\n    python-version: [\"3.9\", \"3.10\", \"3.11\"]\n\n# Add more lint checks\n- name: Run pylint\n  run: pylint src\n</code></pre>"},{"location":"mlops/cicd/#modify-cml-configuration","title":"Modify CML Configuration","text":"<p>Edit <code>.github/workflows/cml.yaml</code>:</p> <pre><code># Train for more epochs\n- name: Train model\n  run: |\n    python src/training/train.py \\\n      hyperparameters.num_epochs=10\n</code></pre>"},{"location":"mlops/cicd/#add-more-jobs","title":"Add More Jobs","text":"<pre><code>deploy:\n  needs: test\n  runs-on: ubuntu-latest\n  if: github.ref == 'refs/heads/main'\n  steps:\n    - name: Deploy model\n      run: |\n        # Your deployment script\n</code></pre>"},{"location":"mlops/cicd/#badges","title":"Badges","text":"<p>Add status badges to your README:</p> <pre><code>![Tests](https://github.com/username/repo/workflows/CI%20-%20Tests%20and%20Linting/badge.svg)\n![CML](https://github.com/username/repo/workflows/CML%20-%20Continuous%20Machine%20Learning/badge.svg)\n[![codecov](https://codecov.io/gh/username/repo/branch/main/graph/badge.svg)](https://codecov.io/gh/username/repo)\n</code></pre>"},{"location":"mlops/cicd/#monitoring","title":"Monitoring","text":""},{"location":"mlops/cicd/#view-workflow-runs","title":"View Workflow Runs","text":"<ol> <li>Go to the \"Actions\" tab in your repository</li> <li>Click on a workflow run to see details</li> <li>View logs for each job</li> </ol>"},{"location":"mlops/cicd/#check-coverage","title":"Check Coverage","text":"<p>If you set up Codecov: - Visit <code>https://codecov.io/gh/username/repo</code> - View coverage trends and reports</p>"},{"location":"mlops/cicd/#best-practices","title":"Best Practices","text":"<ol> <li>Keep CI Fast</li> <li>Use fast config for CI</li> <li>Cache dependencies</li> <li> <p>Run slow tests separately</p> </li> <li> <p>Fail Fast</p> </li> <li>Run linting before tests</li> <li> <p>Use <code>continue-on-error: false</code> for critical jobs</p> </li> <li> <p>Artifact Management</p> </li> <li>Upload trained models</li> <li>Save reports for later review</li> <li> <p>Use appropriate retention days</p> </li> <li> <p>Security</p> </li> <li>Never commit secrets</li> <li>Use GitHub Secrets</li> <li>Limit workflow permissions</li> </ol>"},{"location":"mlops/cicd/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mlops/cicd/#tests-failing-locally-but-passing-in-ci","title":"Tests Failing Locally But Passing in CI","text":"<ul> <li>Check Python version</li> <li>Clear cache: <code>pip cache purge</code></li> <li>Check for environment-specific code</li> </ul>"},{"location":"mlops/cicd/#cml-report-not-appearing","title":"CML Report Not Appearing","text":"<ul> <li>Check GitHub token permissions</li> <li>Verify CML installation</li> <li>Check workflow logs</li> </ul>"},{"location":"mlops/cicd/#out-of-memory-in-ci","title":"Out of Memory in CI","text":"<ul> <li>Reduce batch size in fast config</li> <li>Use smaller model</li> <li>Request more resources</li> </ul>"},{"location":"mlops/cicd/#advanced-topics","title":"Advanced Topics","text":""},{"location":"mlops/cicd/#matrix-strategy","title":"Matrix Strategy","text":"<p>Test multiple configurations:</p> <pre><code>strategy:\n  matrix:\n    python-version: [3.8, 3.9, 3.10]\n    os: [ubuntu-latest, windows-latest, macos-latest]\n</code></pre>"},{"location":"mlops/cicd/#caching","title":"Caching","text":"<p>Speed up workflows with caching:</p> <pre><code>- uses: actions/cache@v3\n  with:\n    path: ~/.cache/pip\n    key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}\n</code></pre>"},{"location":"mlops/cicd/#scheduled-runs","title":"Scheduled Runs","text":"<p>Run tests nightly:</p> <pre><code>on:\n  schedule:\n    - cron: '0 0 * * *'  # Every day at midnight\n</code></pre>"},{"location":"mlops/cicd/#learn-more","title":"Learn More","text":"<ul> <li>GitHub Actions Documentation</li> <li>CML Documentation</li> <li>Codecov Documentation</li> </ul>"},{"location":"mlops/docker/","title":"Docker Containerization","text":"<p>Package and deploy the application using Docker.</p>"},{"location":"mlops/docker/#overview","title":"Overview","text":"<p>Docker containerizes the application with all dependencies for consistent deployment across environments.</p>"},{"location":"mlops/docker/#dockerfile","title":"Dockerfile","text":"<p>Our multi-stage Dockerfile optimizes image size:</p> <pre><code># Build stage\nFROM python:3.11-slim as builder\n\nWORKDIR /app\n\n# Install build dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\n\n# Create wheels\nRUN pip install --no-cache-dir wheel &amp;&amp; \\\n    pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt\n\n# Runtime stage\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Install runtime dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    libsm6 libxext6 libxrender-dev \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy wheels from builder\nCOPY --from=builder /app/wheels /wheels\nCOPY --from=builder /app/requirements.txt .\n\n# Install Python packages\nRUN pip install --no-cache /wheels/*\n\n# Copy application\nCOPY . .\n\n# Install package\nRUN pip install -e .\n\n# Create necessary directories\nRUN mkdir -p data models reports outputs\n\n# Set entrypoint\nENTRYPOINT [\"python\", \"src/training/train.py\"]\n</code></pre>"},{"location":"mlops/docker/#building-images","title":"Building Images","text":""},{"location":"mlops/docker/#build-image","title":"Build Image","text":"<pre><code># Build with default tag\ndocker build -t image-classifier .\n\n# Build with custom tag\ndocker build -t image-classifier:v1.0 .\n\n# Build with build args\ndocker build --build-arg PYTHON_VERSION=3.10 -t image-classifier .\n</code></pre>"},{"location":"mlops/docker/#view-images","title":"View Images","text":"<pre><code># List images\ndocker images | grep image-classifier\n\n# View image info\ndocker inspect image-classifier\n</code></pre>"},{"location":"mlops/docker/#running-containers","title":"Running Containers","text":""},{"location":"mlops/docker/#basic-training","title":"Basic Training","text":"<pre><code># Run default training (CIFAR-10)\ndocker run --rm image-classifier\n\n# Run with GPU support\ndocker run --rm --gpus all image-classifier\n\n# Run on MNIST\ndocker run --rm image-classifier data=mnist\n</code></pre>"},{"location":"mlops/docker/#with-volume-mounts","title":"With Volume Mounts","text":"<pre><code># Mount data directory\ndocker run --rm \\\n    -v /home/user/data:/app/data \\\n    image-classifier\n\n# Mount models directory\ndocker run --rm \\\n    -v /home/user/models:/app/models \\\n    image-classifier\n\n# Mount all project directories\ndocker run --rm \\\n    -v /home/user/project:/app \\\n    image-classifier\n</code></pre>"},{"location":"mlops/docker/#interactive-mode","title":"Interactive Mode","text":"<pre><code># Open bash shell\ndocker run --rm -it \\\n    --entrypoint /bin/bash \\\n    image-classifier\n\n# Run custom command\ndocker run --rm -it \\\n    --entrypoint python \\\n    image-classifier \\\n    -c \"from src.models.model import get_model; print(get_model('simple_cnn'))\"\n</code></pre>"},{"location":"mlops/docker/#environmental-variables","title":"Environmental Variables","text":"<pre><code># Pass configuration via environment\ndocker run --rm \\\n    -e CUDA_VISIBLE_DEVICES=0 \\\n    -e PYTHONUNBUFFERED=1 \\\n    image-classifier\n</code></pre>"},{"location":"mlops/docker/#environment-variables","title":"Environment Variables","text":""},{"location":"mlops/docker/#useful-variables","title":"Useful Variables","text":"<pre><code># Python settings\nPYTHONUNBUFFERED=1        # Unbuffered output\nPYTHONDONTWRITEBYTECODE=1 # Don't create .pyc files\n\n# GPU settings\nCUDA_VISIBLE_DEVICES=0    # Use specific GPU\n\n# Application settings\nLOG_LEVEL=INFO            # Logging level\n</code></pre>"},{"location":"mlops/docker/#multi-gpu-training","title":"Multi-GPU Training","text":""},{"location":"mlops/docker/#single-gpu","title":"Single GPU","text":"<pre><code>docker run --rm --gpus device=0 image-classifier\n</code></pre>"},{"location":"mlops/docker/#multiple-gpus","title":"Multiple GPUs","text":"<pre><code># GPU 0 and 1\ndocker run --rm --gpus '\"device=0,1\"' image-classifier\n</code></pre>"},{"location":"mlops/docker/#all-gpus","title":"All GPUs","text":"<pre><code>docker run --rm --gpus all image-classifier\n</code></pre>"},{"location":"mlops/docker/#docker-compose","title":"Docker Compose","text":""},{"location":"mlops/docker/#multi-service-setup","title":"Multi-service Setup","text":"<p>Create <code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\n\nservices:\n  training:\n    build: .\n    image: image-classifier:latest\n    container_name: image-classifier-train\n    volumes:\n      - ./data:/app/data\n      - ./models:/app/models\n      - ./reports:/app/reports\n    environment:\n      - CUDA_VISIBLE_DEVICES=0\n    command: data=cifar10 hyperparameters.num_epochs=50\n\n  inference:\n    build: .\n    image: image-classifier:latest\n    container_name: image-classifier-inference\n    volumes:\n      - ./models:/app/models\n    ports:\n      - \"8000:8000\"\n    command: python src/api/serve.py\n    depends_on:\n      - training\n</code></pre>"},{"location":"mlops/docker/#run-with-docker-compose","title":"Run with Docker Compose","text":"<pre><code># Start services\ndocker-compose up\n\n# Run specific service\ndocker-compose up training\n\n# Background mode\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f training\n\n# Stop services\ndocker-compose down\n</code></pre>"},{"location":"mlops/docker/#publishing-images","title":"Publishing Images","text":""},{"location":"mlops/docker/#docker-hub","title":"Docker Hub","text":"<pre><code># Tag image\ndocker tag image-classifier:latest yourusername/image-classifier:latest\n\n# Login to Docker Hub\ndocker login\n\n# Push image\ndocker push yourusername/image-classifier:latest\n\n# Pull image\ndocker pull yourusername/image-classifier:latest\n</code></pre>"},{"location":"mlops/docker/#performance-optimization","title":"Performance Optimization","text":""},{"location":"mlops/docker/#reduce-image-size","title":"Reduce Image Size","text":"<p>Current optimizations: - Multi-stage build (~500MB final image) - Python slim base image - Remove build dependencies - Minimal runtime dependencies</p>"},{"location":"mlops/docker/#speed-up-builds","title":"Speed Up Builds","text":"<pre><code># Use BuildKit\nDOCKER_BUILDKIT=1 docker build -t image-classifier .\n\n# Cache layers efficiently\n# Put dependencies early in Dockerfile\n# Order: base image \u2192 system deps \u2192 python deps \u2192 app code\n</code></pre>"},{"location":"mlops/docker/#debugging","title":"Debugging","text":""},{"location":"mlops/docker/#view-image-layers","title":"View Image Layers","text":"<pre><code># List image history\ndocker history image-classifier\n\n# Inspect layer details\ndocker inspect image-classifier\n</code></pre>"},{"location":"mlops/docker/#run-with-debugging","title":"Run with Debugging","text":"<pre><code># Keep container running\ndocker run --rm -it \\\n    --entrypoint /bin/bash \\\n    image-classifier\n\n# Inside container\npython -m pdb src/training/train.py\n</code></pre>"},{"location":"mlops/docker/#view-container-logs","title":"View Container Logs","text":"<pre><code># Run and see logs\ndocker run --rm image-classifier\n\n# View logs of stopped container\ndocker logs container-id\n</code></pre>"},{"location":"mlops/docker/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"mlops/docker/#github-actions","title":"GitHub Actions","text":"<pre><code>name: Build Docker Image\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Build image\n        run: docker build -t image-classifier .\n\n      - name: Run tests in container\n        run: docker run --rm image-classifier pytest\n</code></pre>"},{"location":"mlops/docker/#cleanup","title":"Cleanup","text":""},{"location":"mlops/docker/#remove-images-and-containers","title":"Remove Images and Containers","text":"<pre><code># Remove container\ndocker rm container-id\n\n# Remove image\ndocker rmi image-classifier\n\n# Remove unused images\ndocker image prune\n\n# Remove everything\ndocker system prune -a\n</code></pre>"},{"location":"mlops/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mlops/docker/#out-of-memory","title":"Out of Memory","text":"<pre><code># Limit memory usage\ndocker run --rm -m 4g image-classifier\n\n# Check memory usage\ndocker stats\n</code></pre>"},{"location":"mlops/docker/#gpu-not-available","title":"GPU Not Available","text":"<pre><code># Check GPU access\ndocker run --rm --gpus all nvidia-smi\n\n# Verify nvidia-docker installation\nnvidia-docker --version\n</code></pre>"},{"location":"mlops/docker/#large-build-sizes","title":"Large Build Sizes","text":"<pre><code># Use .dockerignore to exclude files\n# Similar to .gitignore\n\n# Check what's included\ndocker build --progress=plain .\n</code></pre>"},{"location":"mlops/docker/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use specific base image versions <pre><code>FROM python:3.11-slim  # \u2713 Good (specific)\nFROM python:3.11      # \u2717 Could be unstable\n</code></pre></p> </li> <li> <p>Minimize layers <pre><code># \u2713 Good - fewer layers\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y curl &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# \u2717 Bad - more layers\nRUN apt-get update\nRUN apt-get install -y curl\nRUN rm -rf /var/lib/apt/lists/*\n</code></pre></p> </li> <li> <p>Use .dockerignore <pre><code>__pycache__\n*.pyc\n.git\n.gitignore\nREADME.md\n.venv\n</code></pre></p> </li> <li> <p>Set proper entrypoint <pre><code># \u2713 Good - exec form (PID 1)\nENTRYPOINT [\"python\", \"src/training/train.py\"]\n\n# \u2717 Bad - shell form\nENTRYPOINT python src/training/train.py\n</code></pre></p> </li> </ol>"},{"location":"mlops/docker/#references","title":"References","text":"<ul> <li>Docker Documentation</li> <li>Docker Best Practices</li> <li>Python Docker Best Practices</li> <li>nvidia-docker</li> </ul>"},{"location":"mlops/dvc/","title":"Data Versioning with DVC","text":"<p>Learn how to use Data Version Control (DVC) to track datasets, models, and experiments.</p>"},{"location":"mlops/dvc/#why-dvc","title":"Why DVC?","text":"<ul> <li>Version large files - Git can't handle large datasets efficiently</li> <li>Share data - Use remote storage (S3, Google Drive, etc.)</li> <li>Reproduce experiments - Track exact data versions</li> <li>Save storage - Don't duplicate data across branches</li> </ul>"},{"location":"mlops/dvc/#installation","title":"Installation","text":"<p>DVC is already included in <code>requirements.txt</code>. If not installed:</p> <pre><code>pip install dvc dvc-gdrive  # Or dvc-s3, dvc-azure, etc.\n</code></pre>"},{"location":"mlops/dvc/#quick-start","title":"Quick Start","text":""},{"location":"mlops/dvc/#1-initialize-dvc","title":"1. Initialize DVC","text":"<pre><code>dvc init\ngit add .dvc .dvcignore\ngit commit -m \"Initialize DVC\"\n</code></pre>"},{"location":"mlops/dvc/#2-track-data","title":"2. Track Data","text":"<pre><code># Track the data directory\ndvc add data/raw\n\n# This creates data/raw.dvc\ngit add data/raw.dvc data/.gitignore\ngit commit -m \"Track data with DVC\"\n</code></pre>"},{"location":"mlops/dvc/#3-setup-remote-storage","title":"3. Setup Remote Storage","text":"<p>Choose a remote storage backend:</p> Google DriveAWS S3Local Storage <pre><code>dvc remote add -d storage gdrive://FOLDER_ID\ngit add .dvc/config\ngit commit -m \"Configure remote storage\"\n</code></pre> <pre><code>dvc remote add -d storage s3://mybucket/path\ndvc remote modify storage region us-west-2\n</code></pre> <pre><code>dvc remote add -d storage /tmp/dvc-storage\n# Or network drive\ndvc remote add -d storage /mnt/shared/dvc-storage\n</code></pre>"},{"location":"mlops/dvc/#4-push-data","title":"4. Push Data","text":"<pre><code>dvc push\n</code></pre>"},{"location":"mlops/dvc/#common-workflows","title":"Common Workflows","text":""},{"location":"mlops/dvc/#clone-repository-and-get-data","title":"Clone Repository and Get Data","text":"<pre><code># Clone repo\ngit clone https://github.com/username/repo.git\ncd repo\n\n# Install dependencies\npip install -r requirements.txt\n\n# Pull data\ndvc pull\n</code></pre>"},{"location":"mlops/dvc/#update-dataset","title":"Update Dataset","text":"<pre><code># 1. Modify your data\ncp new_data/* data/raw/\n\n# 2. Update DVC tracking\ndvc add data/raw\n\n# 3. Commit changes\ngit add data/raw.dvc\ngit commit -m \"Update dataset with new images\"\n\n# 4. Push to remote\ndvc push\ngit push\n</code></pre>"},{"location":"mlops/dvc/#switch-between-versions","title":"Switch Between Versions","text":"<pre><code># Go to a specific commit\ngit checkout &lt;commit-hash&gt;\n\n# Pull the corresponding data\ndvc pull\n</code></pre>"},{"location":"mlops/dvc/#check-status","title":"Check Status","text":"<pre><code># Check what's changed\ndvc status\n\n# Check DVC cache\ndvc cache dir\n</code></pre>"},{"location":"mlops/dvc/#track-models","title":"Track Models","text":""},{"location":"mlops/dvc/#track-trained-models","title":"Track Trained Models","text":"<pre><code># Add models directory\ndvc add models/\n\ngit add models.dvc\ngit commit -m \"Track trained models\"\ndvc push\n</code></pre>"},{"location":"mlops/dvc/#version-models-with-experiments","title":"Version Models with Experiments","text":"<p>Create a pipeline:</p> <pre><code># dvc.yaml\nstages:\n  train:\n    cmd: python src/training/train.py\n    deps:\n      - src/training/train.py\n      - data/raw\n    params:\n      - config.yaml:hyperparameters.learning_rate\n      - config.yaml:hyperparameters.num_epochs\n    outs:\n      - models/simple_cnn_best.pth\n    metrics:\n      - reports/classification_report.txt:\n          cache: false\n</code></pre> <p>Run pipeline:</p> <pre><code>dvc repro\n</code></pre>"},{"location":"mlops/dvc/#remote-storage-options","title":"Remote Storage Options","text":""},{"location":"mlops/dvc/#google-drive","title":"Google Drive","text":"<ol> <li>Create a folder in Google Drive</li> <li>Get the folder ID from the URL</li> <li>Configure DVC:</li> </ol> <pre><code>dvc remote add -d storage gdrive://FOLDER_ID\n</code></pre> <p>First push will open browser for authentication.</p>"},{"location":"mlops/dvc/#aws-s3","title":"AWS S3","text":"<pre><code># Add remote\ndvc remote add -d storage s3://mybucket/path\n\n# Configure credentials\ndvc remote modify storage access_key_id YOUR_KEY\ndvc remote modify storage secret_access_key YOUR_SECRET\n\n# Or use AWS CLI credentials\nexport AWS_ACCESS_KEY_ID=xxx\nexport AWS_SECRET_ACCESS_KEY=xxx\n</code></pre>"},{"location":"mlops/dvc/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code>dvc remote add -d storage azure://container/path\ndvc remote modify storage account_name myaccount\n</code></pre>"},{"location":"mlops/dvc/#sshlocal","title":"SSH/Local","text":"<pre><code># SSH\ndvc remote add -d storage ssh://user@server:/path/to/storage\n\n# Local or network drive\ndvc remote add -d storage /mnt/shared/dvc-storage\n</code></pre>"},{"location":"mlops/dvc/#dvc-with-github-actions","title":"DVC with GitHub Actions","text":""},{"location":"mlops/dvc/#setup","title":"Setup","text":"<p>Add secrets to GitHub:</p> <ol> <li>Go to Settings \u2192 Secrets</li> <li>Add necessary credentials (AWS keys, etc.)</li> </ol>"},{"location":"mlops/dvc/#workflow-example","title":"Workflow Example","text":"<pre><code>name: DVC Pipeline\n\non: [push]\n\njobs:\n  train:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n\n      - name: Pull data\n        env:\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        run: |\n          dvc pull\n\n      - name: Train model\n        run: |\n          dvc repro\n\n      - name: Push results\n        run: |\n          dvc push\n</code></pre>"},{"location":"mlops/dvc/#best-practices","title":"Best Practices","text":""},{"location":"mlops/dvc/#1-organize-data","title":"1. Organize Data","text":"<pre><code>data/\n\u251c\u2500\u2500 raw/           # Original immutable data - track with DVC\n\u251c\u2500\u2500 processed/     # Processed data - track with DVC\n\u2514\u2500\u2500 interim/       # Temporary data - don't track\n</code></pre>"},{"location":"mlops/dvc/#2-use-dvcignore","title":"2. Use .dvcignore","text":"<p>Exclude files you don't want to track:</p> <pre><code># .dvcignore\n*.log\n*.tmp\n__pycache__/\n.ipynb_checkpoints/\n</code></pre>"},{"location":"mlops/dvc/#3-track-different-versions","title":"3. Track Different Versions","text":"<pre><code># Tag important versions\ngit tag -a v1.0 -m \"Dataset version 1.0\"\ngit push --tags\ndvc push\n</code></pre>"},{"location":"mlops/dvc/#4-share-specific-versions","title":"4. Share Specific Versions","text":"<pre><code># Create branch for experiment\ngit checkout -b experiment/new_data\ndvc add data/raw\ngit add data/raw.dvc\ngit commit -m \"Add new dataset\"\ndvc push\ngit push -u origin experiment/new_data\n</code></pre>"},{"location":"mlops/dvc/#advanced-features","title":"Advanced Features","text":""},{"location":"mlops/dvc/#dvc-pipelines","title":"DVC Pipelines","text":"<p>Define reproducible workflows:</p> <pre><code># dvc.yaml\nstages:\n  preprocess:\n    cmd: python src/data/preprocess.py\n    deps:\n      - data/raw\n    outs:\n      - data/processed\n\n  train:\n    cmd: python src/training/train.py\n    deps:\n      - data/processed\n      - src/training/train.py\n    params:\n      - configs/config.yaml:hyperparameters\n    outs:\n      - models/model.pth\n    metrics:\n      - reports/metrics.json:\n          cache: false\n</code></pre> <p>Run entire pipeline:</p> <pre><code>dvc repro\n</code></pre>"},{"location":"mlops/dvc/#experiments","title":"Experiments","text":"<p>Track experiments:</p> <pre><code># Run experiment\ndvc exp run\n\n# Compare experiments\ndvc exp show\n\n# Apply best experiment\ndvc exp apply &lt;exp-name&gt;\n</code></pre>"},{"location":"mlops/dvc/#metrics-tracking","title":"Metrics Tracking","text":"<pre><code># dvc.yaml\nstages:\n  train:\n    metrics:\n      - reports/metrics.json:\n          cache: false\n    plots:\n      - reports/confusion_matrix.png:\n          cache: false\n</code></pre> <p>Compare metrics:</p> <pre><code>dvc metrics show\ndvc metrics diff\n</code></pre>"},{"location":"mlops/dvc/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mlops/dvc/#large-files-not-tracked","title":"Large Files Not Tracked","text":"<p>Check <code>.gitignore</code>:</p> <pre><code>cat data/.gitignore\n# Should contain /raw (added by dvc add)\n</code></pre>"},{"location":"mlops/dvc/#permission-denied","title":"Permission Denied","text":"<p>For remote storage, check credentials:</p> <pre><code>dvc remote list\ndvc remote modify storage --local access_key_id YOUR_KEY\n</code></pre>"},{"location":"mlops/dvc/#cache-issues","title":"Cache Issues","text":"<p>Clear and rebuild cache:</p> <pre><code>dvc cache dir  # Show cache location\nrm -rf .dvc/cache\ndvc pull\n</code></pre>"},{"location":"mlops/dvc/#slow-pushpull","title":"Slow Push/Pull","text":"<p>Use parallel transfers:</p> <pre><code>dvc config cache.type symlink\ndvc push --jobs 4\n</code></pre>"},{"location":"mlops/dvc/#dvc-vs-git-lfs","title":"DVC vs Git LFS","text":"Feature DVC Git LFS Storage backends Many (S3, Azure, GDrive, SSH, etc.) Git LFS server Versioning Full versioning Full versioning Pipeline support Yes No Metrics tracking Yes No Free tier Depends on storage GitHub: 1GB"},{"location":"mlops/dvc/#learn-more","title":"Learn More","text":"<ul> <li>DVC Documentation</li> <li>DVC Tutorial</li> <li>DVC with CI/CD</li> <li>Example Projects</li> </ul>"},{"location":"mlops/dvc/#summary","title":"Summary","text":"<pre><code># Essential commands\ndvc init              # Initialize DVC\ndvc add data/         # Track data\ndvc push              # Upload to remote\ndvc pull              # Download from remote\ndvc status            # Check status\ndvc repro             # Run pipeline\n</code></pre>"},{"location":"mlops/model-versioning/","title":"Model Versioning","text":"<p>Manage and version trained models for reproducibility and deployment.</p>"},{"location":"mlops/model-versioning/#model-storage","title":"Model Storage","text":""},{"location":"mlops/model-versioning/#local-storage","title":"Local Storage","text":"<p>Models are saved to <code>models/</code> directory:</p> <pre><code>models/\n\u251c\u2500\u2500 simple_cnn_best.pth      # Best SimpleCNN checkpoint\n\u251c\u2500\u2500 resnet_best.pth          # Best ResNet checkpoint\n\u2514\u2500\u2500 archived/\n    \u251c\u2500\u2500 simple_cnn_v1.pth    # Previous version\n    \u2514\u2500\u2500 resnet_v1.pth\n</code></pre>"},{"location":"mlops/model-versioning/#model-checkpoint-format","title":"Model Checkpoint Format","text":"<p>Each checkpoint contains:</p> <pre><code>{\n    'epoch': int,                       # Training epoch\n    'model_state_dict': dict,          # Model weights\n    'optimizer_state_dict': dict,      # Optimizer state\n    'val_acc': float,                  # Validation accuracy\n    'config': dict                     # Training configuration\n}\n</code></pre>"},{"location":"mlops/model-versioning/#model-versioning-with-git","title":"Model Versioning with Git","text":""},{"location":"mlops/model-versioning/#tag-model-releases","title":"Tag Model Releases","text":"<pre><code># Create tag for model version\ngit tag model-v1.0 -m \"SimpleCNN on CIFAR-10: 85% accuracy\"\n\n# List tags\ngit tag -l\n\n# Push tags\ngit push origin --tags\n</code></pre>"},{"location":"mlops/model-versioning/#store-model-info","title":"Store Model Info","text":"<p>Create <code>MODELS.md</code> to track model versions:</p> <pre><code># Model Registry\n\n## SimpleCNN v1.0\n- Dataset: CIFAR-10\n- Accuracy: 85.2%\n- Parameters: 1.2M\n- Training Time: 2.5 hours\n- Config: learning_rate=0.001, epochs=50\n- Commit: abc123def456\n- Date: 2025-12-10\n\n## ResNet v1.0\n- Dataset: CIFAR-10\n- Accuracy: 91.5%\n- Parameters: 2.1M\n- Training Time: 4 hours\n- Config: learning_rate=0.0001, epochs=100\n</code></pre>"},{"location":"mlops/model-versioning/#version-control-with-dvc","title":"Version Control with DVC","text":""},{"location":"mlops/model-versioning/#track-model-files","title":"Track Model Files","text":"<pre><code># Add model to DVC\ndvc add models/simple_cnn_best.pth\n\n# Commit DVC metadata\ngit add models/simple_cnn_best.pth.dvc\ngit commit -m \"Add SimpleCNN v1.0 model\"\n</code></pre>"},{"location":"mlops/model-versioning/#push-to-remote","title":"Push to Remote","text":"<pre><code># Push model to remote storage\ndvc push models/simple_cnn_best.pth.dvc\n\n# Pull model from remote\ndvc pull models/simple_cnn_best.pth.dvc\n</code></pre>"},{"location":"mlops/model-versioning/#loading-models","title":"Loading Models","text":""},{"location":"mlops/model-versioning/#load-checkpoint","title":"Load Checkpoint","text":"<pre><code>import torch\nfrom src.models.model import get_model\n\n# Create model\nmodel = get_model(\n    model_name=\"simple_cnn\",\n    num_classes=10,\n    image_size=32\n)\n\n# Load checkpoint\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n# Get metadata\nepoch = checkpoint['epoch']\nval_acc = checkpoint['val_acc']\nconfig = checkpoint['config']\n\nprint(f\"Model trained at epoch {epoch}: {val_acc:.2%} accuracy\")\n</code></pre>"},{"location":"mlops/model-versioning/#load-optimizer-state-for-resuming-training","title":"Load Optimizer State (for resuming training)","text":"<pre><code>import torch\nfrom torch.optim import Adam\nfrom src.models.model import get_model\n\n# Create model and optimizer\nmodel = get_model(\"simple_cnn\", num_classes=10, image_size=32)\noptimizer = Adam(model.parameters())\n\n# Load checkpoint\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n# Resume training from epoch\nstart_epoch = checkpoint['epoch'] + 1\n</code></pre>"},{"location":"mlops/model-versioning/#model-comparison","title":"Model Comparison","text":""},{"location":"mlops/model-versioning/#compare-multiple-models","title":"Compare Multiple Models","text":"<pre><code>import torch\nfrom pathlib import Path\nfrom src.models.model import get_model\nimport json\n\ndef compare_models(models_dir=\"models\"):\n    results = []\n\n    for model_file in Path(models_dir).glob(\"*_best.pth\"):\n        checkpoint = torch.load(model_file)\n\n        results.append({\n            'model': model_file.stem,\n            'epoch': checkpoint['epoch'],\n            'val_acc': checkpoint['val_acc'],\n            'config': checkpoint['config']\n        })\n\n    # Sort by accuracy\n    results.sort(key=lambda x: x['val_acc'], reverse=True)\n\n    print(\"Model Comparison:\")\n    for result in results:\n        print(f\"{result['model']}: {result['val_acc']:.2%}\")\n\n    return results\n</code></pre>"},{"location":"mlops/model-versioning/#versioning-best-practices","title":"Versioning Best Practices","text":""},{"location":"mlops/model-versioning/#1-semantic-versioning","title":"1. Semantic Versioning","text":"<p>Follow MAJOR.MINOR.PATCH:</p> <ul> <li>MAJOR: Architecture changes or significant accuracy improvements</li> <li>MINOR: Hyperparameter tuning, dataset version updates</li> <li>PATCH: Bug fixes, minor adjustments</li> </ul> <pre><code># Version naming\nmodels/simple_cnn_v1.0.0.pth   # MAJOR change\nmodels/simple_cnn_v1.1.0.pth   # MINOR improvement\nmodels/simple_cnn_v1.0.1.pth   # PATCH fix\n</code></pre>"},{"location":"mlops/model-versioning/#2-metadata-documentation","title":"2. Metadata Documentation","text":"<p>Store with each model:</p> <pre><code># models/simple_cnn_v1.0.0_metadata.yaml\nname: SimpleCNN\nversion: 1.0.0\ndataset: CIFAR-10\ntask: image_classification\nmetrics:\n  accuracy: 0.852\n  precision: 0.851\n  recall: 0.852\n  f1_score: 0.851\narchitecture:\n  num_classes: 10\n  input_channels: 3\n  image_size: 32\n  total_params: 1200000\ntraining:\n  epochs: 50\n  learning_rate: 0.001\n  optimizer: adam\n  batch_size: 32\n  training_time_hours: 2.5\n  gpu: NVIDIA RTX 3080\ndate: 2025-12-10\ngit_commit: abc123def456\nauthor: Your Name\nnotes: \"Best model on validation set\"\n</code></pre>"},{"location":"mlops/model-versioning/#3-model-registry","title":"3. Model Registry","text":"<p>Maintain central registry:</p> <pre><code># models/registry.json\n{\n  \"models\": [\n    {\n      \"id\": \"simple_cnn_v1.0.0\",\n      \"name\": \"SimpleCNN v1.0.0\",\n      \"path\": \"models/simple_cnn_best.pth\",\n      \"accuracy\": 0.852,\n      \"created\": \"2025-12-10\",\n      \"status\": \"production\"\n    },\n    {\n      \"id\": \"resnet_v1.0.0\",\n      \"name\": \"ResNet v1.0.0\",\n      \"path\": \"models/resnet_best.pth\",\n      \"accuracy\": 0.915,\n      \"created\": \"2025-12-10\",\n      \"status\": \"production\"\n    }\n  ]\n}\n</code></pre>"},{"location":"mlops/model-versioning/#integration-with-mlflow-optional","title":"Integration with MLflow (Optional)","text":"<p>Track models with MLflow:</p> <pre><code>pip install mlflow\n</code></pre> <pre><code>import mlflow\nfrom src.models.model import get_model\n\nmlflow.start_run()\n\nmodel = get_model(\"simple_cnn\", num_classes=10, image_size=32)\n# ... training code ...\n\n# Log metrics\nmlflow.log_metrics({'val_acc': 0.852, 'val_loss': 0.450})\n\n# Log model\nmlflow.pytorch.log_model(model, \"models/simple_cnn\")\n\n# Log artifacts\nmlflow.log_artifact(\"models/simple_cnn_best.pth\")\n\nmlflow.end_run()\n</code></pre> <p>View models:</p> <pre><code>mlflow ui\n</code></pre>"},{"location":"mlops/model-versioning/#deployment","title":"Deployment","text":""},{"location":"mlops/model-versioning/#export-model-for-production","title":"Export Model for Production","text":"<pre><code>import torch\nfrom src.models.model import get_model\n\n# Load trained model\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel = get_model(\"simple_cnn\", num_classes=10, image_size=32)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Export to TorchScript for faster inference\nscripted_model = torch.jit.script(model)\nscripted_model.save(\"models/simple_cnn_scripted.pt\")\n\n# Export to ONNX for cross-platform support\ndummy_input = torch.randn(1, 3, 32, 32)\ntorch.onnx.export(\n    model, dummy_input,\n    \"models/simple_cnn.onnx\",\n    input_names=['input'],\n    output_names=['output'],\n    opset_version=11\n)\n</code></pre>"},{"location":"mlops/model-versioning/#model-serving","title":"Model Serving","text":"<p>See Docker section for containerized serving.</p>"},{"location":"mlops/model-versioning/#cleanup-archival","title":"Cleanup &amp; Archival","text":""},{"location":"mlops/model-versioning/#archive-old-models","title":"Archive Old Models","text":"<pre><code># Create archive directory\nmkdir -p models/archive\nmv models/simple_cnn_old.pth models/archive/\n\n# Compress archives\ntar -czf models/archive_2025_12_01.tar.gz models/archive/\n</code></pre>"},{"location":"mlops/model-versioning/#keep-storage-clean","title":"Keep Storage Clean","text":"<pre><code># Remove intermediate checkpoints (keep only best)\nfind models/ -name \"*_epoch_*.pth\" -delete\n\n# Check disk usage\ndu -sh models/\n</code></pre>"},{"location":"mlops/model-versioning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mlops/model-versioning/#model-loading-error","title":"Model Loading Error","text":"<pre><code># Check file exists and is accessible\nfrom pathlib import Path\nmodel_path = Path(\"models/simple_cnn_best.pth\")\nassert model_path.exists(), \"Model file not found\"\n\n# Try loading with map_location for cross-device compatibility\ndevice = torch.device(\"cpu\")\ncheckpoint = torch.load(\n    model_path,\n    map_location=device\n)\n</code></pre>"},{"location":"mlops/model-versioning/#version-mismatch","title":"Version Mismatch","text":"<p>Ensure PyTorch versions match:</p> <pre><code># Check PyTorch version when saving\npython -c \"import torch; print(torch.__version__)\"\n\n# Specify version in requirements\ntorch==2.0.0\n</code></pre>"},{"location":"mlops/model-versioning/#references","title":"References","text":"<ul> <li>PyTorch Model Saving</li> <li>MLflow Model Registry</li> <li>ONNX Runtime</li> </ul>"}]}