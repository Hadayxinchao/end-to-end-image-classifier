{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"End-to-End Image Classifier with MLOps","text":"<p>Welcome to the documentation for the End-to-End Image Classifier project! This project demonstrates complete MLOps best practices with full automation for building, training, and deploying image classification models.</p>"},{"location":"#project-overview","title":"\ud83c\udfaf Project Overview","text":"<p>This project implements a comprehensive MLOps pipeline for image classification, demonstrating:</p> <ul> <li>Standardized Project Structure: Clean, maintainable codebase following industry standards</li> <li>Configuration Management: Flexible experiment configuration with Hydra</li> <li>Data Versioning: Track datasets with DVC</li> <li>Experiment Tracking: MLflow and Weights &amp; Biases integration</li> <li>Pre-commit Hooks: Automated code formatting and quality checks</li> <li>Security Scanning: Bandit for vulnerability detection</li> <li>Dependency Checking: Safety and pip-audit for package vulnerabilities</li> <li>Docker Optimization: Multi-stage builds for production</li> <li>Kubernetes Deployment: Production-ready K8s manifests with auto-scaling</li> <li>Automated Testing: Comprehensive test suite with pytest</li> <li>CI/CD: Complete GitHub Actions workflows</li> <li>Documentation: Full documentation with MkDocs</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Get started in 3 simple steps:</p> <pre><code># 1. Clone the repository\ngit clone https://github.com/Hadayxinchao/end-to-end-image-classifier.git\ncd end-to-end-image-classifier\n\n# 2. Run automated setup\n./scripts/quickstart.sh\n\n# 3. Train with W&amp;B tracking\nmake train-wandb\n</code></pre>"},{"location":"#automation-features","title":"\u2728 Automation Features","text":""},{"location":"#pre-commit-hooks","title":"\ud83d\udd27 Pre-commit Hooks","text":"<p>Automatically format and check your code before every commit:</p> <ul> <li>Black - Code formatting</li> <li>isort - Import sorting  </li> <li>flake8 - Linting with docstring checks</li> <li>Bandit - Security scanning</li> <li>mypy - Type checking</li> <li>detect-secrets - Secret detection</li> </ul> <pre><code>make setup-precommit\n# Now every commit automatically checks and formats code!\n</code></pre>"},{"location":"#security-quality","title":"\ud83d\udd12 Security &amp; Quality","text":"<p>Comprehensive security and quality checks:</p> <pre><code># Security scan\nmake security-scan\n\n# Check dependency vulnerabilities\nmake vulnerability-check\n\n# All quality checks\nmake quality-check\n</code></pre>"},{"location":"#production-ready-deployment","title":"\ud83d\udc33 Production-Ready Deployment","text":"<p>Deploy with confidence:</p> <p>Docker: - Multi-stage optimized builds - Non-root user for security - Health checks - Complete monitoring stack (MLflow, Prometheus, Grafana)</p> <pre><code>make docker-build-optimized\nmake docker-compose-up\n</code></pre> <p>Kubernetes: - Auto-scaling (2-10 pods) - LoadBalancer service - HTTPS ingress - Persistent storage - Resource limits - Prometheus monitoring</p> <pre><code>make k8s-deploy\nmake k8s-status\n</code></pre>"},{"location":"#enhanced-experiment-tracking","title":"\ud83d\udcca Enhanced Experiment Tracking","text":"<p>Track everything with Weights &amp; Biases:</p> <ul> <li>Model architecture and hyperparameters</li> <li>Training/validation metrics</li> <li>Weight and bias histograms</li> <li>Gradient distributions</li> <li>Model checkpoints</li> <li>Code versioning</li> </ul> <pre><code># Setup W&amp;B\nmake wandb-setup\n\n# Train with tracking\nmake train-wandb\n\n# Run hyperparameter sweeps\npython scripts/wandb_sweep.py\n</code></pre>"},{"location":"#key-features","title":"\ud83d\udcda Key Features","text":""},{"location":"#smart-configuration-management","title":"Smart Configuration Management","text":"<p>Use Hydra to manage all configurations without touching code:</p> <pre><code># Override learning rate\npython src/training/train.py hyperparameters.learning_rate=0.001\n\n# Use different model\npython src/training/train.py model=resnet\n\n# Track with W&amp;B\npython src/training/train.py tracking=wandb\n\n# Use both MLflow and W&amp;B\npython src/training/train.py tracking=wandb tracking.backend=both\n</code></pre>"},{"location":"#data-version-control","title":"Data Version Control","text":"<p>Never lose track of your datasets:</p> <pre><code># Track data with DVC\ndvc add data/raw\n\n# Push to remote storage\ndvc push\n\n# Pull data from any commit\ngit checkout &lt;commit&gt;\ndvc pull\n</code></pre>"},{"location":"#automated-testing","title":"Automated Testing","text":"<p>Ensure code quality with comprehensive tests:</p> <pre><code># Run all tests\npytest tests/ -v\n\n# Run with coverage\npytest tests/ --cov=src --cov-report=html\n\n# Quick tests only\npytest tests/ -m \"not slow\"\n</code></pre>"},{"location":"#experiment-tracking","title":"Experiment Tracking","text":"<p>Track and compare experiments:</p> <pre><code># MLflow UI\nmlflow ui\n# Visit http://localhost:5000\n\n# W&amp;B Dashboard\n# Visit https://wandb.ai/your-username/image-classifier\n\n# Compare experiments\npython scripts/compare_experiments.py\n</code></pre>"},{"location":"#development-workflow","title":"\ud83d\udcbb Development Workflow","text":""},{"location":"#1-start-a-new-feature","title":"1. Start a New Feature","text":"<pre><code># Create branch\ngit checkout -b feature/new-feature\n\n# Make changes\n# Pre-commit hooks run automatically on commit!\n</code></pre>"},{"location":"#2-run-quality-checks","title":"2. Run Quality Checks","text":"<pre><code># Format code\nmake format\n\n# Check linting\nmake lint\n\n# Run tests\nmake test\n\n# Security checks\nmake security-all\n</code></pre>"},{"location":"#3-train-and-experiment","title":"3. Train and Experiment","text":"<pre><code># Train with tracking\nmake train-wandb\n\n# Try different configs\npython src/training/train.py model=resnet hyperparameters.learning_rate=0.01\n</code></pre>"},{"location":"#4-deploy","title":"4. Deploy","text":"<pre><code># Build Docker image\nmake docker-build-optimized\n\n# Deploy to Kubernetes\nmake k8s-deploy\n\n# Monitor deployment\nmake k8s-status\n</code></pre>"},{"location":"#documentation-structure","title":"\ud83d\udcd6 Documentation Structure","text":"<ul> <li>Getting Started - Installation and setup</li> <li>User Guide - Training, inference, and data management</li> <li>MLOps - Complete automation guide</li> <li>Automation Guide - Pre-commit, security, deployment</li> <li>CI/CD Pipeline - GitHub Actions workflows</li> <li>Data Versioning - DVC setup and usage</li> <li>Experiment Tracking - MLflow and W&amp;B</li> <li>Docker - Docker usage and optimization</li> <li>Tracking Cheatsheet - Quick reference</li> <li>API Reference - Code documentation</li> <li>Development - Contributing guidelines</li> </ul>"},{"location":"#common-tasks","title":"\ud83c\udfaf Common Tasks","text":""},{"location":"#training","title":"Training","text":"<pre><code># Quick test training\nmake train-fast\n\n# Full training with W&amp;B\nmake train-wandb\n\n# Offline W&amp;B mode\nmake train-wandb-offline\n\n# Both MLflow and W&amp;B\nmake train-both\n</code></pre>"},{"location":"#code-quality","title":"Code Quality","text":"<pre><code># Format all code\nmake format\n\n# Check linting\nmake lint\n\n# Type checking\nmake type-check\n\n# All checks\nmake quality-check\n</code></pre>"},{"location":"#docker","title":"Docker","text":"<pre><code># Build optimized image\nmake docker-build-optimized\n\n# Start full stack\nmake docker-compose-up\n\n# Stop stack\nmake docker-compose-down\n</code></pre>"},{"location":"#kubernetes","title":"Kubernetes","text":"<pre><code># Deploy\nmake k8s-deploy\n\n# Check status\nmake k8s-status\n\n# View logs\nmake k8s-logs\n\n# Port forward\nmake k8s-port-forward\n\n# Delete deployment\nmake k8s-delete\n</code></pre>"},{"location":"#help","title":"Help","text":"<pre><code># View all available commands\nmake help\n</code></pre>"},{"location":"#tech-stack","title":"\ud83d\udee0\ufe0f Tech Stack","text":"<p>ML/DL: - PyTorch - torchvision - scikit-learn</p> <p>MLOps: - Hydra (configuration) - DVC (data versioning) - MLflow (experiment tracking) - Weights &amp; Biases (experiment tracking) - CML (continuous ML)</p> <p>Code Quality: - Black (formatting) - isort (import sorting) - flake8 (linting) - mypy (type checking) - pytest (testing) - Bandit (security) - Safety (dependency checking)</p> <p>DevOps: - Docker - Kubernetes - GitHub Actions - Prometheus - Grafana</p> <p>Documentation: - MkDocs - Material theme</p>"},{"location":"#project-metrics","title":"\ud83d\udcca Project Metrics","text":"<ul> <li>Code Coverage: 80%+</li> <li>Security Scans: Automated with Bandit</li> <li>Type Coverage: mypy enabled</li> <li>Documentation: 100% coverage</li> <li>Pre-commit Hooks: 12 hooks</li> <li>CI/CD: 4 automated jobs</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! Please see our Contributing Guide for details.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the License file for details.</p>"},{"location":"#links","title":"\ud83d\udd17 Links","text":"<ul> <li>GitHub Repository</li> <li>Documentation</li> <li>Issues</li> </ul>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<p>This project demonstrates MLOps best practices and industry-standard tools for production ML systems.</p> <p>Ready to get started? Head to the Installation Guide!</p>"},{"location":"about/license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2025 Hadayxinchao</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"about/license/#additional-licenses","title":"Additional Licenses","text":"<p>This project includes or depends on:</p>"},{"location":"about/license/#pytorch","title":"PyTorch","text":"<ul> <li>License: BSD</li> <li>Website: https://pytorch.org/</li> </ul>"},{"location":"about/license/#torchvision","title":"torchvision","text":"<ul> <li>License: BSD</li> <li>Website: https://github.com/pytorch/vision</li> </ul>"},{"location":"about/license/#hydra","title":"Hydra","text":"<ul> <li>License: Apache 2.0</li> <li>Website: https://hydra.cc/</li> </ul>"},{"location":"about/license/#dvc","title":"DVC","text":"<ul> <li>License: Apache 2.0</li> <li>Website: https://dvc.org/</li> </ul>"},{"location":"about/license/#pytest","title":"pytest","text":"<ul> <li>License: MIT</li> <li>Website: https://pytest.org/</li> </ul>"},{"location":"about/license/#mkdocs","title":"MkDocs","text":"<ul> <li>License: BSD</li> <li>Website: https://www.mkdocs.org/</li> </ul>"},{"location":"about/license/#material-for-mkdocs","title":"Material for MkDocs","text":"<ul> <li>License: MIT</li> <li>Website: https://squidfunk.github.io/mkdocs-material/</li> </ul>"},{"location":"about/license/#scikit-learn","title":"scikit-learn","text":"<ul> <li>License: BSD 3-Clause</li> <li>Website: https://scikit-learn.org/</li> </ul>"},{"location":"about/license/#numpy","title":"NumPy","text":"<ul> <li>License: BSD</li> <li>Website: https://numpy.org/</li> </ul>"},{"location":"about/license/#pandas","title":"Pandas","text":"<ul> <li>License: BSD 3-Clause</li> <li>Website: https://pandas.pydata.org/</li> </ul>"},{"location":"about/license/#matplotlib","title":"Matplotlib","text":"<ul> <li>License: PSF</li> <li>Website: https://matplotlib.org/</li> </ul>"},{"location":"about/license/#seaborn","title":"Seaborn","text":"<ul> <li>License: BSD 3-Clause</li> <li>Website: https://seaborn.pydata.org/</li> </ul> <p>See individual package licenses for more details.</p>"},{"location":"about/license/#third-party-resources","title":"Third-Party Resources","text":""},{"location":"about/license/#datasets","title":"Datasets","text":"<ul> <li>CIFAR-10: Published under CC BY 4.0</li> <li>MNIST: Published under CC BY 4.0</li> </ul>"},{"location":"about/license/#models","title":"Models","text":"<p>Built using published architectures and best practices from the deep learning community.</p>"},{"location":"about/license/#your-contributions","title":"Your Contributions","text":"<p>By contributing to this project, you agree that your contributions will be licensed under the same MIT License.</p> <p>For any questions about licensing, please open an issue on GitHub.</p>"},{"location":"about/overview/","title":"Project Overview","text":"<p>End-to-end image classification project following MLOps best practices.</p>"},{"location":"about/overview/#project-goals","title":"Project Goals","text":"<p>This project demonstrates a production-ready machine learning pipeline with:</p> <ol> <li>Reproducibility: Fixed seeds, versioned data, documented configurations</li> <li>Scalability: Modular architecture supporting multiple models and datasets</li> <li>Maintainability: Clean code, comprehensive tests, full documentation</li> <li>Automation: CI/CD pipelines, automated testing, model versioning</li> <li>Best Practices: DVC data versioning, Hydra config management, Docker containerization</li> </ol>"},{"location":"about/overview/#whats-included","title":"What's Included","text":""},{"location":"about/overview/#1-core-components","title":"1. Core Components","text":"<ul> <li>Data Management: Automatic CIFAR-10 &amp; MNIST loading with preprocessing</li> <li>Model Architectures: SimpleCNN and ResNet for image classification</li> <li>Training Pipeline: Configurable training with multiple optimizers and schedulers</li> <li>Inference: Batch and single-image prediction utilities</li> <li>Evaluation: Comprehensive metrics and visualization tools</li> </ul>"},{"location":"about/overview/#2-mlops-infrastructure","title":"2. MLOps Infrastructure","text":"<ul> <li>Configuration Management: Hydra for reproducible experiments</li> <li>Data Versioning: DVC integration for dataset tracking</li> <li>Unit Testing: 30+ pytest tests covering data, models, and training</li> <li>CI/CD: GitHub Actions workflows for automated testing</li> <li>Containerization: Docker setup for reproducible environments</li> <li>Documentation: MkDocs with comprehensive guides and API reference</li> </ul>"},{"location":"about/overview/#3-code-quality","title":"3. Code Quality","text":"<ul> <li>Code Formatting: Black for consistent style</li> <li>Import Sorting: isort for organized imports</li> <li>Linting: flake8 for code quality checks</li> <li>Type Checking: mypy for static type verification</li> </ul>"},{"location":"about/overview/#quick-start","title":"Quick Start","text":""},{"location":"about/overview/#installation","title":"Installation","text":"<pre><code># Clone repository\ngit clone &lt;repository-url&gt;\ncd end-to-end-image-classifier\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"about/overview/#training","title":"Training","text":"<pre><code># Train on CIFAR-10 (default)\npython src/training/train.py\n\n# Train on MNIST\npython src/training/train.py data=mnist\n\n# Custom configuration\npython src/training/train.py \\\n    hyperparameters.learning_rate=0.001 \\\n    hyperparameters.num_epochs=100\n</code></pre>"},{"location":"about/overview/#making-predictions","title":"Making Predictions","text":"<pre><code>from src.models.model import get_model\nfrom src.models.predict import predict_batch\nfrom src.data.make_dataset import load_cifar10\nimport torch\n\n# Load model and data\nmodel = get_model(\"simple_cnn\", num_classes=10, image_size=32)\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n_, _, test_loader = load_cifar10()\n\n# Make predictions\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\npredictions, labels = predict_batch(model, test_loader, device)\n</code></pre>"},{"location":"about/overview/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 src/                      # Source code\n\u2502   \u251c\u2500\u2500 data/                # Data loading and preprocessing\n\u2502   \u251c\u2500\u2500 models/              # Model architectures\n\u2502   \u251c\u2500\u2500 training/            # Training scripts\n\u2502   \u2514\u2500\u2500 utils/               # Utility functions\n\u251c\u2500\u2500 tests/                    # Unit and integration tests\n\u251c\u2500\u2500 configs/                  # Hydra configuration files\n\u251c\u2500\u2500 data/                     # Data directory (gitignored)\n\u251c\u2500\u2500 models/                   # Model checkpoints (gitignored)\n\u251c\u2500\u2500 reports/                  # Training reports and visualizations\n\u251c\u2500\u2500 docs/                     # Documentation (MkDocs)\n\u251c\u2500\u2500 .github/workflows/        # CI/CD pipelines\n\u251c\u2500\u2500 Dockerfile                # Container setup\n\u251c\u2500\u2500 requirements.txt          # Python dependencies\n\u251c\u2500\u2500 pyproject.toml            # Project configuration\n\u2514\u2500\u2500 mkdocs.yml               # Documentation configuration\n</code></pre>"},{"location":"about/overview/#technology-stack","title":"Technology Stack","text":"<ul> <li>Framework: PyTorch 2.0+</li> <li>Configuration: Hydra 1.3+</li> <li>Data Versioning: DVC 3.0+</li> <li>Testing: pytest 7.4+</li> <li>CI/CD: GitHub Actions</li> <li>Container: Docker</li> <li>Documentation: MkDocs Material</li> <li>Code Quality: Black, isort, flake8, mypy</li> </ul>"},{"location":"about/overview/#performance","title":"Performance","text":""},{"location":"about/overview/#model-accuracy","title":"Model Accuracy","text":"<ul> <li>CIFAR-10: ~85% (SimpleCNN), ~90%+ (ResNet)</li> <li>MNIST: ~98%+ (both architectures)</li> </ul>"},{"location":"about/overview/#training-time-cifar-10-simplecnn","title":"Training Time (CIFAR-10, SimpleCNN)","text":"<ul> <li>GPU: ~2-3 minutes for 50 epochs</li> <li>CPU: ~15-20 minutes for 50 epochs</li> </ul>"},{"location":"about/overview/#key-features","title":"Key Features","text":""},{"location":"about/overview/#hydra-configuration-management","title":"Hydra Configuration Management","text":"<p>Easily modify training via command line or YAML configs:</p> <pre><code>python src/training/train.py \\\n    data=cifar10 \\\n    model=resnet \\\n    hyperparameters.learning_rate=0.001 \\\n    hyperparameters.batch_size=64\n</code></pre>"},{"location":"about/overview/#reproducibility","title":"Reproducibility","text":"<ul> <li>Fixed random seeds for data and model</li> <li>Version control for code and configurations</li> <li>Data versioning with DVC</li> <li>Containerization for consistent environments</li> </ul>"},{"location":"about/overview/#modular-architecture","title":"Modular Architecture","text":"<ul> <li>Easy to add new models in <code>src/models/</code></li> <li>Simple to support new datasets in <code>src/data/</code></li> <li>Extensible training pipeline in <code>src/training/</code></li> </ul>"},{"location":"about/overview/#comprehensive-testing","title":"Comprehensive Testing","text":"<ul> <li>Data loading tests</li> <li>Model architecture tests</li> <li>Training pipeline tests</li> <li>90%+ code coverage</li> </ul>"},{"location":"about/overview/#continuous-integration","title":"Continuous Integration","text":"<p>Automated testing on every commit: - Multi-Python version support (3.9, 3.10, 3.11, 3.12, 3.13) - Linting and type checking - Unit test execution</p>"},{"location":"about/overview/#getting-started","title":"Getting Started","text":"<p>See Getting Started for installation instructions.</p>"},{"location":"about/overview/#documentation","title":"Documentation","text":"<p>Full documentation available at: - Installation &amp; Setup - Quick Start Guide - Configuration Guide - Training Guide - Inference Guide - API Reference</p>"},{"location":"about/overview/#contributing","title":"Contributing","text":"<p>We welcome contributions! See Contributing Guide for details.</p>"},{"location":"about/overview/#license","title":"License","text":"<p>This project is licensed under the MIT License. See License for details.</p>"},{"location":"about/overview/#authors","title":"Authors","text":"<ul> <li>Created as part of MLOps best practices demonstration</li> <li>Maintaining: Open to community contributions</li> </ul>"},{"location":"about/overview/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>PyTorch team for the excellent deep learning framework</li> <li>Hydra team for configuration management</li> <li>DVC team for data versioning</li> <li>GitHub for CI/CD infrastructure</li> </ul>"},{"location":"about/overview/#citation","title":"Citation","text":"<p>If you use this project in your research, please cite:</p> <pre><code>@misc{image-classifier-mlops,\n  title={End-to-End Image Classifier with MLOps},\n  author={Hoang Ha},\n  year={2025},\n  url={https://github.com/Hadayxinchao/end-to-end-image-classifier}\n}\n</code></pre>"},{"location":"about/overview/#contact","title":"Contact","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Email: [your-email]</li> </ul>"},{"location":"about/overview/#roadmap","title":"Roadmap","text":"<ul> <li> Add more model architectures (EfficientNet, Vision Transformer)</li> <li> Support additional datasets (ImageNet, COCO)</li> <li> MLflow integration for experiment tracking</li> <li> Model serving with FastAPI</li> <li> Batch inference optimization</li> <li> GPU optimization (mixed precision training)</li> <li> Model quantization</li> <li> Federated learning support</li> </ul>"},{"location":"about/overview/#related-resources","title":"Related Resources","text":"<ul> <li>PyTorch Documentation</li> <li>Hydra Documentation</li> <li>DVC Documentation</li> <li>MLOps Guide</li> <li>GitHub Actions Docs</li> </ul>"},{"location":"api/data/","title":"API Reference - Data Module","text":"<p>Auto-generated API documentation for the data module.</p>"},{"location":"api/data/#loading-datasets","title":"Loading Datasets","text":""},{"location":"api/data/#cifar-10-dataset","title":"CIFAR-10 Dataset","text":"<p>Load CIFAR-10 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Directory to store/load data</p> <code>'./data/raw'</code> <code>batch_size</code> <code>int</code> <p>Batch size for dataloaders</p> <code>64</code> <code>val_split</code> <code>float</code> <p>Fraction of training data to use for validation</p> <code>0.1</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading</p> <code>2</code> <p>Returns:</p> Type Description <code>Tuple[DataLoader, DataLoader, DataLoader]</code> <p>Tuple of (train_loader, val_loader, test_loader)</p> Source code in <code>src/data/make_dataset.py</code> <pre><code>def load_cifar10(\n    data_dir: str = \"./data/raw\", batch_size: int = 64, val_split: float = 0.1, num_workers: int = 2\n) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"\n    Load CIFAR-10 dataset.\n\n    Args:\n        data_dir: Directory to store/load data\n        batch_size: Batch size for dataloaders\n        val_split: Fraction of training data to use for validation\n        num_workers: Number of workers for data loading\n\n    Returns:\n        Tuple of (train_loader, val_loader, test_loader)\n    \"\"\"\n    data_path = Path(data_dir)\n    data_path.mkdir(parents=True, exist_ok=True)\n\n    # Get transforms\n    train_transform, test_transform = get_transforms()\n\n    # Load datasets\n    train_dataset = datasets.CIFAR10(\n        root=data_dir, train=True, download=True, transform=train_transform\n    )\n\n    test_dataset = datasets.CIFAR10(\n        root=data_dir, train=False, download=True, transform=test_transform\n    )\n\n    # Split training into train and validation\n    val_size = int(len(train_dataset) * val_split)\n    train_size = len(train_dataset) - val_size\n\n    train_dataset, val_dataset = random_split(\n        train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n    )\n\n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n    )\n\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n    )\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"api/data/#mnist-dataset","title":"MNIST Dataset","text":"<p>Load MNIST dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Directory to store/load data</p> <code>'./data/raw'</code> <code>batch_size</code> <code>int</code> <p>Batch size for dataloaders</p> <code>64</code> <code>val_split</code> <code>float</code> <p>Fraction of training data to use for validation</p> <code>0.1</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading</p> <code>2</code> <p>Returns:</p> Type Description <code>Tuple[DataLoader, DataLoader, DataLoader]</code> <p>Tuple of (train_loader, val_loader, test_loader)</p> Source code in <code>src/data/make_dataset.py</code> <pre><code>def load_mnist(\n    data_dir: str = \"./data/raw\", batch_size: int = 64, val_split: float = 0.1, num_workers: int = 2\n) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"\n    Load MNIST dataset.\n\n    Args:\n        data_dir: Directory to store/load data\n        batch_size: Batch size for dataloaders\n        val_split: Fraction of training data to use for validation\n        num_workers: Number of workers for data loading\n\n    Returns:\n        Tuple of (train_loader, val_loader, test_loader)\n    \"\"\"\n    data_path = Path(data_dir)\n    data_path.mkdir(parents=True, exist_ok=True)\n\n    # MNIST-specific transforms\n    train_transform = transforms.Compose(\n        [\n            transforms.Resize((28, 28)),\n            transforms.RandomRotation(10),\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n        ]\n    )\n\n    test_transform = transforms.Compose(\n        [\n            transforms.Resize((28, 28)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n        ]\n    )\n\n    # Load datasets\n    train_dataset = datasets.MNIST(\n        root=data_dir, train=True, download=True, transform=train_transform\n    )\n\n    test_dataset = datasets.MNIST(\n        root=data_dir, train=False, download=True, transform=test_transform\n    )\n\n    # Split training into train and validation\n    val_size = int(len(train_dataset) * val_split)\n    train_size = len(train_dataset) - val_size\n\n    train_dataset, val_dataset = random_split(\n        train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n    )\n\n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n    )\n\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n    )\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"api/data/#data-utilities","title":"Data Utilities","text":""},{"location":"api/data/#transform-functions","title":"Transform Functions","text":"<p>Data preprocessing and augmentation utilities:</p> <p>Get train and test transforms.</p> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>int</code> <p>Size to resize images to</p> <code>32</code> <code>mean</code> <code>Tuple</code> <p>Mean for normalization</p> <code>(0.5, 0.5, 0.5)</code> <code>std</code> <code>Tuple</code> <p>Standard deviation for normalization</p> <code>(0.5, 0.5, 0.5)</code> <p>Returns:</p> Type Description <code>Tuple[Compose, Compose]</code> <p>Tuple of (train_transform, test_transform)</p> Source code in <code>src/data/make_dataset.py</code> <pre><code>def get_transforms(\n    image_size: int = 32, mean: Tuple = (0.5, 0.5, 0.5), std: Tuple = (0.5, 0.5, 0.5)\n) -&gt; Tuple[transforms.Compose, transforms.Compose]:\n    \"\"\"\n    Get train and test transforms.\n\n    Args:\n        image_size: Size to resize images to\n        mean: Mean for normalization\n        std: Standard deviation for normalization\n\n    Returns:\n        Tuple of (train_transform, test_transform)\n    \"\"\"\n    train_transform = transforms.Compose(\n        [\n            transforms.Resize((image_size, image_size)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ]\n    )\n\n    test_transform = transforms.Compose(\n        [\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ]\n    )\n\n    return train_transform, test_transform\n</code></pre>"},{"location":"api/data/#class-reference","title":"Class Reference","text":""},{"location":"api/data/#dataloader-configuration","title":"DataLoader Configuration","text":"<pre><code>from torch.utils.data import DataLoader\n\n# Default parameters\nDataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True\n)\n</code></pre>"},{"location":"api/data/#module-contents","title":"Module Contents","text":"<pre><code>src/data/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 make_dataset.py\n\u2502   \u251c\u2500\u2500 load_cifar10()\n\u2502   \u251c\u2500\u2500 load_mnist()\n\u2502   \u2514\u2500\u2500 get_transforms()\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"api/data/#example-usage","title":"Example Usage","text":"<pre><code>from src.data.make_dataset import load_cifar10\n\n# Load datasets\ntrain_loader, val_loader, test_loader = load_cifar10(\n    data_dir=\"data/raw\",\n    batch_size=32,\n    val_split=0.2,\n    num_workers=4\n)\n\n# Iterate through batches\nfor images, labels in train_loader:\n    print(f\"Batch shape: {images.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n    break\n</code></pre>"},{"location":"api/models/","title":"API Reference - Models Module","text":"<p>Auto-generated API documentation for the models module.</p>"},{"location":"api/models/#model-factory","title":"Model Factory","text":""},{"location":"api/models/#get-model","title":"Get Model","text":"<p>Factory function to get model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model ('simple_cnn' or 'resnet')</p> <code>'simple_cnn'</code> <code>num_classes</code> <code>int</code> <p>Number of output classes</p> <code>10</code> <code>input_channels</code> <code>int</code> <p>Number of input channels</p> <code>3</code> <code>image_size</code> <code>int</code> <p>Input image size</p> <code>32</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for model</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>PyTorch model</p> Source code in <code>src/models/model.py</code> <pre><code>def get_model(\n    model_name: str = \"simple_cnn\",\n    num_classes: int = 10,\n    input_channels: int = 3,\n    image_size: int = 32,\n    **kwargs: Any,\n) -&gt; nn.Module:\n    \"\"\"\n    Factory function to get model by name.\n\n    Args:\n        model_name: Name of the model ('simple_cnn' or 'resnet')\n        num_classes: Number of output classes\n        input_channels: Number of input channels\n        image_size: Input image size\n        **kwargs: Additional arguments for model\n\n    Returns:\n        PyTorch model\n    \"\"\"\n    if model_name == \"simple_cnn\":\n        return SimpleCNN(\n            num_classes=num_classes, input_channels=input_channels, image_size=image_size, **kwargs\n        )\n    elif model_name == \"resnet\":\n        return ResNet(num_classes=num_classes, input_channels=input_channels)\n    else:\n        raise ValueError(f\"Unknown model name: {model_name}\")\n</code></pre>"},{"location":"api/models/#model-classes","title":"Model Classes","text":""},{"location":"api/models/#simplecnn","title":"SimpleCNN","text":"<p>               Bases: <code>Module</code></p> <p>Simple CNN for image classification.</p> Source code in <code>src/models/model.py</code> <pre><code>class SimpleCNN(nn.Module):\n    \"\"\"Simple CNN for image classification.\"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 10,\n        input_channels: int = 3,\n        dropout: float = 0.5,\n        image_size: int = 32,\n    ):\n        \"\"\"\n        Initialize the CNN model.\n\n        Args:\n            num_classes: Number of output classes\n            input_channels: Number of input channels (3 for RGB, 1 for grayscale)\n            dropout: Dropout probability\n            image_size: Input image size (height and width)\n        \"\"\"\n        super(SimpleCNN, self).__init__()\n\n        self.num_classes = num_classes\n        self.input_channels = input_channels\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n\n        # Pooling and dropout\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(dropout)\n\n        # Calculate size after conv layers\n        # After 3 pooling layers (each divides by 2): image_size / 8\n        conv_output_size = image_size // 8\n        fc_input_size = 128 * conv_output_size * conv_output_size\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(fc_input_size, 256)\n        self.fc2 = nn.Linear(256, num_classes)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            x: Input tensor of shape (batch_size, channels, height, width)\n\n        Returns:\n            Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        # Conv block 1\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.pool(x)  # 32x32 -&gt; 16x16 (for CIFAR-10)\n\n        # Conv block 2\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = self.pool(x)  # 16x16 -&gt; 8x8\n\n        # Conv block 3\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = F.relu(x)\n        x = self.pool(x)  # 8x8 -&gt; 4x4\n\n        # Flatten\n        x = x.view(x.size(0), -1)\n\n        # FC layers\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n</code></pre>"},{"location":"api/models/#src.models.model.SimpleCNN.__init__","title":"<code>__init__(num_classes=10, input_channels=3, dropout=0.5, image_size=32)</code>","text":"<p>Initialize the CNN model.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of output classes</p> <code>10</code> <code>input_channels</code> <code>int</code> <p>Number of input channels (3 for RGB, 1 for grayscale)</p> <code>3</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.5</code> <code>image_size</code> <code>int</code> <p>Input image size (height and width)</p> <code>32</code> Source code in <code>src/models/model.py</code> <pre><code>def __init__(\n    self,\n    num_classes: int = 10,\n    input_channels: int = 3,\n    dropout: float = 0.5,\n    image_size: int = 32,\n):\n    \"\"\"\n    Initialize the CNN model.\n\n    Args:\n        num_classes: Number of output classes\n        input_channels: Number of input channels (3 for RGB, 1 for grayscale)\n        dropout: Dropout probability\n        image_size: Input image size (height and width)\n    \"\"\"\n    super(SimpleCNN, self).__init__()\n\n    self.num_classes = num_classes\n    self.input_channels = input_channels\n\n    # Convolutional layers\n    self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n    self.bn1 = nn.BatchNorm2d(32)\n\n    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n    self.bn2 = nn.BatchNorm2d(64)\n\n    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n    self.bn3 = nn.BatchNorm2d(128)\n\n    # Pooling and dropout\n    self.pool = nn.MaxPool2d(2, 2)\n    self.dropout = nn.Dropout(dropout)\n\n    # Calculate size after conv layers\n    # After 3 pooling layers (each divides by 2): image_size / 8\n    conv_output_size = image_size // 8\n    fc_input_size = 128 * conv_output_size * conv_output_size\n\n    # Fully connected layers\n    self.fc1 = nn.Linear(fc_input_size, 256)\n    self.fc2 = nn.Linear(256, num_classes)\n</code></pre>"},{"location":"api/models/#src.models.model.SimpleCNN.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, channels, height, width)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, num_classes)</p> Source code in <code>src/models/model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Args:\n        x: Input tensor of shape (batch_size, channels, height, width)\n\n    Returns:\n        Output tensor of shape (batch_size, num_classes)\n    \"\"\"\n    # Conv block 1\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = F.relu(x)\n    x = self.pool(x)  # 32x32 -&gt; 16x16 (for CIFAR-10)\n\n    # Conv block 2\n    x = self.conv2(x)\n    x = self.bn2(x)\n    x = F.relu(x)\n    x = self.pool(x)  # 16x16 -&gt; 8x8\n\n    # Conv block 3\n    x = self.conv3(x)\n    x = self.bn3(x)\n    x = F.relu(x)\n    x = self.pool(x)  # 8x8 -&gt; 4x4\n\n    # Flatten\n    x = x.view(x.size(0), -1)\n\n    # FC layers\n    x = self.fc1(x)\n    x = F.relu(x)\n    x = self.dropout(x)\n    x = self.fc2(x)\n\n    return x\n</code></pre>"},{"location":"api/models/#resnet","title":"ResNet","text":"<p>               Bases: <code>Module</code></p> <p>Simple ResNet-style architecture.</p> Source code in <code>src/models/model.py</code> <pre><code>class ResNet(nn.Module):\n    \"\"\"Simple ResNet-style architecture.\"\"\"\n\n    def __init__(self, num_classes: int = 10, input_channels: int = 3):\n        \"\"\"\n        Initialize ResNet model.\n\n        Args:\n            num_classes: Number of output classes\n            input_channels: Number of input channels\n        \"\"\"\n        super(ResNet, self).__init__()\n\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        self.layer1 = self._make_layer(64, 2, stride=1)\n        self.layer2 = self._make_layer(128, 2, stride=2)\n        self.layer3 = self._make_layer(256, 2, stride=2)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)\n\n    def _make_layer(self, out_channels: int, num_blocks: int, stride: int):\n        \"\"\"Create a layer with multiple residual blocks.\"\"\"\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\"\"\"\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avg_pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n</code></pre>"},{"location":"api/models/#src.models.model.ResNet.__init__","title":"<code>__init__(num_classes=10, input_channels=3)</code>","text":"<p>Initialize ResNet model.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of output classes</p> <code>10</code> <code>input_channels</code> <code>int</code> <p>Number of input channels</p> <code>3</code> Source code in <code>src/models/model.py</code> <pre><code>def __init__(self, num_classes: int = 10, input_channels: int = 3):\n    \"\"\"\n    Initialize ResNet model.\n\n    Args:\n        num_classes: Number of output classes\n        input_channels: Number of input channels\n    \"\"\"\n    super(ResNet, self).__init__()\n\n    self.in_channels = 64\n\n    self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n    self.bn1 = nn.BatchNorm2d(64)\n\n    self.layer1 = self._make_layer(64, 2, stride=1)\n    self.layer2 = self._make_layer(128, 2, stride=2)\n    self.layer3 = self._make_layer(256, 2, stride=2)\n\n    self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n    self.fc = nn.Linear(256, num_classes)\n</code></pre>"},{"location":"api/models/#src.models.model.ResNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> Source code in <code>src/models/model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\"\"\"\n    out = F.relu(self.bn1(self.conv1(x)))\n    out = self.layer1(out)\n    out = self.layer2(out)\n    out = self.layer3(out)\n    out = self.avg_pool(out)\n    out = out.view(out.size(0), -1)\n    out = self.fc(out)\n    return out\n</code></pre>"},{"location":"api/models/#inference","title":"Inference","text":""},{"location":"api/models/#batch-prediction","title":"Batch Prediction","text":"<p>Make predictions on a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Trained model</p> required <code>dataloader</code> <code>DataLoader</code> <p>DataLoader containing images</p> required <code>device</code> <code>str</code> <p>Device to run inference on</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple of (predictions, true_labels)</p> Source code in <code>src/models/predict.py</code> <pre><code>def predict_batch(\n    model: nn.Module, dataloader: DataLoader, device: str = \"cpu\"\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Make predictions on a batch of images.\n\n    Args:\n        model: Trained model\n        dataloader: DataLoader containing images\n        device: Device to run inference on\n\n    Returns:\n        Tuple of (predictions, true_labels)\n    \"\"\"\n    model.eval()\n    all_predictions = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images = images.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n\n            all_predictions.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return np.array(all_predictions), np.array(all_labels)\n</code></pre>"},{"location":"api/models/#module-contents","title":"Module Contents","text":"<pre><code>src/models/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 model.py\n\u2502   \u251c\u2500\u2500 get_model()\n\u2502   \u251c\u2500\u2500 SimpleCNN\n\u2502   \u2514\u2500\u2500 ResNet\n\u251c\u2500\u2500 predict.py\n\u2502   \u251c\u2500\u2500 predict_batch()\n\u2502   \u2514\u2500\u2500 predict_single()\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"api/models/#model-architecture","title":"Model Architecture","text":""},{"location":"api/models/#simplecnn_1","title":"SimpleCNN","text":"<pre><code>SimpleCNN(\n    num_classes=10,\n    input_channels=3,\n    dropout=0.5,\n    image_size=32\n)\n</code></pre> <p>Structure: - Conv Layer 1: 3 \u2192 64 channels - Conv Layer 2: 64 \u2192 128 channels - Conv Layer 3: 128 \u2192 128 channels - Fully Connected: 12844 \u2192 256 \u2192 num_classes</p>"},{"location":"api/models/#resnet_1","title":"ResNet","text":"<p>Standard ResNet architecture adapted for CIFAR-10/MNIST with: - Residual blocks - Batch normalization - Global average pooling</p>"},{"location":"api/models/#example-usage","title":"Example Usage","text":"<pre><code>import torch\nfrom src.models.model import get_model\n\n# Create model\nmodel = get_model(\n    model_name=\"simple_cnn\",\n    num_classes=10,\n    input_channels=3,\n    image_size=32,\n    dropout=0.5\n)\n\n# Forward pass\nbatch = torch.randn(32, 3, 32, 32)\noutput = model(batch)\nprint(output.shape)  # [32, 10]\n\n# Load checkpoint\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Predictions\nwith torch.no_grad():\n    predictions = model(batch)\n    probs = torch.softmax(predictions, dim=1)\n    class_ids = predictions.argmax(dim=1)\n</code></pre>"},{"location":"api/models/#model-properties","title":"Model Properties","text":"<ul> <li>Total Parameters: Varies by architecture</li> <li>Input Size: Configurable (32x32 for CIFAR-10, 28x28 for MNIST)</li> <li>Output Size: Number of classes (10)</li> <li>Device: CPU/CUDA compatible</li> </ul>"},{"location":"api/training/","title":"API Reference - Training Module","text":"<p>Auto-generated API documentation for the training module.</p>"},{"location":"api/training/#training-functions","title":"Training Functions","text":""},{"location":"api/training/#main-training-loop","title":"Main Training Loop","text":"<p>Run the main training pipeline.</p> Source code in <code>src/training/train.py</code> <pre><code>@hydra.main(version_base=None, config_path=\"../../configs\", config_name=\"config\")\ndef main(cfg: DictConfig):  # noqa: C901\n    \"\"\"Run the main training pipeline.\"\"\"\n    # Print configuration\n    print(\"=\" * 80)\n    print(\"Configuration:\")\n    print(OmegaConf.to_yaml(cfg))\n    print(\"=\" * 80)\n\n    # Initialize experiment tracking\n    tracking_backend: Optional[str] = \"wandb\"  # Default to W&amp;B\n    if hasattr(cfg, \"tracking\"):\n        if hasattr(cfg.tracking, \"enabled\") and not cfg.tracking.enabled:\n            tracking_backend = None\n        elif hasattr(cfg.tracking, \"backend\"):\n            tracking_backend = cfg.tracking.backend\n\n    tracker = None\n    if tracking_backend:\n        try:\n            tracker = ExperimentTracker(cfg, tracking_backend=tracking_backend)\n        except Exception as e:\n            print(f\"Warning: Could not initialize experiment tracking: {e}\")\n            tracker = None\n\n    # Set seed\n    set_seed(cfg.seed)\n\n    # Get device\n    device = get_device(cfg.device)\n    print(f\"\\nUsing device: {device}\")\n\n    # Load data\n    print(f\"\\nLoading {cfg.data.name} dataset...\")\n    if cfg.data.name == \"cifar10\":\n        train_loader, val_loader, test_loader = load_cifar10(\n            data_dir=cfg.data.data_dir,\n            batch_size=cfg.hyperparameters.batch_size,\n            val_split=cfg.data.val_split,\n            num_workers=cfg.num_workers,\n        )\n    elif cfg.data.name == \"mnist\":\n        train_loader, val_loader, test_loader = load_mnist(\n            data_dir=cfg.data.data_dir,\n            batch_size=cfg.hyperparameters.batch_size,\n            val_split=cfg.data.val_split,\n            num_workers=cfg.num_workers,\n        )\n    else:\n        raise ValueError(f\"Unknown dataset: {cfg.data.name}\")\n\n    print(f\"Train batches: {len(train_loader)}\")\n    print(f\"Val batches: {len(val_loader)}\")\n    print(f\"Test batches: {len(test_loader)}\")\n\n    # Create model\n    print(f\"\\nCreating {cfg.model.name} model...\")\n    model = get_model(\n        model_name=cfg.model.name,\n        num_classes=cfg.data.num_classes,\n        input_channels=cfg.data.input_channels,\n        image_size=cfg.data.image_size,\n        dropout=cfg.hyperparameters.dropout,\n    )\n    model = model.to(device)\n\n    # Count parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n\n    # Log model architecture and watch for gradient tracking\n    if tracker:\n        tracker.log_model_architecture(model)\n\n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss(label_smoothing=cfg.hyperparameters.label_smoothing)\n    optimizer = get_optimizer(model, cfg)\n    scheduler = get_scheduler(optimizer, cfg)\n\n    # Log hyperparameters and configuration\n    if tracker:\n        params_to_log = {\n            \"model_name\": cfg.model.name,\n            \"dataset_name\": cfg.data.name,\n            \"num_classes\": cfg.data.num_classes,\n            \"batch_size\": cfg.hyperparameters.batch_size,\n            \"learning_rate\": cfg.hyperparameters.learning_rate,\n            \"optimizer\": cfg.hyperparameters.optimizer,\n            \"num_epochs\": cfg.hyperparameters.num_epochs,\n            \"total_params\": total_params,\n            \"trainable_params\": trainable_params,\n            \"device_name\": str(device),\n        }\n        tracker.log_params(params_to_log)\n\n    # Training history\n    history: Dict[str, list] = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n\n    best_val_acc = 0.0\n    patience_counter = 0\n\n    # Training loop\n    print(f\"\\nStarting training for {cfg.hyperparameters.num_epochs} epochs...\")\n    print(\"=\" * 80)\n\n    for epoch in range(cfg.hyperparameters.num_epochs):\n        print(f\"\\nEpoch {epoch + 1}/{cfg.hyperparameters.num_epochs}\")\n        print(\"-\" * 80)\n\n        # Train\n        train_loss, train_acc = train_epoch(\n            model,\n            train_loader,\n            criterion,\n            optimizer,\n            device,\n            clip_grad_norm=cfg.hyperparameters.clip_grad_norm,\n        )\n\n        # Validate\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n\n        # Update learning rate\n        if scheduler is not None:\n            if isinstance(scheduler, ReduceLROnPlateau):\n                scheduler.step(val_loss)\n            else:\n                scheduler.step()\n\n        # Save history\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n\n        # Print epoch summary\n        current_lr = optimizer.param_groups[0][\"lr\"]\n        print(\"\\nEpoch Summary:\")\n        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n        print(f\"  Learning Rate: {current_lr:.6f}\")\n\n        # Log metrics to experiment tracker\n        if tracker:\n            tracker.log_metrics(\n                {\n                    \"train_loss\": train_loss,\n                    \"train_acc\": train_acc,\n                    \"val_loss\": val_loss,\n                    \"val_acc\": val_acc,\n                    \"learning_rate\": current_lr,\n                },\n                step=epoch,\n            )\n            # Log learning rate separately for better tracking\n            tracker.log_learning_rate(current_lr, epoch)\n\n            # Log weight histograms every 10 epochs\n            if epoch % 10 == 0:\n                tracker.log_weights_histograms(model, epoch)\n\n        # Save best model\n        if val_acc &gt; best_val_acc:\n            best_val_acc = val_acc\n            patience_counter = 0\n\n            if cfg.save_best_only:\n                model_save_path = Path(cfg.model_save_dir) / f\"{cfg.model.name}_best.pth\"\n                model_save_path.parent.mkdir(parents=True, exist_ok=True)\n\n                torch.save(\n                    {\n                        \"epoch\": epoch,\n                        \"model_state_dict\": model.state_dict(),\n                        \"optimizer_state_dict\": optimizer.state_dict(),\n                        \"val_acc\": val_acc,\n                        \"config\": OmegaConf.to_container(cfg),\n                    },\n                    model_save_path,\n                )\n\n                print(f\"  \u2713 Best model saved! (Val Acc: {val_acc:.2f}%)\")\n\n                # Log model checkpoint to tracker\n                if tracker:\n                    tracker.log_model_checkpoint(\n                        model,\n                        str(model_save_path),\n                        {\"val_acc\": val_acc, \"val_loss\": val_loss, \"epoch\": epoch},\n                        is_best=True,\n                    )\n        else:\n            patience_counter += 1\n\n        # Early stopping\n        if patience_counter &gt;= cfg.early_stopping_patience:\n            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n            break\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Training completed!\")\n    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n\n    # Load best model for evaluation\n    best_model_path = Path(cfg.model_save_dir) / f\"{cfg.model.name}_best.pth\"\n    if best_model_path.exists():\n        checkpoint = torch.load(best_model_path, map_location=device)\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n        epoch_info = f\" from epoch {checkpoint['epoch'] + 1}\" if \"epoch\" in checkpoint else \"\"\n        print(f\"\\nLoaded best model{epoch_info}\")\n\n    # Evaluate on test set\n    print(\"\\nEvaluating on test set...\")\n    test_loss, test_acc = validate(model, test_loader, criterion, device)\n    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n\n    # Generate predictions and metrics\n    print(\"\\nGenerating predictions and metrics...\")\n    predictions, true_labels = predict_batch(model, test_loader, device)  # type: ignore[arg-type]\n\n    # Calculate metrics\n    metrics = calculate_metrics(predictions, true_labels)\n    print(\"\\nTest Metrics:\")\n    for metric_name, metric_value in metrics.items():\n        print(f\"  {metric_name}: {metric_value:.4f}\")\n\n    # Log test metrics to tracker\n    if tracker:\n        test_metrics = {f\"test_{k}\": v for k, v in metrics.items()}\n        test_metrics[\"test_loss\"] = test_loss\n        test_metrics[\"test_acc\"] = test_acc\n        tracker.log_metrics(test_metrics)\n\n    # Save reports\n    report_dir = Path(cfg.report_dir)\n    report_dir.mkdir(parents=True, exist_ok=True)\n\n    # Confusion matrix\n    plot_confusion_matrix(\n        predictions,\n        true_labels,\n        cfg.data.classes,\n        save_path=str(report_dir / \"confusion_matrix.png\"),\n    )\n\n    # Classification report\n    save_classification_report(\n        predictions,\n        true_labels,\n        cfg.data.classes,\n        save_path=str(report_dir / \"classification_report.txt\"),\n    )\n\n    # Training history plot\n    plot_training_history(history, save_path=str(report_dir / \"training_history.png\"))\n\n    print(f\"\\nReports saved to {report_dir}\")\n\n    # Log artifacts to experiment tracker\n    if tracker:\n        print(\"\\nLogging artifacts to experiment tracker...\")\n        tracker.log_artifact(str(report_dir / \"confusion_matrix.png\"), \"report\", \"confusion_matrix\")\n        tracker.log_artifact(\n            str(report_dir / \"classification_report.txt\"), \"report\", \"classification_report\"\n        )\n        tracker.log_artifact(str(report_dir / \"training_history.png\"), \"report\", \"training_history\")\n\n        # Log confusion matrix as image to W&amp;B\n        from PIL import Image\n\n        cm_img = Image.open(report_dir / \"confusion_matrix.png\")\n        tracker.log_image(\"confusion_matrix\", cm_img, \"Final Confusion Matrix\")\n\n        print(\"\u2713 Artifacts logged successfully\")\n\n    # Finish experiment tracking\n    if tracker:\n        tracker.finish()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"All done! \ud83c\udf89\")\n</code></pre>"},{"location":"api/training/#train-epoch","title":"Train Epoch","text":"<p>Train for one epoch.</p> Source code in <code>src/training/train.py</code> <pre><code>def train_epoch(model, train_loader, criterion, optimizer, device, clip_grad_norm=None):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n\n    losses = AverageMeter()\n    correct = 0\n    total = 0\n\n    pbar = tqdm(train_loader, desc=\"Training\")\n    for images, labels in pbar:\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Gradient clipping\n        if clip_grad_norm is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n\n        optimizer.step()\n\n        # Update metrics\n        losses.update(loss.item(), images.size(0))\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n        # Update progress bar\n        pbar.set_postfix({\"loss\": f\"{losses.avg:.4f}\", \"acc\": f\"{100.*correct/total:.2f}%\"})\n\n    return losses.avg, 100.0 * correct / total\n</code></pre>"},{"location":"api/training/#validation","title":"Validation","text":"<p>Validate the model.</p> Source code in <code>src/training/train.py</code> <pre><code>def validate(model, val_loader, criterion, device):\n    \"\"\"Validate the model.\"\"\"\n    model.eval()\n\n    losses = AverageMeter()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        pbar = tqdm(val_loader, desc=\"Validation\")\n        for images, labels in pbar:\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            losses.update(loss.item(), images.size(0))\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n            pbar.set_postfix({\"loss\": f\"{losses.avg:.4f}\", \"acc\": f\"{100.*correct/total:.2f}%\"})\n\n    return losses.avg, 100.0 * correct / total\n</code></pre>"},{"location":"api/training/#utilities","title":"Utilities","text":""},{"location":"api/training/#set-seed","title":"Set Seed","text":"<p>Set random seed for reproducibility.</p> Source code in <code>src/training/train.py</code> <pre><code>def set_seed(seed: int):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"api/training/#get-device","title":"Get Device","text":"<p>Get device based on configuration.</p> Source code in <code>src/training/train.py</code> <pre><code>def get_device(device_config: str) -&gt; torch.device:\n    \"\"\"Get device based on configuration.\"\"\"\n    if device_config == \"auto\":\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        return torch.device(device_config)\n</code></pre>"},{"location":"api/training/#get-optimizer","title":"Get Optimizer","text":"<p>Get optimizer based on configuration.</p> Source code in <code>src/training/train.py</code> <pre><code>def get_optimizer(model, cfg):\n    \"\"\"Get optimizer based on configuration.\"\"\"\n    if cfg.hyperparameters.optimizer.lower() == \"adam\":\n        return optim.Adam(\n            model.parameters(),\n            lr=cfg.hyperparameters.learning_rate,\n            weight_decay=cfg.hyperparameters.weight_decay,\n        )\n    elif cfg.hyperparameters.optimizer.lower() == \"sgd\":\n        return optim.SGD(\n            model.parameters(),\n            lr=cfg.hyperparameters.learning_rate,\n            momentum=cfg.hyperparameters.momentum,\n            weight_decay=cfg.hyperparameters.weight_decay,\n        )\n    else:\n        raise ValueError(f\"Unknown optimizer: {cfg.hyperparameters.optimizer}\")\n</code></pre>"},{"location":"api/training/#get-scheduler","title":"Get Scheduler","text":"<p>Get learning rate scheduler based on configuration.</p> Source code in <code>src/training/train.py</code> <pre><code>def get_scheduler(optimizer, cfg):\n    \"\"\"Get learning rate scheduler based on configuration.\"\"\"\n    if not cfg.hyperparameters.use_scheduler:\n        return None\n\n    scheduler_type = cfg.hyperparameters.scheduler_type.lower()\n\n    if scheduler_type == \"step\":\n        return StepLR(\n            optimizer, step_size=cfg.hyperparameters.step_size, gamma=cfg.hyperparameters.gamma\n        )\n    elif scheduler_type == \"cosine\":\n        return CosineAnnealingLR(\n            optimizer, T_max=cfg.hyperparameters.num_epochs, eta_min=cfg.hyperparameters.min_lr\n        )\n    elif scheduler_type == \"plateau\":\n        return ReduceLROnPlateau(\n            optimizer,\n            mode=\"min\",\n            factor=cfg.hyperparameters.gamma,\n            patience=5,\n            min_lr=cfg.hyperparameters.min_lr,\n        )\n    else:\n        raise ValueError(f\"Unknown scheduler type: {scheduler_type}\")\n</code></pre>"},{"location":"api/training/#module-contents","title":"Module Contents","text":"<pre><code>src/training/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 train.py\n\u2502   \u251c\u2500\u2500 main()\n\u2502   \u251c\u2500\u2500 train_epoch()\n\u2502   \u251c\u2500\u2500 validate()\n\u2502   \u251c\u2500\u2500 set_seed()\n\u2502   \u251c\u2500\u2500 get_device()\n\u2502   \u251c\u2500\u2500 get_optimizer()\n\u2502   \u2514\u2500\u2500 get_scheduler()\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"api/training/#training-configuration","title":"Training Configuration","text":""},{"location":"api/training/#hydra-configuration","title":"Hydra Configuration","text":"<p>Configure training via YAML files in <code>configs/</code>:</p> <pre><code># Main config\nhyperparameters:\n  optimizer: adam\n  learning_rate: 0.001\n  batch_size: 32\n  num_epochs: 50\n  dropout: 0.5\n  label_smoothing: 0.1\n  clip_grad_norm: 1.0\n</code></pre>"},{"location":"api/training/#supported-optimizers","title":"Supported Optimizers","text":"<ul> <li>Adam: Adaptive learning rates</li> <li>SGD: Stochastic gradient descent</li> </ul>"},{"location":"api/training/#supported-schedulers","title":"Supported Schedulers","text":"<ul> <li>Step: Decay learning rate by gamma every step_size epochs</li> <li>Cosine: Cosine annealing schedule</li> <li>Plateau: Reduce learning rate on metric plateau</li> </ul>"},{"location":"api/training/#example-usage","title":"Example Usage","text":"<pre><code>from src.training.train import (\n    train_epoch, validate, set_seed,\n    get_device, get_optimizer\n)\nimport torch\nimport torch.nn as nn\n\n# Setup\nset_seed(42)\ndevice = get_device(\"auto\")\nmodel = ...  # Create model\ncriterion = nn.CrossEntropyLoss()\noptimizer = get_optimizer(model, config)\n\n# Train one epoch\ntrain_loss, train_acc = train_epoch(\n    model, train_loader, criterion,\n    optimizer, device\n)\n\n# Validate\nval_loss, val_acc = validate(\n    model, val_loader, criterion, device\n)\n\nprint(f\"Train: loss={train_loss:.4f}, acc={train_acc:.2f}%\")\nprint(f\"Val: loss={val_loss:.4f}, acc={val_acc:.2f}%\")\n</code></pre>"},{"location":"api/training/#training-outputs","title":"Training Outputs","text":"<p>During training, the script saves: - Best model: <code>models/{model_name}_best.pth</code> - Reports: <code>reports/</code> directory   - <code>confusion_matrix.png</code>   - <code>classification_report.txt</code>   - <code>training_history.png</code></p>"},{"location":"api/training/#checkpoint-format","title":"Checkpoint Format","text":"<pre><code>checkpoint = {\n    'epoch': int,\n    'model_state_dict': dict,\n    'optimizer_state_dict': dict,\n    'val_acc': float,\n    'config': dict\n}\n</code></pre> <p>Load checkpoint:</p> <pre><code>checkpoint = torch.load('models/simple_cnn_best.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nepoch = checkpoint['epoch']\n</code></pre>"},{"location":"api/utils/","title":"API Reference - Utils Module","text":"<p>Auto-generated API documentation for the utilities module.</p>"},{"location":"api/utils/#metrics","title":"Metrics","text":""},{"location":"api/utils/#calculate-metrics","title":"Calculate Metrics","text":"<p>Calculate classification metrics.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Predicted labels</p> required <code>labels</code> <code>ndarray</code> <p>True labels</p> required <code>average</code> <code>str</code> <p>Averaging strategy for multi-class metrics</p> <code>'macro'</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing metrics</p> Source code in <code>src/utils/metrics.py</code> <pre><code>def calculate_metrics(predictions: np.ndarray, labels: np.ndarray, average: str = \"macro\") -&gt; dict:\n    \"\"\"\n    Calculate classification metrics.\n\n    Args:\n        predictions: Predicted labels\n        labels: True labels\n        average: Averaging strategy for multi-class metrics\n\n    Returns:\n        Dictionary containing metrics\n    \"\"\"\n    metrics = {\n        \"accuracy\": accuracy_score(labels, predictions),\n        \"precision\": precision_score(labels, predictions, average=average, zero_division=0),\n        \"recall\": recall_score(labels, predictions, average=average, zero_division=0),\n        \"f1_score\": f1_score(labels, predictions, average=average, zero_division=0),\n    }\n\n    return metrics\n</code></pre>"},{"location":"api/utils/#confusion-matrix","title":"Confusion Matrix","text":"<p>Plot confusion matrix.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Predicted labels</p> required <code>labels</code> <code>ndarray</code> <p>True labels</p> required <code>class_names</code> <code>List[str]</code> <p>List of class names</p> required <code>save_path</code> <code>Optional[str]</code> <p>Path to save figure</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size</p> <code>(10, 8)</code> Source code in <code>src/utils/metrics.py</code> <pre><code>def plot_confusion_matrix(\n    predictions: np.ndarray,\n    labels: np.ndarray,\n    class_names: List[str],\n    save_path: Optional[str] = None,\n    figsize: Tuple[int, int] = (10, 8),\n):\n    \"\"\"\n    Plot confusion matrix.\n\n    Args:\n        predictions: Predicted labels\n        labels: True labels\n        class_names: List of class names\n        save_path: Path to save figure\n        figsize: Figure size\n    \"\"\"\n    cm = confusion_matrix(labels, predictions)\n\n    plt.figure(figsize=figsize)\n    sns.heatmap(\n        cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names\n    )\n    plt.title(\"Confusion Matrix\")\n    plt.ylabel(\"True Label\")\n    plt.xlabel(\"Predicted Label\")\n    plt.tight_layout()\n\n    if save_path:\n        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n        print(f\"Confusion matrix saved to {save_path}\")\n\n    plt.close()\n</code></pre>"},{"location":"api/utils/#classification-report","title":"Classification Report","text":"<p>Save classification report to file.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Predicted labels</p> required <code>labels</code> <code>ndarray</code> <p>True labels</p> required <code>class_names</code> <code>List[str]</code> <p>List of class names</p> required <code>save_path</code> <code>str</code> <p>Path to save report</p> required Source code in <code>src/utils/metrics.py</code> <pre><code>def save_classification_report(\n    predictions: np.ndarray, labels: np.ndarray, class_names: List[str], save_path: str\n):\n    \"\"\"\n    Save classification report to file.\n\n    Args:\n        predictions: Predicted labels\n        labels: True labels\n        class_names: List of class names\n        save_path: Path to save report\n    \"\"\"\n    report = classification_report(labels, predictions, target_names=class_names, digits=4)\n\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n\n    with open(save_path, \"w\") as f:\n        f.write(\"Classification Report\\n\")\n        f.write(\"=\" * 80 + \"\\n\\n\")\n        f.write(report)\n        f.write(\"\\n\\n\")\n\n        # Add overall metrics\n        metrics = calculate_metrics(predictions, labels)\n        f.write(\"Overall Metrics\\n\")\n        f.write(\"-\" * 80 + \"\\n\")\n        for metric_name, metric_value in metrics.items():\n            f.write(f\"{metric_name}: {metric_value:.4f}\\n\")\n\n    print(f\"Classification report saved to {save_path}\")\n</code></pre>"},{"location":"api/utils/#training-history-plot","title":"Training History Plot","text":"<p>Plot training history (loss and accuracy).</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>dict</code> <p>Dictionary containing 'train_loss', 'val_loss', 'train_acc', 'val_acc'</p> required <code>save_path</code> <code>Optional[str]</code> <p>Path to save figure</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size</p> <code>(12, 4)</code> Source code in <code>src/utils/metrics.py</code> <pre><code>def plot_training_history(\n    history: dict, save_path: Optional[str] = None, figsize: Tuple[int, int] = (12, 4)\n):\n    \"\"\"\n    Plot training history (loss and accuracy).\n\n    Args:\n        history: Dictionary containing 'train_loss', 'val_loss', 'train_acc', 'val_acc'\n        save_path: Path to save figure\n        figsize: Figure size\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=figsize)\n\n    # Plot loss\n    axes[0].plot(history[\"train_loss\"], label=\"Train Loss\")\n    if \"val_loss\" in history:\n        axes[0].plot(history[\"val_loss\"], label=\"Val Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].set_title(\"Training and Validation Loss\")\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    # Plot accuracy\n    axes[1].plot(history[\"train_acc\"], label=\"Train Accuracy\")\n    if \"val_acc\" in history:\n        axes[1].plot(history[\"val_acc\"], label=\"Val Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Accuracy\")\n    axes[1].set_title(\"Training and Validation Accuracy\")\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n\n    if save_path:\n        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n        print(f\"Training history plot saved to {save_path}\")\n\n    plt.close()\n</code></pre>"},{"location":"api/utils/#utilities","title":"Utilities","text":""},{"location":"api/utils/#average-meter","title":"Average Meter","text":"<p>Computes and stores the average and current value.</p> Source code in <code>src/utils/metrics.py</code> <pre><code>class AverageMeter:\n    \"\"\"Computes and stores the average and current value.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the AverageMeter.\"\"\"\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset all statistics.\"\"\"\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val: float, n: int = 1) -&gt; None:\n        \"\"\"\n        Update statistics.\n\n        Args:\n            val: Value to add\n            n: Number of items\n        \"\"\"\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count if self.count != 0 else 0\n</code></pre>"},{"location":"api/utils/#src.utils.metrics.AverageMeter.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the AverageMeter.</p> Source code in <code>src/utils/metrics.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the AverageMeter.\"\"\"\n    self.reset()\n</code></pre>"},{"location":"api/utils/#src.utils.metrics.AverageMeter.reset","title":"<code>reset()</code>","text":"<p>Reset all statistics.</p> Source code in <code>src/utils/metrics.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all statistics.\"\"\"\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n</code></pre>"},{"location":"api/utils/#src.utils.metrics.AverageMeter.update","title":"<code>update(val, n=1)</code>","text":"<p>Update statistics.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>float</code> <p>Value to add</p> required <code>n</code> <code>int</code> <p>Number of items</p> <code>1</code> Source code in <code>src/utils/metrics.py</code> <pre><code>def update(self, val: float, n: int = 1) -&gt; None:\n    \"\"\"\n    Update statistics.\n\n    Args:\n        val: Value to add\n        n: Number of items\n    \"\"\"\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count if self.count != 0 else 0\n</code></pre>"},{"location":"api/utils/#module-contents","title":"Module Contents","text":"<pre><code>src/utils/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 metrics.py\n\u2502   \u251c\u2500\u2500 AverageMeter\n\u2502   \u251c\u2500\u2500 calculate_metrics()\n\u2502   \u251c\u2500\u2500 plot_confusion_matrix()\n\u2502   \u251c\u2500\u2500 save_classification_report()\n\u2502   \u2514\u2500\u2500 plot_training_history()\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"api/utils/#metrics-calculation","title":"Metrics Calculation","text":"<p>Supported metrics: - Accuracy: Overall correctness - Precision: True positives / (true positives + false positives) - Recall: True positives / (true positives + false negatives) - F1 Score: Harmonic mean of precision and recall</p>"},{"location":"api/utils/#example-usage","title":"Example Usage","text":"<pre><code>from src.utils.metrics import (\n    AverageMeter, calculate_metrics,\n    plot_confusion_matrix\n)\nimport numpy as np\n\n# Track average loss\nloss_meter = AverageMeter()\nfor batch_loss in losses:\n    loss_meter.update(batch_loss, batch_size)\n\nprint(f\"Average loss: {loss_meter.avg:.4f}\")\n\n# Calculate metrics\npredictions = np.array([...])  # Predicted labels\nlabels = np.array([...])  # True labels\n\nmetrics = calculate_metrics(predictions, labels)\nprint(f\"Accuracy: {metrics['accuracy']:.4f}\")\nprint(f\"Precision: {metrics['precision']:.4f}\")\nprint(f\"Recall: {metrics['recall']:.4f}\")\nprint(f\"F1 Score: {metrics['f1_score']:.4f}\")\n\n# Generate visualizations\nplot_confusion_matrix(\n    predictions, labels,\n    class_names=['class_0', 'class_1', ...],\n    save_path=\"reports/confusion_matrix.png\"\n)\n</code></pre>"},{"location":"api/utils/#averagemeter-class","title":"AverageMeter Class","text":"<p>Track running average of a metric:</p> <pre><code>meter = AverageMeter()\nmeter.update(value, n)  # n = batch size\nprint(meter.avg)  # Get average\n</code></pre>"},{"location":"api/utils/#visualization-functions","title":"Visualization Functions","text":""},{"location":"api/utils/#confusion-matrix_1","title":"Confusion Matrix","text":"<p>Heatmap showing prediction vs true labels.</p>"},{"location":"api/utils/#classification-report_1","title":"Classification Report","text":"<p>Text report with precision, recall, F1-score per class.</p>"},{"location":"api/utils/#training-history","title":"Training History","text":"<p>Line plots of loss and accuracy over epochs.</p>"},{"location":"development/code-style/","title":"Code Style","text":"<p>Code style guidelines and standards for this project.</p>"},{"location":"development/code-style/#python-style-guide","title":"Python Style Guide","text":"<p>We follow PEP 8 with tools for enforcement:</p> <ul> <li>Black: Code formatting</li> <li>isort: Import sorting</li> <li>flake8: Linting</li> <li>mypy: Type checking</li> </ul>"},{"location":"development/code-style/#running-style-checks","title":"Running Style Checks","text":""},{"location":"development/code-style/#format-code-with-black","title":"Format Code with Black","text":"<pre><code># Format all Python files\nblack src/ tests/\n\n# Format specific file\nblack src/training/train.py\n\n# Check without modifying\nblack --check src/\n</code></pre>"},{"location":"development/code-style/#sort-imports-with-isort","title":"Sort Imports with isort","text":"<pre><code># Sort imports\nisort src/ tests/\n\n# Check without modifying\nisort --check-only src/\n</code></pre>"},{"location":"development/code-style/#lint-with-flake8","title":"Lint with flake8","text":"<pre><code># Run linter\nflake8 src/ tests/\n\n# Show statistics\nflake8 --statistics src/\n</code></pre>"},{"location":"development/code-style/#type-check-with-mypy","title":"Type Check with mypy","text":"<pre><code># Run type checker\nmypy src/\n\n# Strict mode\nmypy --strict src/\n</code></pre>"},{"location":"development/code-style/#code-style-rules","title":"Code Style Rules","text":""},{"location":"development/code-style/#naming-conventions","title":"Naming Conventions","text":"<pre><code># Constants: UPPER_CASE\nLEARNING_RATE = 0.001\nMAX_EPOCHS = 100\n\n# Functions and variables: snake_case\ndef train_model(data_loader, model):\n    pass\n\n# Classes: PascalCase\nclass SimpleCNN(torch.nn.Module):\n    pass\n\n# Private methods: _leading_underscore\ndef _preprocess_data(data):\n    pass\n</code></pre>"},{"location":"development/code-style/#line-length","title":"Line Length","text":"<p>Maximum 88 characters (Black default):</p> <pre><code># Good - fits within 88 chars\nresult = some_function(arg1, arg2, arg3)\n\n# Bad - exceeds 88 chars\nresult = some_very_long_function_name(argument_one, argument_two, argument_three)\n\n# Good - break long lines\nresult = some_very_long_function_name(\n    argument_one,\n    argument_two,\n    argument_three\n)\n</code></pre>"},{"location":"development/code-style/#imports","title":"Imports","text":"<pre><code># Good - grouped and sorted\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nfrom src.models.model import SimpleCNN\nfrom src.utils.metrics import calculate_metrics\n\n# Bad - unsorted, mixed order\nfrom src.models.model import SimpleCNN\nimport sys\nimport torch\nfrom src.utils.metrics import calculate_metrics\nimport os\n</code></pre>"},{"location":"development/code-style/#docstring-format","title":"Docstring Format","text":"<p>Use Google-style docstrings:</p> <pre><code>def train_model(\n    model: torch.nn.Module,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    num_epochs: int = 50,\n    learning_rate: float = 0.001\n) -&gt; Dict[str, List[float]]:\n    \"\"\"Train a neural network model.\n\n    Long description of what this function does,\n    including important details and context.\n\n    Args:\n        model: PyTorch model to train\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        num_epochs: Number of training epochs. Defaults to 50.\n        learning_rate: Learning rate for optimizer. Defaults to 0.001.\n\n    Returns:\n        Dictionary containing training history with keys:\n        - 'train_loss': List of training losses\n        - 'val_loss': List of validation losses\n        - 'train_acc': List of training accuracies\n        - 'val_acc': List of validation accuracies\n\n    Raises:\n        ValueError: If num_epochs &lt;= 0\n        TypeError: If model is not a torch.nn.Module\n\n    Example:\n        &gt;&gt;&gt; model = SimpleCNN()\n        &gt;&gt;&gt; history = train_model(model, train_loader, val_loader)\n        &gt;&gt;&gt; print(history['val_acc'][-1])\n\n    Note:\n        This function modifies the model in-place.\n\n    See Also:\n        validate() - Function for model validation\n    \"\"\"\n    pass\n</code></pre>"},{"location":"development/code-style/#type-hints","title":"Type Hints","text":"<p>Always use type hints:</p> <pre><code># Good\ndef process_batch(\n    images: torch.Tensor,\n    labels: torch.Tensor,\n    device: torch.device\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    images = images.to(device)\n    labels = labels.to(device)\n    return images, labels\n\n# Bad\ndef process_batch(images, labels, device):\n    images = images.to(device)\n    labels = labels.to(device)\n    return images, labels\n\n# Optional types\nfrom typing import Optional\n\ndef train_model(\n    model: torch.nn.Module,\n    checkpoint_path: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    if checkpoint_path is not None:\n        load_checkpoint(model, checkpoint_path)\n    return {}\n</code></pre>"},{"location":"development/code-style/#string-formatting","title":"String Formatting","text":"<p>Use f-strings:</p> <pre><code># Good\nname = \"CIFAR-10\"\nprint(f\"Training on {name} dataset\")\n\n# OK but less readable\nprint(\"Training on {} dataset\".format(name))\n\n# Avoid\nprint(\"Training on \" + name + \" dataset\")\n</code></pre>"},{"location":"development/code-style/#whitespace","title":"Whitespace","text":"<pre><code># Good - proper spacing\ndef calculate_metrics(predictions: np.ndarray, labels: np.ndarray) -&gt; float:\n    accuracy = (predictions == labels).sum() / len(labels)\n    return accuracy\n\n# Bad - inconsistent spacing\ndef calculate_metrics(predictions:np.ndarray,labels:np.ndarray)-&gt;float:\n    accuracy=(predictions==labels).sum()/len(labels)\n    return accuracy\n</code></pre>"},{"location":"development/code-style/#listdict-comprehensions","title":"List/Dict Comprehensions","text":"<p>Use comprehensions for readability:</p> <pre><code># Good\nsquares = [x ** 2 for x in range(10)]\neven_numbers = {x: x**2 for x in range(10) if x % 2 == 0}\n\n# Acceptable but less readable\nsquares = []\nfor x in range(10):\n    squares.append(x ** 2)\n</code></pre>"},{"location":"development/code-style/#comments","title":"Comments","text":"<pre><code># Good - explains WHY, not WHAT\n# Use exponential moving average for smoother loss tracking\nsmoothed_loss = 0.99 * smoothed_loss + 0.01 * current_loss\n\n# Bad - just repeats the code\n# Multiply smoothed_loss by 0.99 and add current_loss multiplied by 0.01\nsmoothed_loss = 0.99 * smoothed_loss + 0.01 * current_loss\n\n# Good - explain non-obvious logic\n# We use 32x32 for CIFAR-10 to match standard benchmark size\nimage_size = 32\n</code></pre>"},{"location":"development/code-style/#file-organization","title":"File Organization","text":""},{"location":"development/code-style/#module-structure","title":"Module Structure","text":"<pre><code># src/models/model.py\n\n\"\"\"CNN architectures for image classification.\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom typing import Optional\n\n# Constants first\nDEFAULT_IMAGE_SIZE = 32\nDEFAULT_NUM_CLASSES = 10\n\n# Then functions\ndef get_model(model_name: str, **kwargs) -&gt; nn.Module:\n    \"\"\"Factory function for creating models.\"\"\"\n    pass\n\n# Then classes\nclass SimpleCNN(nn.Module):\n    \"\"\"Simple CNN for image classification.\"\"\"\n    pass\n\nclass ResNet(nn.Module):\n    \"\"\"ResNet architecture.\"\"\"\n    pass\n</code></pre>"},{"location":"development/code-style/#linting-configuration","title":"Linting Configuration","text":""},{"location":"development/code-style/#flake8-configuration","title":"flake8 Configuration","text":"<pre><code># setup.cfg or .flake8\n[flake8]\nmax-line-length = 88\nexclude = .git,__pycache__,venv\nignore = E203,W503\n</code></pre>"},{"location":"development/code-style/#isort-configuration","title":"isort Configuration","text":"<pre><code># pyproject.toml\n[tool.isort]\nprofile = \"black\"\nline_length = 88\nmulti_line_mode = 3\ninclude_trailing_comma = true\n</code></pre>"},{"location":"development/code-style/#mypy-configuration","title":"mypy Configuration","text":"<pre><code># mypy.ini\n[mypy]\npython_version = 3.9\nwarn_return_any = True\nwarn_unused_configs = True\nignore_missing_imports = True\n</code></pre>"},{"location":"development/code-style/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Automatically format code before commit:</p> <pre><code># Install\npip install pre-commit\n\n# Setup\npre-commit install\n\n# Run manually\npre-commit run --all-files\n</code></pre>"},{"location":"development/code-style/#ide-integration","title":"IDE Integration","text":""},{"location":"development/code-style/#vs-code","title":"VS Code","text":"<p>Install extensions: - Python - Pylance - Black Formatter - isort</p> <p>Add to <code>.vscode/settings.json</code>:</p> <pre><code>{\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.mypyEnabled\": true,\n    \"[python]\": {\n        \"editor.formatOnSave\": true,\n        \"editor.codeActionsOnSave\": {\n            \"source.organizeImports\": true\n        }\n    }\n}\n</code></pre>"},{"location":"development/code-style/#checklist-before-commit","title":"Checklist Before Commit","text":"<ul> <li> Code formatted with Black</li> <li> Imports sorted with isort</li> <li> No flake8 warnings</li> <li> Type hints added</li> <li> Docstrings present</li> <li> Tests pass</li> <li> No commented-out code</li> </ul>"},{"location":"development/contributing/","title":"Contributing","text":"<p>Guidelines for contributing to this project.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#fork-and-clone","title":"Fork and Clone","text":"<pre><code># Fork on GitHub, then\ngit clone https://github.com/Hadayxinchao/end-to-end-image-classifier.git\ncd end-to-end-image-classifier\ngit remote add upstream https://github.com/original/end-to-end-image-classifier.git\n</code></pre>"},{"location":"development/contributing/#setup-development-environment","title":"Setup Development Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\npip install -e .  # Install in development mode\n</code></pre>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#create-feature-branch","title":"Create Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#make-changes","title":"Make Changes","text":"<p>Follow the code style guidelines (see Code Style).</p>"},{"location":"development/contributing/#write-tests","title":"Write Tests","text":"<p>Add tests in <code>tests/</code> directory:</p> <pre><code># tests/test_my_feature.py\nimport pytest\nfrom src.module import function\n\ndef test_function():\n    assert function(input) == expected_output\n</code></pre>"},{"location":"development/contributing/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test\npytest tests/test_my_feature.py\n\n# With coverage\npytest --cov=src tests/\n</code></pre>"},{"location":"development/contributing/#commit-changes","title":"Commit Changes","text":"<pre><code>git add .\ngit commit -m \"Add feature: description\"\n</code></pre>"},{"location":"development/contributing/#push-and-create-pr","title":"Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"development/contributing/#code-standards","title":"Code Standards","text":""},{"location":"development/contributing/#python-style","title":"Python Style","text":"<p>Follow PEP 8 and use: - Black: Code formatting - isort: Import sorting - flake8: Linting - mypy: Type checking</p> <pre><code># Format code\nblack src/ tests/\n\n# Sort imports\nisort src/ tests/\n\n# Lint\nflake8 src/ tests/\n\n# Type check\nmypy src/\n</code></pre>"},{"location":"development/contributing/#type-hints","title":"Type Hints","text":"<p>Use type hints for all functions:</p> <pre><code>from typing import List, Tuple\nimport numpy as np\n\ndef process_batch(\n    images: np.ndarray,\n    labels: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Process a batch of images and labels.\n\n    Args:\n        images: Image array of shape (B, C, H, W)\n        labels: Label array of shape (B,)\n\n    Returns:\n        Processed images and labels\n    \"\"\"\n    return images, labels\n</code></pre>"},{"location":"development/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def train_model(\n    train_loader,\n    val_loader,\n    num_epochs: int = 50,\n    learning_rate: float = 0.001\n) -&gt; dict:\n    \"\"\"Train a model.\n\n    Args:\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        num_epochs: Number of training epochs\n        learning_rate: Learning rate for optimizer\n\n    Returns:\n        Dictionary containing training history\n\n    Raises:\n        ValueError: If num_epochs &lt;= 0\n\n    Example:\n        &gt;&gt;&gt; history = train_model(train_loader, val_loader)\n        &gt;&gt;&gt; print(history['accuracy'])\n    \"\"\"\n    if num_epochs &lt;= 0:\n        raise ValueError(\"num_epochs must be positive\")\n\n    # Implementation\n    return {}\n</code></pre>"},{"location":"development/contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"development/contributing/#test-structure","title":"Test Structure","text":"<pre><code># tests/test_models.py\nimport pytest\nimport torch\nfrom src.models.model import SimpleCNN, get_model\n\nclass TestSimpleCNN:\n    \"\"\"Test SimpleCNN model.\"\"\"\n\n    @pytest.fixture\n    def model(self):\n        \"\"\"Create test model.\"\"\"\n        return SimpleCNN(num_classes=10, image_size=32)\n\n    def test_initialization(self, model):\n        \"\"\"Test model initialization.\"\"\"\n        assert model is not None\n\n    def test_forward_pass(self, model):\n        \"\"\"Test forward pass.\"\"\"\n        x = torch.randn(2, 3, 32, 32)\n        y = model(x)\n        assert y.shape == (2, 10)\n\n    @pytest.mark.parametrize(\"image_size\", [28, 32, 64])\n    def test_different_sizes(self, image_size):\n        \"\"\"Test model with different input sizes.\"\"\"\n        model = SimpleCNN(num_classes=10, image_size=image_size)\n        x = torch.randn(2, 3, image_size, image_size)\n        y = model(x)\n        assert y.shape == (2, 10)\n</code></pre>"},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test class\npytest tests/test_models.py::TestSimpleCNN\n\n# Run specific test\npytest tests/test_models.py::TestSimpleCNN::test_forward_pass\n\n# Run with coverage report\npytest --cov=src --cov-report=html tests/\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#update-documentation","title":"Update Documentation","text":"<ol> <li>Update relevant <code>.md</code> file in <code>docs/</code></li> <li>Follow Markdown format</li> <li>Include code examples where applicable</li> </ol>"},{"location":"development/contributing/#build-documentation-locally","title":"Build Documentation Locally","text":"<pre><code>pip install mkdocs mkdocs-material\nmkdocs serve\n</code></pre> <p>Then visit <code>http://localhost:8000</code></p>"},{"location":"development/contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"development/contributing/#bug-reports","title":"Bug Reports","text":"<p>Create an issue with: - Title: Clear, concise description - Description: What happened, what should happen - Reproduction: Steps to reproduce - Environment: Python version, OS, packages</p>"},{"location":"development/contributing/#feature-requests","title":"Feature Requests","text":"<p>Describe: - Use case: Why it's needed - Proposed solution: How to implement - Alternatives: Other approaches considered</p>"},{"location":"development/contributing/#pr-review-process","title":"PR Review Process","text":"<ol> <li>Check automated tests: Must pass all CI/CD checks</li> <li>Code review: 2 approvals required</li> <li>Documentation: Updated if needed</li> <li>Merge: Squash and merge to main</li> </ol>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Discussions: GitHub Discussions</li> <li>Issues: GitHub Issues</li> <li>Documentation: Read the docs site</li> <li>Email: Contact maintainers</li> </ul>"},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>No harassment or discrimination</li> <li>Assume good faith</li> <li>Give and accept constructive feedback</li> </ul> <p>Thank you for contributing! \ud83c\udf89</p>"},{"location":"development/testing/","title":"Testing","text":"<p>Comprehensive guide to testing in this project.</p>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 test_data.py         # Data loading tests\n\u251c\u2500\u2500 test_model.py        # Model architecture tests\n\u251c\u2500\u2500 test_training.py     # Training pipeline tests\n\u2514\u2500\u2500 fixtures/            # Test fixtures and mocks\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#run-all-tests","title":"Run All Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"development/testing/#run-specific-test-file","title":"Run Specific Test File","text":"<pre><code>pytest tests/test_model.py\n</code></pre>"},{"location":"development/testing/#run-specific-test-class","title":"Run Specific Test Class","text":"<pre><code>pytest tests/test_model.py::TestSimpleCNN\n</code></pre>"},{"location":"development/testing/#run-specific-test","title":"Run Specific Test","text":"<pre><code>pytest tests/test_model.py::TestSimpleCNN::test_forward_pass\n</code></pre>"},{"location":"development/testing/#verbose-output","title":"Verbose Output","text":"<pre><code>pytest -v\n</code></pre>"},{"location":"development/testing/#show-print-statements","title":"Show Print Statements","text":"<pre><code>pytest -s\n</code></pre>"},{"location":"development/testing/#stop-on-first-failure","title":"Stop on First Failure","text":"<pre><code>pytest -x\n</code></pre>"},{"location":"development/testing/#code-coverage","title":"Code Coverage","text":""},{"location":"development/testing/#generate-coverage-report","title":"Generate Coverage Report","text":"<pre><code>pytest --cov=src tests/\n</code></pre>"},{"location":"development/testing/#html-coverage-report","title":"HTML Coverage Report","text":"<pre><code>pytest --cov=src --cov-report=html tests/\n</code></pre> <p>View report in <code>htmlcov/index.html</code></p>"},{"location":"development/testing/#coverage-threshold","title":"Coverage Threshold","text":"<p>Configure in <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\naddopts = \"--cov=src --cov-report=term-missing --cov-fail-under=80\"\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Test individual functions:</p> <pre><code># tests/test_utils.py\nfrom src.utils.metrics import AverageMeter\n\ndef test_average_meter():\n    meter = AverageMeter()\n    meter.update(10, 1)\n    meter.update(20, 2)\n    assert meter.avg == 15.0  # (10*1 + 20*2) / 3\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Test module interactions:</p> <pre><code># tests/test_training.py\ndef test_training_pipeline(train_loader, val_loader):\n    model = get_model(\"simple_cnn\", num_classes=10, image_size=32)\n    optimizer = torch.optim.Adam(model.parameters())\n    criterion = torch.nn.CrossEntropyLoss()\n\n    train_loss, train_acc = train_epoch(\n        model, train_loader, criterion, optimizer, device\n    )\n\n    assert train_loss &gt; 0\n    assert 0 &lt;= train_acc &lt;= 100\n</code></pre>"},{"location":"development/testing/#parametrized-tests","title":"Parametrized Tests","text":"<p>Test multiple inputs:</p> <pre><code>import pytest\n\n@pytest.mark.parametrize(\"model_name\", [\"simple_cnn\", \"resnet\"])\n@pytest.mark.parametrize(\"image_size\", [28, 32])\ndef test_models(model_name, image_size):\n    model = get_model(model_name, num_classes=10, image_size=image_size)\n    x = torch.randn(2, 3, image_size, image_size)\n    y = model(x)\n    assert y.shape == (2, 10)\n</code></pre>"},{"location":"development/testing/#test-fixtures","title":"Test Fixtures","text":""},{"location":"development/testing/#common-fixtures","title":"Common Fixtures","text":"<pre><code># tests/conftest.py\nimport pytest\nimport torch\n\n@pytest.fixture\ndef device():\n    return torch.device(\"cpu\")\n\n@pytest.fixture\ndef model():\n    from src.models.model import SimpleCNN\n    return SimpleCNN(num_classes=10, image_size=32)\n\n@pytest.fixture\ndef sample_batch():\n    images = torch.randn(4, 3, 32, 32)\n    labels = torch.tensor([0, 1, 2, 3])\n    return images, labels\n</code></pre>"},{"location":"development/testing/#using-fixtures","title":"Using Fixtures","text":"<pre><code>def test_model_forward(model, sample_batch, device):\n    images, labels = sample_batch\n    images = images.to(device)\n\n    output = model(images)\n    assert output.shape == (4, 10)\n</code></pre>"},{"location":"development/testing/#mocking","title":"Mocking","text":""},{"location":"development/testing/#mock-external-calls","title":"Mock External Calls","text":"<pre><code>from unittest.mock import patch, MagicMock\n\n@patch('src.data.make_dataset.torchvision.datasets.CIFAR10')\ndef test_load_cifar10(mock_cifar10):\n    mock_cifar10.return_value = MagicMock()\n\n    train_loader, _, _ = load_cifar10()\n    mock_cifar10.assert_called_once()\n</code></pre>"},{"location":"development/testing/#performance-tests","title":"Performance Tests","text":""},{"location":"development/testing/#test-execution-time","title":"Test Execution Time","text":"<pre><code>import pytest\nimport time\n\n@pytest.mark.performance\ndef test_training_speed():\n    model = get_model(\"simple_cnn\", num_classes=10, image_size=32)\n    batch = torch.randn(32, 3, 32, 32)\n\n    start = time.time()\n    for _ in range(100):\n        _ = model(batch)\n    elapsed = time.time() - start\n\n    # Should complete 100 batches in &lt; 5 seconds\n    assert elapsed &lt; 5.0\n</code></pre>"},{"location":"development/testing/#skip-slow-tests","title":"Skip Slow Tests","text":"<pre><code>pytest -m \"not performance\"\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions","title":"GitHub Actions","text":"<p>Tests run on every push/PR:</p> <pre><code># .github/workflows/tests.yaml\n- name: Run tests\n  run: pytest --cov=src tests/\n</code></pre>"},{"location":"development/testing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<pre><code># Install pre-commit\npip install pre-commit\n\n# Setup hooks\npre-commit install\n\n# Run manually\npre-commit run --all-files\n</code></pre>"},{"location":"development/testing/#debugging-tests","title":"Debugging Tests","text":""},{"location":"development/testing/#use-pytest-debugger","title":"Use pytest Debugger","text":"<pre><code>pytest --pdb\n</code></pre> <p>This drops into debugger on failure.</p>"},{"location":"development/testing/#use-print-statements","title":"Use Print Statements","text":"<pre><code>pytest -s  # Show print output\n</code></pre>"},{"location":"development/testing/#verbose-output_1","title":"Verbose Output","text":"<pre><code>pytest -vv  # Extra verbose\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":"<ol> <li> <p>One assertion per test (when possible)    <pre><code># Good\ndef test_model_output_shape():\n    output = model(x)\n    assert output.shape == (2, 10)\n\n# Bad\ndef test_model():\n    output = model(x)\n    assert output.shape == (2, 10)\n    assert output.dtype == torch.float32\n</code></pre></p> </li> <li> <p>Use descriptive names <pre><code># Good\ndef test_simple_cnn_forward_pass_with_correct_input_shape()\n\n# Bad\ndef test_model()\n</code></pre></p> </li> <li> <p>Test edge cases <pre><code>def test_metrics_with_empty_predictions():\n    predictions = np.array([])\n    labels = np.array([])\n    # Should handle gracefully\n</code></pre></p> </li> <li> <p>Use fixtures for setup <pre><code># Good\n@pytest.fixture\ndef model():\n    return SimpleCNN()\n\n# Bad\ndef test_something():\n    model = SimpleCNN()  # Setup in test\n</code></pre></p> </li> <li> <p>Mock external dependencies <pre><code># Good - Mock network call\n@patch('requests.get')\ndef test_download(mock_get)\n\n# Bad - Actually calls network\ndef test_download():\n    response = requests.get(url)\n</code></pre></p> </li> </ol>"},{"location":"development/testing/#test-examples","title":"Test Examples","text":""},{"location":"development/testing/#data-loading-test","title":"Data Loading Test","text":"<pre><code>def test_load_cifar10(tmp_path):\n    train_loader, val_loader, test_loader = load_cifar10(\n        data_dir=str(tmp_path),\n        batch_size=32\n    )\n\n    assert len(train_loader) &gt; 0\n    assert len(val_loader) &gt; 0\n\n    images, labels = next(iter(train_loader))\n    assert images.shape == (32, 3, 32, 32)\n    assert labels.shape == (32,)\n</code></pre>"},{"location":"development/testing/#model-test","title":"Model Test","text":"<pre><code>def test_simple_cnn_different_image_sizes():\n    for image_size in [28, 32, 64]:\n        model = SimpleCNN(num_classes=10, image_size=image_size)\n        x = torch.randn(4, 3, image_size, image_size)\n        y = model(x)\n        assert y.shape == (4, 10)\n</code></pre>"},{"location":"development/testing/#training-test","title":"Training Test","text":"<pre><code>def test_training_improves_loss(train_loader, model):\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n    initial_loss = None\n    final_loss = None\n\n    for epoch in range(2):\n        loss, _ = train_epoch(\n            model, train_loader, criterion, optimizer, device\n        )\n        if epoch == 0:\n            initial_loss = loss\n        else:\n            final_loss = loss\n\n    # Loss should decrease after training\n    assert final_loss &lt; initial_loss\n</code></pre> <p>Enjoy testing! \ud83e\uddea</p>"},{"location":"getting-started/configuration/","title":"Configuration Management with Hydra","text":"<p>This project uses Hydra for configuration management, making it easy to run experiments without modifying code.</p>"},{"location":"getting-started/configuration/#configuration-structure","title":"Configuration Structure","text":"<pre><code>configs/\n\u251c\u2500\u2500 config.yaml              # Main configuration\n\u251c\u2500\u2500 model/\n\u2502   \u251c\u2500\u2500 simple_cnn.yaml     # Simple CNN config\n\u2502   \u2514\u2500\u2500 resnet.yaml         # ResNet config\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 cifar10.yaml        # CIFAR-10 dataset config\n\u2502   \u2514\u2500\u2500 mnist.yaml          # MNIST dataset config\n\u2514\u2500\u2500 hyperparameters/\n    \u251c\u2500\u2500 default.yaml        # Default hyperparameters\n    \u2514\u2500\u2500 fast.yaml           # Fast training config\n</code></pre>"},{"location":"getting-started/configuration/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/configuration/#override-single-parameter","title":"Override Single Parameter","text":"<pre><code>python src/training/train.py hyperparameters.learning_rate=0.01\n</code></pre>"},{"location":"getting-started/configuration/#override-multiple-parameters","title":"Override Multiple Parameters","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.learning_rate=0.01 \\\n  hyperparameters.batch_size=128 \\\n  hyperparameters.num_epochs=20\n</code></pre>"},{"location":"getting-started/configuration/#use-different-config-group","title":"Use Different Config Group","text":"<pre><code># Use ResNet model\npython src/training/train.py model=resnet\n\n# Use MNIST dataset\npython src/training/train.py data=mnist\n\n# Use fast hyperparameters\npython src/training/train.py hyperparameters=fast\n</code></pre>"},{"location":"getting-started/configuration/#combine-overrides","title":"Combine Overrides","text":"<pre><code>python src/training/train.py \\\n  model=resnet \\\n  data=mnist \\\n  hyperparameters=fast\n</code></pre>"},{"location":"getting-started/configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"getting-started/configuration/#main-config-configyaml","title":"Main Config (<code>config.yaml</code>)","text":"<pre><code>defaults:\n  - model: simple_cnn\n  - data: cifar10\n  - hyperparameters: default\n  - _self_\n\nseed: 42\ndevice: auto\nnum_workers: 2\n\noutput_dir: ./outputs\nmodel_save_dir: ./models\nreport_dir: ./reports\n\nlog_interval: 10\nsave_best_only: true\nearly_stopping_patience: 10\n\nexperiment_name: image_classifier\nrun_name: ${now:%Y-%m-%d_%H-%M-%S}\n</code></pre>"},{"location":"getting-started/configuration/#model-configs","title":"Model Configs","text":"<p>Simple CNN (<code>model/simple_cnn.yaml</code>): <pre><code>name: simple_cnn\nnum_classes: 10\ninput_channels: 3\ndropout: 0.5\n</code></pre></p> <p>ResNet (<code>model/resnet.yaml</code>): <pre><code>name: resnet\nnum_classes: 10\ninput_channels: 3\n</code></pre></p>"},{"location":"getting-started/configuration/#data-configs","title":"Data Configs","text":"<p>CIFAR-10 (<code>data/cifar10.yaml</code>): <pre><code>name: cifar10\ndata_dir: ./data/raw\nbatch_size: 64\nval_split: 0.1\nimage_size: 32\nnum_classes: 10\ninput_channels: 3\n\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\n\nclasses:\n  - airplane\n  - automobile\n  - bird\n  # ... etc\n</code></pre></p>"},{"location":"getting-started/configuration/#hyperparameter-configs","title":"Hyperparameter Configs","text":"<p>Default (<code>hyperparameters/default.yaml</code>): <pre><code>learning_rate: 0.001\nweight_decay: 1e-4\nmomentum: 0.9\noptimizer: adam\n\nnum_epochs: 50\nbatch_size: 64\n\nuse_scheduler: true\nscheduler_type: step\nstep_size: 10\ngamma: 0.1\nmin_lr: 1e-6\n\ndropout: 0.5\nlabel_smoothing: 0.0\nclip_grad_norm: 1.0\n</code></pre></p>"},{"location":"getting-started/configuration/#advanced-usage","title":"Advanced Usage","text":""},{"location":"getting-started/configuration/#create-custom-experiment-config","title":"Create Custom Experiment Config","text":"<p>Create <code>configs/experiment/my_exp.yaml</code>:</p> <pre><code># @package _global_\n\ndefaults:\n  - override /model: resnet\n  - override /data: cifar10\n  - override /hyperparameters: default\n\n# Override specific values\nhyperparameters:\n  learning_rate: 0.005\n  num_epochs: 100\n  batch_size: 128\n  use_scheduler: true\n  scheduler_type: cosine\n\nseed: 1234\nexperiment_name: resnet_cifar10_cosine\n</code></pre> <p>Run it:</p> <pre><code>python src/training/train.py --config-name=experiment/my_exp\n</code></pre>"},{"location":"getting-started/configuration/#access-config-in-code","title":"Access Config in Code","text":"<pre><code>import hydra\nfrom omegaconf import DictConfig\n\n@hydra.main(version_base=None, config_path=\"../configs\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    print(cfg.hyperparameters.learning_rate)\n    print(cfg.model.name)\n    # Access nested configs easily\n</code></pre>"},{"location":"getting-started/configuration/#print-configuration","title":"Print Configuration","text":"<pre><code># Print full config\npython src/training/train.py --cfg job\n\n# Print specific group\npython src/training/train.py --cfg job model\n</code></pre>"},{"location":"getting-started/configuration/#composition","title":"Composition","text":"<p>Create different combinations:</p> <pre><code># Experiment 1: Simple CNN on CIFAR-10\npython src/training/train.py \\\n  model=simple_cnn \\\n  data=cifar10 \\\n  experiment_name=exp1_simple_cifar\n\n# Experiment 2: ResNet on MNIST\npython src/training/train.py \\\n  model=resnet \\\n  data=mnist \\\n  experiment_name=exp2_resnet_mnist\n</code></pre>"},{"location":"getting-started/configuration/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/configuration/#learning-rate-tuning","title":"Learning Rate Tuning","text":"<pre><code># Try different learning rates\npython src/training/train.py hyperparameters.learning_rate=0.0001\npython src/training/train.py hyperparameters.learning_rate=0.001\npython src/training/train.py hyperparameters.learning_rate=0.01\n</code></pre>"},{"location":"getting-started/configuration/#batch-size-tuning","title":"Batch Size Tuning","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.batch_size=32 \\\n  data.batch_size=32\n</code></pre>"},{"location":"getting-started/configuration/#scheduler-experiments","title":"Scheduler Experiments","text":"<pre><code># Step scheduler\npython src/training/train.py \\\n  hyperparameters.scheduler_type=step \\\n  hyperparameters.step_size=15\n\n# Cosine annealing\npython src/training/train.py \\\n  hyperparameters.scheduler_type=cosine\n\n# No scheduler\npython src/training/train.py \\\n  hyperparameters.use_scheduler=false\n</code></pre>"},{"location":"getting-started/configuration/#regularization-tuning","title":"Regularization Tuning","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.dropout=0.3 \\\n  hyperparameters.weight_decay=1e-5 \\\n  hyperparameters.label_smoothing=0.1\n</code></pre>"},{"location":"getting-started/configuration/#multi-run","title":"Multi-Run","text":"<p>Run multiple experiments with different parameters:</p> <pre><code>python src/training/train.py -m \\\n  hyperparameters.learning_rate=0.001,0.01,0.1\n</code></pre> <p>This creates separate runs for each learning rate.</p>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Use environment variables in configs:</p> <pre><code>data_dir: ${oc.env:DATA_DIR,./data/raw}\n</code></pre> <p>Then:</p> <pre><code>export DATA_DIR=/path/to/data\npython src/training/train.py\n</code></pre>"},{"location":"getting-started/configuration/#tips-best-practices","title":"Tips &amp; Best Practices","text":"<ol> <li>Never hardcode values - Use configs for everything</li> <li>Create experiment configs - Document your experiments</li> <li>Use meaningful names - <code>experiment_name: resnet_lr001_bs128</code></li> <li>Version configs with git - Track what worked</li> <li>Use defaults - Override only what you need</li> </ol>"},{"location":"getting-started/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/configuration/#config-not-found","title":"Config Not Found","text":"<p>Make sure you're in the project root:</p> <pre><code>cd /path/to/end-to-end-image-classifier\npython src/training/train.py\n</code></pre>"},{"location":"getting-started/configuration/#override-not-working","title":"Override Not Working","text":"<p>Use correct syntax:</p> <pre><code># \u2705 Correct\npython src/training/train.py hyperparameters.learning_rate=0.01\n\n# \u274c Wrong\npython src/training/train.py --learning_rate=0.01\n</code></pre>"},{"location":"getting-started/configuration/#nested-override","title":"Nested Override","text":"<p>For nested values:</p> <pre><code>python src/training/train.py model.dropout=0.3\n</code></pre>"},{"location":"getting-started/configuration/#learn-more","title":"Learn More","text":"<ul> <li>Hydra Documentation</li> <li>OmegaConf Documentation</li> <li>Configuration Patterns</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will help you set up the Image Classifier project on your local machine.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.8 or higher: Download Python</li> <li>Git: Download Git</li> <li>pip: Python package installer (usually comes with Python)</li> <li>(Optional) Docker: Download Docker</li> </ul>"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/Hadayxinchao/end-to-end-image-classifier.git\ncd end-to-end-image-classifier\n</code></pre>"},{"location":"getting-started/installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<p>It's recommended to use a virtual environment to avoid dependency conflicts.</p> Linux/MacWindows <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <pre><code>python -m venv venv\nvenv\\Scripts\\activate\n</code></pre>"},{"location":"getting-started/installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code># Upgrade pip\npip install --upgrade pip\n\n# Install project dependencies\npip install -r requirements.txt\n\n# Install the package in editable mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#4-verify-installation","title":"4. Verify Installation","text":"<p>Run the tests to ensure everything is working:</p> <pre><code>pytest tests/ -v -m \"not slow\"\n</code></pre> <p>If all tests pass, you're ready to go! \u2705</p>"},{"location":"getting-started/installation/#optional-setup-dvc","title":"Optional: Setup DVC","text":"<p>If you want to use Data Version Control:</p> <pre><code># Initialize DVC\ndvc init\n\n# Add remote storage (example: local storage)\ndvc remote add -d storage /tmp/dvc-storage\n</code></pre> <p>For more details, see the Data Versioning Guide.</p>"},{"location":"getting-started/installation/#optional-setup-docker","title":"Optional: Setup Docker","text":"<p>Build the Docker image:</p> <pre><code>docker build -t image-classifier:latest .\n</code></pre> <p>Test the Docker image:</p> <pre><code>docker run --rm image-classifier:latest python --version\n</code></pre>"},{"location":"getting-started/installation/#development-setup","title":"Development Setup","text":"<p>If you plan to contribute to the project:</p>"},{"location":"getting-started/installation/#install-development-dependencies","title":"Install Development Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>This includes: - <code>pytest</code> - Testing framework - <code>black</code> - Code formatter - <code>flake8</code> - Linter - <code>mypy</code> - Type checker</p>"},{"location":"getting-started/installation/#setup-pre-commit-hooks-optional","title":"Setup Pre-commit Hooks (Optional)","text":"<pre><code>pip install pre-commit\npre-commit install\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#issue-pytorch-installation-fails","title":"Issue: PyTorch Installation Fails","text":"<p>If you have GPU and want CUDA support:</p> <pre><code># CUDA 11.8\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\n# CUDA 12.1\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>For CPU-only: <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n</code></pre></p>"},{"location":"getting-started/installation/#issue-permission-denied","title":"Issue: Permission Denied","text":"<p>On Linux/Mac, you might need to use <code>sudo</code> or fix permissions:</p> <pre><code>sudo chown -R $USER:$USER .\n</code></pre>"},{"location":"getting-started/installation/#issue-module-not-found","title":"Issue: Module Not Found","text":"<p>Make sure you installed the package:</p> <pre><code>pip install -e .\n</code></pre> <p>And that your virtual environment is activated.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you've installed the project, check out:</p> <ul> <li>Quick Start Guide - Train your first model</li> <li>Configuration Guide - Learn about Hydra configs</li> <li>User Guide - Detailed training instructions</li> </ul>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>CPU: Dual-core processor</li> <li>RAM: 4 GB</li> <li>Storage: 2 GB free space</li> <li>OS: Linux, macOS, or Windows 10+</li> </ul>"},{"location":"getting-started/installation/#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>CPU: Quad-core processor or better</li> <li>RAM: 8 GB or more</li> <li>GPU: NVIDIA GPU with CUDA support (for faster training)</li> <li>Storage: 10 GB free space (for datasets and models)</li> <li>OS: Ubuntu 20.04+ or equivalent</li> </ul>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter any issues:</p> <ol> <li>Check the Troubleshooting section</li> <li>Search GitHub Issues</li> <li>Create a new issue with details about your problem</li> </ol>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>Get up and running with the Image Classifier in just a few minutes!</p>"},{"location":"getting-started/quickstart/#train-your-first-model","title":"Train Your First Model","text":""},{"location":"getting-started/quickstart/#using-default-configuration","title":"Using Default Configuration","text":"<p>The simplest way to start:</p> <pre><code>python src/training/train.py\n</code></pre> <p>This will:</p> <ul> <li>Download the CIFAR-10 dataset automatically</li> <li>Train a simple CNN model</li> <li>Save the best model to <code>models/</code></li> <li>Generate reports in <code>reports/</code></li> </ul>"},{"location":"getting-started/quickstart/#training-output","title":"Training Output","text":"<p>You'll see output like this:</p> <pre><code>================================================================================\nConfiguration:\ndata:\n  batch_size: 64\n  name: cifar10\n  num_classes: 10\nhyperparameters:\n  learning_rate: 0.001\n  num_epochs: 50\n...\n================================================================================\n\nUsing device: cuda\n\nLoading cifar10 dataset...\nTrain batches: 704\nVal batches: 79\nTest batches: 157\n\nCreating simple_cnn model...\nTotal parameters: 1,234,567\nTrainable parameters: 1,234,567\n\nStarting training for 50 epochs...\n</code></pre>"},{"location":"getting-started/quickstart/#quick-experiments","title":"Quick Experiments","text":""},{"location":"getting-started/quickstart/#change-learning-rate","title":"Change Learning Rate","text":"<pre><code>python src/training/train.py hyperparameters.learning_rate=0.01\n</code></pre>"},{"location":"getting-started/quickstart/#use-different-model","title":"Use Different Model","text":"<pre><code>python src/training/train.py model=resnet\n</code></pre>"},{"location":"getting-started/quickstart/#train-on-mnist-instead","title":"Train on MNIST Instead","text":"<pre><code>python src/training/train.py data=mnist\n</code></pre>"},{"location":"getting-started/quickstart/#fast-training-for-testing","title":"Fast Training (for testing)","text":"<pre><code>python src/training/train.py hyperparameters=fast\n</code></pre> <p>This uses: - 5 epochs instead of 50 - Larger batch size (128) - No learning rate scheduler</p>"},{"location":"getting-started/quickstart/#view-results","title":"View Results","text":"<p>After training, check the results:</p>"},{"location":"getting-started/quickstart/#training-report","title":"Training Report","text":"<pre><code>cat reports/classification_report.txt\n</code></pre>"},{"location":"getting-started/quickstart/#confusion-matrix","title":"Confusion Matrix","text":"<p>The confusion matrix is saved as an image:</p> <pre><code># Linux/Mac\nxdg-open reports/figures/confusion_matrix.png\n\n# Mac\nopen reports/figures/confusion_matrix.png\n\n# Windows\nstart reports/figures/confusion_matrix.png\n</code></pre>"},{"location":"getting-started/quickstart/#training-history","title":"Training History","text":"<p>View the training curves:</p> <pre><code># Linux/Mac\nxdg-open reports/figures/training_history.png\n</code></pre>"},{"location":"getting-started/quickstart/#make-predictions","title":"Make Predictions","text":""},{"location":"getting-started/quickstart/#on-a-single-image","title":"On a Single Image","text":"<pre><code>python src/models/predict.py \\\n  --model_path models/simple_cnn_best.pth \\\n  --image_path path/to/your/image.jpg \\\n  --dataset cifar10\n</code></pre> <p>Example output:</p> <pre><code>Prediction: cat\nConfidence: 0.9234\n\nAll class probabilities:\n  airplane: 0.0012\n  automobile: 0.0045\n  bird: 0.0234\n  cat: 0.9234\n  deer: 0.0123\n  ...\n</code></pre>"},{"location":"getting-started/quickstart/#run-tests","title":"Run Tests","text":"<p>Make sure everything works:</p> <pre><code># Run fast tests only\npytest tests/ -m \"not slow\"\n\n# Run all tests\npytest tests/\n\n# Run with coverage\npytest --cov=src tests/\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":""},{"location":"getting-started/quickstart/#learn-configuration-management","title":"Learn Configuration Management","text":"<p>Hydra makes it easy to manage experiments. Learn more:</p> <pre><code># See all config options\npython src/training/train.py --help\n\n# Use different config group\npython src/training/train.py --config-name=experiment1\n</code></pre> <p>Read the Configuration Guide for details.</p>"},{"location":"getting-started/quickstart/#experiment-tracking","title":"Experiment Tracking","text":"<p>Create a new experiment config:</p> <pre><code>mkdir -p configs/experiment\n</code></pre> <p>Create <code>configs/experiment/my_experiment.yaml</code>:</p> <pre><code># @package _global_\n\ndefaults:\n  - override /model: resnet\n  - override /hyperparameters: default\n\nhyperparameters:\n  learning_rate: 0.005\n  num_epochs: 30\n  batch_size: 128\n\nexperiment_name: my_first_experiment\n</code></pre> <p>Run it:</p> <pre><code>python src/training/train.py --config-name=experiment/my_experiment\n</code></pre>"},{"location":"getting-started/quickstart/#use-docker","title":"Use Docker","text":"<p>Train in a container:</p> <pre><code># Build image\ndocker build -t image-classifier .\n\n# Run training\ndocker run --rm \\\n  -v $(pwd)/models:/app/models \\\n  -v $(pwd)/reports:/app/reports \\\n  image-classifier\n</code></pre>"},{"location":"getting-started/quickstart/#version-your-data","title":"Version Your Data","text":"<p>Track datasets with DVC:</p> <pre><code># Initialize DVC\ndvc init\n\n# Track data\ndvc add data/raw\n\n# Commit to git\ngit add data/raw.dvc .dvc/config\ngit commit -m \"Track data with DVC\"\n</code></pre>"},{"location":"getting-started/quickstart/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/quickstart/#workflow-1-quick-iteration","title":"Workflow 1: Quick Iteration","text":"<pre><code># Fast training for debugging\npython src/training/train.py \\\n  hyperparameters=fast \\\n  hyperparameters.num_epochs=2\n\n# Check results\ncat reports/classification_report.txt\n</code></pre>"},{"location":"getting-started/quickstart/#workflow-2-production-training","title":"Workflow 2: Production Training","text":"<pre><code># Full training with best settings\npython src/training/train.py \\\n  model=resnet \\\n  hyperparameters.num_epochs=100 \\\n  hyperparameters.learning_rate=0.001 \\\n  hyperparameters.use_scheduler=true\n</code></pre>"},{"location":"getting-started/quickstart/#workflow-3-hyperparameter-search","title":"Workflow 3: Hyperparameter Search","text":"<pre><code># Try different learning rates\nfor lr in 0.0001 0.001 0.01; do\n  python src/training/train.py \\\n    hyperparameters.learning_rate=$lr \\\n    experiment_name=lr_search_$lr\ndone\n</code></pre>"},{"location":"getting-started/quickstart/#tips-tricks","title":"Tips &amp; Tricks","text":""},{"location":"getting-started/quickstart/#monitor-gpu-usage","title":"Monitor GPU Usage","text":"<pre><code># While training, in another terminal:\nwatch -n 1 nvidia-smi\n</code></pre>"},{"location":"getting-started/quickstart/#save-outputs-to-custom-directory","title":"Save Outputs to Custom Directory","text":"<pre><code>python src/training/train.py \\\n  output_dir=./outputs/experiment_1 \\\n  model_save_dir=./models/experiment_1\n</code></pre>"},{"location":"getting-started/quickstart/#use-multiple-gpus","title":"Use Multiple GPUs","text":"<p>PyTorch will automatically use all available GPUs. To restrict:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python src/training/train.py\n</code></pre>"},{"location":"getting-started/quickstart/#resume-training","title":"Resume Training","text":"<p>Currently not implemented, but you can add this feature! See Contributing.</p>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quickstart/#out-of-memory","title":"Out of Memory","text":"<p>Reduce batch size:</p> <pre><code>python src/training/train.py hyperparameters.batch_size=32\n</code></pre>"},{"location":"getting-started/quickstart/#training-too-slow","title":"Training Too Slow","text":"<p>Use fast config or fewer epochs:</p> <pre><code>python src/training/train.py hyperparameters.num_epochs=10\n</code></pre>"},{"location":"getting-started/quickstart/#cant-find-model-file","title":"Can't Find Model File","text":"<p>Check the model save directory:</p> <pre><code>ls -la models/\n</code></pre> <p>Models are saved as <code>{model_name}_best.pth</code>.</p>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Configuration Guide - Master Hydra configuration</li> <li>Training Guide - Advanced training techniques</li> <li>CI/CD Setup - Automate your workflow</li> </ul>"},{"location":"guide/data/","title":"Data Management","text":"<p>This guide explains how to manage datasets and data versioning.</p>"},{"location":"guide/data/#dataset-overview","title":"Dataset Overview","text":"<p>The project supports two datasets:</p>"},{"location":"guide/data/#cifar-10","title":"CIFAR-10","text":"<ul> <li>Resolution: 32\u00d732 RGB images</li> <li>Classes: 10 (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)</li> <li>Samples: 50,000 training, 10,000 test</li> <li>Download: Automatic via torchvision</li> </ul>"},{"location":"guide/data/#mnist","title":"MNIST","text":"<ul> <li>Resolution: 28\u00d728 grayscale images</li> <li>Classes: 10 (digits 0-9)</li> <li>Samples: 60,000 training, 10,000 test</li> <li>Download: Automatic via torchvision</li> </ul>"},{"location":"guide/data/#downloading-datasets","title":"Downloading Datasets","text":""},{"location":"guide/data/#automatic-download","title":"Automatic Download","text":"<p>Data downloads automatically on first run:</p> <pre><code>python src/training/train.py data=cifar10\n</code></pre>"},{"location":"guide/data/#manual-download","title":"Manual Download","text":"<p>To pre-download datasets:</p> <pre><code>from src.data.make_dataset import load_cifar10, load_mnist\n\n# Download CIFAR-10\nload_cifar10(data_dir=\"data/raw\")\n\n# Download MNIST\nload_mnist(data_dir=\"data/raw\")\n</code></pre>"},{"location":"guide/data/#data-directory-structure","title":"Data Directory Structure","text":"<pre><code>data/\n\u251c\u2500\u2500 raw/                 # Original datasets\n\u2502   \u251c\u2500\u2500 cifar-10/\n\u2502   \u2514\u2500\u2500 mnist/\n\u2514\u2500\u2500 processed/           # Processed data (if any)\n</code></pre>"},{"location":"guide/data/#data-configuration","title":"Data Configuration","text":"<p>Configure datasets in <code>configs/data/</code>:</p> <p>CIFAR-10 (<code>configs/data/cifar10.yaml</code>): <pre><code>name: cifar10\nnum_classes: 10\ninput_channels: 3\nimage_size: 32\nval_split: 0.2\nclasses: [airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]\n</code></pre></p> <p>MNIST (<code>configs/data/mnist.yaml</code>): <pre><code>name: mnist\nnum_classes: 10\ninput_channels: 1\nimage_size: 28\nval_split: 0.2\nclasses: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre></p>"},{"location":"guide/data/#data-augmentation","title":"Data Augmentation","text":""},{"location":"guide/data/#training-augmentation","title":"Training Augmentation","text":"<p>Images are augmented during training: - Random horizontal flip - Random rotation (\u00b110 degrees) - Random crop - Normalization with ImageNet statistics</p>"},{"location":"guide/data/#validationtest","title":"Validation/Test","text":"<p>No augmentation for validation/test sets.</p>"},{"location":"guide/data/#custom-augmentation","title":"Custom Augmentation","text":"<p>Modify <code>src/data/make_dataset.py</code>:</p> <pre><code>train_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.RandomErasing(p=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n</code></pre>"},{"location":"guide/data/#data-versioning-with-dvc","title":"Data Versioning with DVC","text":""},{"location":"guide/data/#initialize-dvc","title":"Initialize DVC","text":"<pre><code>dvc init\ngit add .dvc .dvcignore\ngit commit -m \"Initialize DVC\"\n</code></pre>"},{"location":"guide/data/#track-data","title":"Track Data","text":"<pre><code>dvc add data/raw/cifar10\ndvc add data/raw/mnist\ngit add data/raw/cifar10.dvc data/raw/mnist.dvc\ngit commit -m \"Add dataset versions\"\n</code></pre>"},{"location":"guide/data/#remote-storage","title":"Remote Storage","text":"<p>Configure remote storage:</p> <pre><code># Local storage\ndvc remote add -d storage /path/to/storage\n\n# S3\ndvc remote add -d storage s3://bucket-name/path\n\n# Google Drive\ndvc remote add -d storage gdrive://folder-id\n</code></pre>"},{"location":"guide/data/#push-data","title":"Push Data","text":"<pre><code>dvc push\n</code></pre>"},{"location":"guide/data/#pull-data","title":"Pull Data","text":"<pre><code>dvc pull\n</code></pre>"},{"location":"guide/data/#data-statistics","title":"Data Statistics","text":"<p>Calculate dataset statistics:</p> <pre><code>from src.data.make_dataset import load_cifar10\nimport numpy as np\n\ntrain_loader, _, _ = load_cifar10(batch_size=1000)\n\nmeans = []\nstds = []\n\nfor images, _ in train_loader:\n    means.append(images.mean([0, 2, 3]))\n    stds.append(images.std([0, 2, 3]))\n\nmean = np.stack(means).mean(axis=0)\nstd = np.stack(stds).mean(axis=0)\n\nprint(f\"Mean: {mean}\")\nprint(f\"Std: {std}\")\n</code></pre>"},{"location":"guide/data/#custom-dataset","title":"Custom Dataset","text":"<p>To add custom datasets:</p> <ol> <li>Create dataset loader in <code>src/data/make_dataset.py</code></li> <li>Add configuration file in <code>configs/data/</code></li> <li>Update training script to support new dataset</li> </ol> <p>Example:</p> <pre><code>def load_custom_dataset(data_dir, batch_size=32, val_split=0.2):\n    dataset = ImageFolder(data_dir, transform=transform)\n    val_size = int(len(dataset) * val_split)\n    train_size = len(dataset) - val_size\n\n    train_set, val_set = torch.utils.data.random_split(\n        dataset, [train_size, val_size]\n    )\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"guide/data/#data-issues-solutions","title":"Data Issues &amp; Solutions","text":""},{"location":"guide/data/#disk-space","title":"Disk Space","text":"<ul> <li>CIFAR-10: ~170MB</li> <li>MNIST: ~12MB</li> <li>Ensure sufficient disk space before downloading</li> </ul>"},{"location":"guide/data/#network-issues","title":"Network Issues","text":"<ul> <li>Use <code>--offline</code> flag if data already downloaded</li> <li>Check internet connection for automatic downloads</li> </ul>"},{"location":"guide/data/#corrupted-files","title":"Corrupted Files","text":"<ul> <li>Delete cached files and re-download:   <pre><code>rm -rf data/raw\npython src/training/train.py data=cifar10\n</code></pre></li> </ul>"},{"location":"guide/experiments/","title":"Experiments &amp; Tracking","text":"<p>Track and manage machine learning experiments.</p>"},{"location":"guide/experiments/#experiment-configuration","title":"Experiment Configuration","text":""},{"location":"guide/experiments/#using-hydra-multi-run","title":"Using Hydra Multi-run","text":"<p>Run multiple experiments with different configurations:</p> <pre><code># Grid search over learning rates\npython src/training/train.py -m \\\n    hyperparameters.learning_rate=0.0001,0.001,0.01\n\n# Different models and datasets\npython src/training/train.py -m \\\n    model=simple_cnn,resnet \\\n    data=cifar10,mnist\n</code></pre>"},{"location":"guide/experiments/#output-structure","title":"Output Structure","text":"<p>Results are saved in <code>multirun/</code> with timestamps:</p> <pre><code>multirun/\n\u251c\u2500\u2500 2025-12-10/\n\u2502   \u251c\u2500\u2500 01-00-00/outputs/\n\u2502   \u251c\u2500\u2500 02-00-00/outputs/\n\u2502   \u2514\u2500\u2500 03-00-00/outputs/\n</code></pre>"},{"location":"guide/experiments/#comparing-results","title":"Comparing Results","text":""},{"location":"guide/experiments/#manual-comparison","title":"Manual Comparison","text":"<p>Compare metrics from different runs:</p> <pre><code>from pathlib import Path\nimport json\n\nruns_dir = Path(\"multirun/2025-12-10\")\n\nfor run_dir in sorted(runs_dir.iterdir()):\n    config_file = run_dir / \"outputs\" / \".hydra\" / \"config.yaml\"\n    if config_file.exists():\n        print(f\"\\nRun: {run_dir.name}\")\n        with open(config_file) as f:\n            print(f.read())\n</code></pre>"},{"location":"guide/experiments/#experiment-logs","title":"Experiment Logs","text":"<p>Training logs are saved in <code>outputs/</code> with timestamps:</p> <pre><code>outputs/\n\u251c\u2500\u2500 2025-12-10/\n\u2502   \u251c\u2500\u2500 .hydra/\n\u2502   \u2502   \u251c\u2500\u2500 config.yaml\n\u2502   \u2502   \u251c\u2500\u2500 hydra.yaml\n\u2502   \u2502   \u2514\u2500\u2500 launcher.yaml\n\u2502   \u2514\u2500\u2500 .hydra.log\n</code></pre>"},{"location":"guide/experiments/#model-checkpoints","title":"Model Checkpoints","text":"<p>Best models are saved during training:</p> <pre><code>models/\n\u251c\u2500\u2500 simple_cnn_best.pth\n\u251c\u2500\u2500 resnet_best.pth\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guide/experiments/#tracking-with-version-control","title":"Tracking with Version Control","text":""},{"location":"guide/experiments/#commit-experiment","title":"Commit Experiment","text":"<pre><code>git add .\ngit commit -m \"Exp: SimpleCNN on CIFAR-10, LR=0.001, 50% dropout\"\n</code></pre>"},{"location":"guide/experiments/#tag-important-runs","title":"Tag Important Runs","text":"<pre><code>git tag exp-001-baseline\ngit tag exp-002-with-augmentation\n</code></pre>"},{"location":"guide/experiments/#analysis-visualization","title":"Analysis &amp; Visualization","text":""},{"location":"guide/experiments/#plot-training-history","title":"Plot Training History","text":"<pre><code>import matplotlib.pyplot as plt\nfrom pathlib import Path\n\ndef plot_experiment(run_dir):\n    history_file = run_dir / \"reports\" / \"training_history.png\"\n    confusion_file = run_dir / \"reports\" / \"confusion_matrix.png\"\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n    # Load and display images\n    from PIL import Image\n    axes[0].imshow(Image.open(history_file))\n    axes[1].imshow(Image.open(confusion_file))\n\n    plt.show()\n</code></pre>"},{"location":"guide/experiments/#compare-multiple-runs","title":"Compare Multiple Runs","text":"<pre><code>import pandas as pd\nfrom pathlib import Path\n\ndef compare_runs(experiment_dir):\n    results = []\n\n    for run_dir in experiment_dir.iterdir():\n        config = run_dir / \".hydra\" / \"config.yaml\"\n        metrics_file = run_dir / \"reports\" / \"metrics.json\"\n\n        if config.exists() and metrics_file.exists():\n            results.append({\n                'run': run_dir.name,\n                'config': config,\n                'metrics': metrics_file\n            })\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"guide/experiments/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Name experiments clearly <pre><code>git commit -m \"Exp: SimpleCNN with batch norm, LR=0.001\"\n</code></pre></p> </li> <li> <p>Track hyperparameters in config</p> </li> <li>Use Hydra for consistent tracking</li> <li> <p>Save config.yaml with each run</p> </li> <li> <p>Save model and logs</p> </li> <li>All runs save to timestamped directories</li> <li> <p>Easy to reproduce past experiments</p> </li> <li> <p>Document findings</p> </li> <li>Keep experiment notes in README or wiki</li> <li> <p>Record best configurations</p> </li> <li> <p>Version control checkpoints <pre><code>dvc add models/simple_cnn_best.pth\ngit add models/simple_cnn_best.pth.dvc\n</code></pre></p> </li> </ol>"},{"location":"guide/experiments/#integration-with-mlflow-optional","title":"Integration with MLflow (Optional)","text":"<p>To add MLflow tracking:</p> <pre><code>pip install mlflow\n</code></pre> <p>Then modify <code>src/training/train.py</code>:</p> <pre><code>import mlflow\n\nwith mlflow.start_run():\n    mlflow.log_params(OmegaConf.to_container(cfg))\n    # ... training code ...\n    mlflow.log_metrics({'val_acc': val_acc})\n    mlflow.pytorch.log_model(model, \"models\")\n</code></pre> <p>View results:</p> <pre><code>mlflow ui\n</code></pre> <p>Then open <code>http://localhost:5000</code></p>"},{"location":"guide/fastapi/","title":"FastAPI Web Interface Guide","text":""},{"location":"guide/fastapi/#overview","title":"Overview","text":"<p>The project now includes a FastAPI-based web interface for image classification with a beautiful, interactive UI.</p>"},{"location":"guide/fastapi/#features","title":"Features","text":"<ul> <li>\ud83c\udfa8 Modern UI - Beautiful gradient design with drag-and-drop support</li> <li>\ud83d\ude80 Fast API - RESTful API built with FastAPI</li> <li>\ud83d\udcca Real-time Results - Instant predictions with confidence scores</li> <li>\ud83c\udfaf Batch Processing - Support for multiple image uploads</li> <li>\ud83d\udcf1 Responsive - Works on desktop and mobile devices</li> </ul>"},{"location":"guide/fastapi/#quick-start","title":"Quick Start","text":""},{"location":"guide/fastapi/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"guide/fastapi/#2-run-the-api-server","title":"2. Run the API Server","text":"<pre><code># Run directly\nuvicorn src.app.main:app --host 0.0.0.0 --port 8000 --reload\n\n# Or using Python\npython -m src.app.main\n</code></pre>"},{"location":"guide/fastapi/#3-access-the-interface","title":"3. Access the Interface","text":"<p>Open your browser and navigate to: - Web UI: http://localhost:8000 - API Docs: http://localhost:8000/docs - Health Check: http://localhost:8000/health</p>"},{"location":"guide/fastapi/#using-the-web-interface","title":"Using the Web Interface","text":"<ol> <li>Upload Image:</li> <li>Click the upload area or drag and drop an image</li> <li> <p>Supports JPG, PNG (max 5MB)</p> </li> <li> <p>Classify:</p> </li> <li>Click \"Classify Image\" button</li> <li> <p>Wait for results</p> </li> <li> <p>View Results:</p> </li> <li>Predicted class with confidence score</li> <li>Visual confidence bar</li> <li>All class probabilities ranked</li> </ol>"},{"location":"guide/fastapi/#api-endpoints","title":"API Endpoints","text":""},{"location":"guide/fastapi/#get","title":"GET <code>/</code>","text":"<p>Main web interface (HTML page)</p>"},{"location":"guide/fastapi/#get-health","title":"GET <code>/health</code>","text":"<p>Health check endpoint</p> <p>Response: <pre><code>{\n  \"status\": \"healthy\",\n  \"model_loaded\": true,\n  \"device\": \"cpu\",\n  \"dataset\": \"cifar10\"\n}\n</code></pre></p>"},{"location":"guide/fastapi/#post-predict","title":"POST <code>/predict</code>","text":"<p>Predict single image</p> <p>Request: - <code>file</code>: Image file (multipart/form-data)</p> <p>Response: <pre><code>{\n  \"predicted_class\": \"airplane\",\n  \"confidence\": 0.9523,\n  \"all_probabilities\": [\n    {\"class\": \"airplane\", \"probability\": 0.9523},\n    {\"class\": \"ship\", \"probability\": 0.0234},\n    ...\n  ]\n}\n</code></pre></p>"},{"location":"guide/fastapi/#post-predict_batch","title":"POST <code>/predict_batch</code>","text":"<p>Predict multiple images</p> <p>Request: - <code>files</code>: List of image files</p> <p>Response: <pre><code>{\n  \"results\": [\n    {\n      \"filename\": \"img1.jpg\",\n      \"predicted_class\": \"cat\",\n      \"confidence\": 0.8765\n    },\n    ...\n  ]\n}\n</code></pre></p>"},{"location":"guide/fastapi/#get-classes","title":"GET <code>/classes</code>","text":"<p>Get available classes</p> <p>Response: <pre><code>{\n  \"dataset\": \"cifar10\",\n  \"classes\": [\"airplane\", \"automobile\", ...]\n}\n</code></pre></p>"},{"location":"guide/fastapi/#using-the-api-programmatically","title":"Using the API Programmatically","text":""},{"location":"guide/fastapi/#python-example","title":"Python Example","text":"<pre><code>import requests\n\n# Single prediction\nurl = \"http://localhost:8000/predict\"\nfiles = {\"file\": open(\"image.jpg\", \"rb\")}\nresponse = requests.post(url, files=files)\nresult = response.json()\n\nprint(f\"Predicted: {result['predicted_class']}\")\nprint(f\"Confidence: {result['confidence']:.2%}\")\n</code></pre>"},{"location":"guide/fastapi/#curl-example","title":"cURL Example","text":"<pre><code># Single prediction\ncurl -X POST \"http://localhost:8000/predict\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"file=@image.jpg\"\n\n# Health check\ncurl http://localhost:8000/health\n</code></pre>"},{"location":"guide/fastapi/#javascript-example","title":"JavaScript Example","text":"<pre><code>// Upload and classify image\nasync function classifyImage(file) {\n  const formData = new FormData();\n  formData.append('file', file);\n\n  const response = await fetch('http://localhost:8000/predict', {\n    method: 'POST',\n    body: formData\n  });\n\n  const result = await response.json();\n  console.log('Prediction:', result.predicted_class);\n  console.log('Confidence:', result.confidence);\n}\n</code></pre>"},{"location":"guide/fastapi/#docker-deployment","title":"Docker Deployment","text":""},{"location":"guide/fastapi/#build-and-run","title":"Build and Run","text":"<pre><code># Build image\ndocker build -t image-classifier .\n\n# Run container (API will start automatically)\ndocker run -p 8000:8000 \\\n  -v $(pwd)/models:/app/models \\\n  image-classifier\n</code></pre>"},{"location":"guide/fastapi/#access-from-docker","title":"Access from Docker","text":"<ul> <li>Web UI: http://localhost:8000</li> <li>API Docs: http://localhost:8000/docs</li> </ul>"},{"location":"guide/fastapi/#override-command-for-training","title":"Override Command (for training)","text":"<pre><code># Run training instead of API\ndocker run image-classifier python src/training/train.py\n\n# Run inference CLI\ndocker run image-classifier \\\n  python src/models/predict.py \\\n  --model_path models/simple_cnn_best.pth \\\n  --image_path data/test.jpg \\\n  --dataset cifar10\n</code></pre>"},{"location":"guide/fastapi/#configuration","title":"Configuration","text":""},{"location":"guide/fastapi/#change-model","title":"Change Model","text":"<p>Modify the startup function in <code>src/app/main.py</code>:</p> <pre><code>@app.on_event(\"startup\")\nasync def startup_event():\n    model_path = \"models/your_model.pth\"\n    initialize_model(\n        model_path=model_path,\n        model_name=\"simple_cnn\",  # or \"resnet\"\n        dataset=\"cifar10\"  # or \"mnist\"\n    )\n</code></pre>"},{"location":"guide/fastapi/#change-port","title":"Change Port","text":"<pre><code>uvicorn src.app.main:app --host 0.0.0.0 --port 5000\n</code></pre>"},{"location":"guide/fastapi/#enable-gpu","title":"Enable GPU","text":"<p>The API automatically detects and uses GPU if available:</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"},{"location":"guide/fastapi/#cors-configuration","title":"CORS Configuration","text":"<p>Edit CORS settings in <code>src/app/main.py</code>:</p> <pre><code>app.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"https://yourdomain.com\"],  # Specific domains\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</code></pre>"},{"location":"guide/fastapi/#production-deployment","title":"Production Deployment","text":""},{"location":"guide/fastapi/#using-gunicorn","title":"Using Gunicorn","text":"<pre><code>pip install gunicorn\n\ngunicorn src.app.main:app \\\n  --workers 4 \\\n  --worker-class uvicorn.workers.UvicornWorker \\\n  --bind 0.0.0.0:8000\n</code></pre>"},{"location":"guide/fastapi/#environment-variables","title":"Environment Variables","text":"<pre><code>export MODEL_PATH=/path/to/model.pth\nexport DATASET=cifar10\nexport PORT=8000\n</code></pre>"},{"location":"guide/fastapi/#nginx-reverse-proxy","title":"Nginx Reverse Proxy","text":"<pre><code>server {\n    listen 80;\n    server_name your-domain.com;\n\n    location / {\n        proxy_pass http://127.0.0.1:8000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n</code></pre>"},{"location":"guide/fastapi/#api-documentation","title":"API Documentation","text":"<p>FastAPI automatically generates interactive API documentation:</p> <ul> <li>Swagger UI: http://localhost:8000/docs</li> <li>ReDoc: http://localhost:8000/redoc</li> </ul> <p>These interfaces allow you to: - Test all endpoints interactively - See request/response schemas - Download OpenAPI specification</p>"},{"location":"guide/fastapi/#testing-the-api","title":"Testing the API","text":""},{"location":"guide/fastapi/#unit-tests","title":"Unit Tests","text":"<pre><code># tests/test_api.py\nfrom fastapi.testclient import TestClient\nfrom src.app.main import app\n\nclient = TestClient(app)\n\ndef test_health():\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"healthy\"\n\ndef test_predict():\n    with open(\"test_image.jpg\", \"rb\") as f:\n        response = client.post(\n            \"/predict\",\n            files={\"file\": (\"test.jpg\", f, \"image/jpeg\")}\n        )\n    assert response.status_code == 200\n    assert \"predicted_class\" in response.json()\n</code></pre>"},{"location":"guide/fastapi/#load-testing","title":"Load Testing","text":"<pre><code># Install locust\npip install locust\n\n# Create locustfile.py and run\nlocust -f locustfile.py --host http://localhost:8000\n</code></pre>"},{"location":"guide/fastapi/#monitoring","title":"Monitoring","text":""},{"location":"guide/fastapi/#logging","title":"Logging","text":"<p>Add logging configuration:</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@app.post(\"/predict\")\nasync def predict_image_api(file: UploadFile = File(...)):\n    logger.info(f\"Received prediction request for {file.filename}\")\n    # ... rest of code\n</code></pre>"},{"location":"guide/fastapi/#metrics","title":"Metrics","text":"<p>Install Prometheus client:</p> <pre><code>pip install prometheus-fastapi-instrumentator\n</code></pre> <p>Add to app:</p> <pre><code>from prometheus_fastapi_instrumentator import Instrumentator\n\nInstrumentator().instrument(app).expose(app)\n</code></pre>"},{"location":"guide/fastapi/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/fastapi/#model-not-found","title":"Model Not Found","text":"<p>Ensure model exists: <pre><code>ls -la models/simple_cnn_best.pth\n</code></pre></p>"},{"location":"guide/fastapi/#port-already-in-use","title":"Port Already in Use","text":"<p>Change port or kill existing process: <pre><code>lsof -ti:8000 | xargs kill -9\n</code></pre></p>"},{"location":"guide/fastapi/#large-file-upload","title":"Large File Upload","text":"<p>Increase max file size in <code>main.py</code>: <pre><code>from fastapi import FastAPI, File, UploadFile\nfrom fastapi.middleware.trustedhost import TrustedHostMiddleware\n\napp = FastAPI()\napp.add_middleware(TrustedHostMiddleware, max_upload_size=10485760)  # 10MB\n</code></pre></p>"},{"location":"guide/fastapi/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Reduce batch size or use CPU: <pre><code>device = torch.device(\"cpu\")  # Force CPU\n</code></pre></p>"},{"location":"guide/fastapi/#best-practices","title":"Best Practices","text":"<ol> <li>Model Versioning: Include version in API response</li> <li>Input Validation: Validate image size and format</li> <li>Error Handling: Return meaningful error messages</li> <li>Rate Limiting: Implement rate limiting for production</li> <li>Caching: Cache predictions for identical images</li> <li>Async Processing: Use background tasks for batch processing</li> <li>Security: Add authentication for production APIs</li> </ol>"},{"location":"guide/fastapi/#next-steps","title":"Next Steps","text":"<ul> <li>Add authentication (JWT, OAuth2)</li> <li>Implement rate limiting</li> <li>Add model versioning</li> <li>Set up CI/CD for deployment</li> <li>Add monitoring and alerting</li> <li>Implement A/B testing for models</li> <li>Add feedback mechanism for predictions</li> </ul>"},{"location":"guide/fastapi/#resources","title":"Resources","text":"<ul> <li>FastAPI Documentation</li> <li>Uvicorn Documentation</li> <li>Production Deployment Guide</li> </ul>"},{"location":"guide/inference/","title":"Making Predictions","text":"<p>This guide explains how to use trained models for inference.</p>"},{"location":"guide/inference/#batch-prediction","title":"Batch Prediction","text":"<p>Predict on a batch of images:</p> <pre><code>import torch\nfrom pathlib import Path\nfrom src.models.model import get_model\nfrom src.models.predict import predict_batch\nfrom src.data.make_dataset import load_cifar10\n\n# Load model\nmodel = get_model(\n    model_name=\"simple_cnn\",\n    num_classes=10,\n    input_channels=3,\n    image_size=32\n)\n\n# Load checkpoint\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Load data\n_, _, test_loader = load_cifar10(batch_size=32)\n\n# Make predictions\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\npredictions, true_labels = predict_batch(model, test_loader, device)\n</code></pre>"},{"location":"guide/inference/#single-image-prediction","title":"Single Image Prediction","text":"<p>Predict on a single image:</p> <pre><code>import torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom src.models.model import get_model\n\n# Load model\nmodel = get_model(\n    model_name=\"simple_cnn\",\n    num_classes=10,\n    input_channels=3,\n    image_size=32\n)\n\n# Load checkpoint\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Prepare image\ntransform = transforms.Compose([\n    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\nimage = Image.open(\"path/to/image.jpg\")\nimage_tensor = transform(image).unsqueeze(0)\n\n# Predict\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nimage_tensor = image_tensor.to(device)\n\nwith torch.no_grad():\n    output = model(image_tensor)\n    probabilities = torch.softmax(output, dim=1)\n    prediction = output.argmax(dim=1).item()\n    confidence = probabilities[0][prediction].item()\n\nprint(f\"Prediction: {prediction}, Confidence: {confidence:.2%}\")\n</code></pre>"},{"location":"guide/inference/#command-line-inference","title":"Command Line Inference","text":"<p>Use the prediction script:</p> <pre><code>python src/models/predict.py \\\n    --model_path models/simple_cnn_best.pth \\\n    --image_path path/to/image.jpg \\\n    --device cuda\n</code></pre>"},{"location":"guide/inference/#batch-processing","title":"Batch Processing","text":"<p>Process multiple images:</p> <pre><code>import torch\nfrom pathlib import Path\nfrom src.models.model import get_model\nfrom src.models.predict import predict_batch\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\n\n# Prepare data\ntransform = transforms.Compose([\n    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\ndataset = ImageFolder(\"path/to/images\", transform=transform)\ndata_loader = DataLoader(dataset, batch_size=32, num_workers=4)\n\n# Load model\nmodel = get_model(\n    model_name=\"simple_cnn\",\n    num_classes=10,\n    input_channels=3,\n    image_size=32\n)\n\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Predict\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\npredictions, labels = predict_batch(model, data_loader, device)\n</code></pre>"},{"location":"guide/inference/#output-format","title":"Output Format","text":"<p>Predictions are returned as: - predictions: Numpy array of predicted class indices - true_labels: Numpy array of true labels (if available) - probabilities: Softmax probabilities for each class</p>"},{"location":"guide/inference/#class-labels","title":"Class Labels","text":"<p>CIFAR-10 classes: - 0: airplane - 1: automobile - 2: bird - 3: cat - 4: deer - 5: dog - 6: frog - 7: horse - 8: ship - 9: truck</p> <p>MNIST classes: - 0-9: Digits</p>"},{"location":"guide/inference/#performance-metrics","title":"Performance Metrics","text":"<p>Calculate prediction metrics:</p> <pre><code>from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report\n)\n\naccuracy = accuracy_score(true_labels, predictions)\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\nf1 = f1_score(true_labels, predictions, average='macro')\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(true_labels, predictions))\n</code></pre>"},{"location":"guide/training/","title":"Training Models","text":"<p>This guide explains how to train image classification models using this project.</p>"},{"location":"guide/training/#basic-training","title":"Basic Training","text":""},{"location":"guide/training/#cifar-10","title":"CIFAR-10","text":"<p>Train on CIFAR-10 dataset with default configuration:</p> <pre><code>python src/training/train.py data=cifar10\n</code></pre>"},{"location":"guide/training/#mnist","title":"MNIST","text":"<p>Train on MNIST dataset:</p> <pre><code>python src/training/train.py data=mnist\n</code></pre>"},{"location":"guide/training/#configuration-override","title":"Configuration Override","text":"<p>Override hyperparameters via command line:</p> <pre><code># Custom learning rate\npython src/training/train.py hyperparameters.learning_rate=0.001\n\n# Custom batch size\npython src/training/train.py hyperparameters.batch_size=128\n\n# Different optimizer\npython src/training/train.py hyperparameters.optimizer=adam\n\n# Multiple overrides\npython src/training/train.py \\\n  data=cifar10 \\\n  hyperparameters.num_epochs=100 \\\n  hyperparameters.learning_rate=0.001 \\\n  model=resnet\n</code></pre>"},{"location":"guide/training/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":""},{"location":"guide/training/#step-decay","title":"Step Decay","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.use_scheduler=true \\\n  hyperparameters.scheduler_type=step \\\n  hyperparameters.step_size=30 \\\n  hyperparameters.gamma=0.1\n</code></pre>"},{"location":"guide/training/#cosine-annealing","title":"Cosine Annealing","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.use_scheduler=true \\\n  hyperparameters.scheduler_type=cosine \\\n  hyperparameters.min_lr=1e-5\n</code></pre>"},{"location":"guide/training/#reduce-on-plateau","title":"Reduce on Plateau","text":"<pre><code>python src/training/train.py \\\n  hyperparameters.use_scheduler=true \\\n  hyperparameters.scheduler_type=plateau\n</code></pre>"},{"location":"guide/training/#model-selection","title":"Model Selection","text":""},{"location":"guide/training/#simplecnn","title":"SimpleCNN","text":"<pre><code>python src/training/train.py model=simple_cnn\n</code></pre>"},{"location":"guide/training/#resnet","title":"ResNet","text":"<pre><code>python src/training/train.py model=resnet\n</code></pre>"},{"location":"guide/training/#training-output","title":"Training Output","text":"<p>Training generates: - Best model: <code>models/simple_cnn_best.pth</code> - Reports: <code>reports/</code> directory with:   - <code>confusion_matrix.png</code> - Confusion matrix visualization   - <code>classification_report.txt</code> - Detailed metrics   - <code>training_history.png</code> - Loss and accuracy curves</p>"},{"location":"guide/training/#early-stopping","title":"Early Stopping","text":"<p>Configure early stopping patience:</p> <pre><code>python src/training/train.py early_stopping_patience=10\n</code></pre>"},{"location":"guide/training/#device-configuration","title":"Device Configuration","text":""},{"location":"guide/training/#auto-detect-default","title":"Auto-detect (default)","text":"<pre><code>python src/training/train.py device=auto\n</code></pre>"},{"location":"guide/training/#explicit-gpu","title":"Explicit GPU","text":"<pre><code>python src/training/train.py device=cuda:0\n</code></pre>"},{"location":"guide/training/#cpu-only","title":"CPU only","text":"<pre><code>python src/training/train.py device=cpu\n</code></pre>"},{"location":"guide/training/#monitoring-training","title":"Monitoring Training","text":"<p>The training script provides real-time updates: - Loss and accuracy per epoch - Learning rate schedule - Best validation accuracy - Training history plots</p>"},{"location":"guide/training/#tips-for-better-results","title":"Tips for Better Results","text":"<ol> <li>Data augmentation: Adjust in <code>src/data/make_dataset.py</code></li> <li>Batch size: Larger batches for better GPU utilization</li> <li>Learning rate: Start with 0.001 and tune based on results</li> <li>Epochs: Use early stopping to avoid overfitting</li> <li>Optimizer: Adam for adaptive learning, SGD for stable training</li> </ol>"},{"location":"mlops/automation/","title":"MLOps Automation Guide","text":"<p>This comprehensive guide covers all automation features implemented in the project.</p>"},{"location":"mlops/automation/#overview","title":"\ud83d\udccb Overview","text":"<p>The project includes complete automation for:</p> <ol> <li>\u2705 Pre-commit hooks - Auto format and quality checks</li> <li>\ud83d\udd12 Security scanning - Bandit for vulnerability detection</li> <li>\ud83d\udee1\ufe0f Dependency checking - Safety and pip-audit</li> <li>\ud83d\udc33 Docker optimization - Multi-stage production builds</li> <li>\u2638\ufe0f Kubernetes deployment - Production-ready manifests</li> <li>\ud83d\udcca W&amp;B Tracking - Weights, biases, and experiment tracking</li> </ol>"},{"location":"mlops/automation/#quick-start-3-minutes","title":"\ud83d\ude80 Quick Start (3 Minutes)","text":""},{"location":"mlops/automation/#1-setup-environment-1-minute","title":"1. Setup Environment (1 minute)","text":"<pre><code># Run automated setup script\n./scripts/quickstart.sh\n</code></pre>"},{"location":"mlops/automation/#2-install-pre-commit-hooks-30-seconds","title":"2. Install Pre-commit Hooks (30 seconds)","text":"<pre><code>make setup-precommit\n</code></pre>"},{"location":"mlops/automation/#3-setup-wb-1-minute","title":"3. Setup W&amp;B (1 minute)","text":"<pre><code>make wandb-setup\n# Enter API key from: https://wandb.ai/authorize\n</code></pre>"},{"location":"mlops/automation/#4-start-training-30-seconds-to-start","title":"4. Start Training! (30 seconds to start)","text":"<pre><code># Train with W&amp;B tracking\nmake train-wandb\n\n# Or quick test training\nmake train-fast\n</code></pre>"},{"location":"mlops/automation/#common-commands","title":"\ud83d\udca1 Common Commands","text":""},{"location":"mlops/automation/#development","title":"Development","text":"<pre><code>make format              # Format code\nmake lint                # Check linting\nmake test                # Run tests\nmake quality-check       # All quality checks\n</code></pre>"},{"location":"mlops/automation/#security","title":"Security","text":"<pre><code>make security-scan       # Scan code\nmake vulnerability-check # Check dependencies\nmake security-all        # All security checks\n</code></pre>"},{"location":"mlops/automation/#training","title":"Training","text":"<pre><code>make train-wandb         # Train with W&amp;B\nmake train-wandb-offline # W&amp;B offline mode\nmake train-both          # MLflow + W&amp;B\n</code></pre>"},{"location":"mlops/automation/#docker","title":"Docker","text":"<pre><code>make docker-build-optimized  # Build optimized image\nmake docker-compose-up       # Start full stack\nmake docker-compose-down     # Stop stack\n</code></pre>"},{"location":"mlops/automation/#kubernetes","title":"Kubernetes","text":"<pre><code>make k8s-deploy          # Deploy to K8s\nmake k8s-status          # Check status\nmake k8s-logs            # View logs\nmake k8s-port-forward    # Port forward to localhost\n</code></pre>"},{"location":"mlops/automation/#view-all-commands","title":"View All Commands","text":"<pre><code>make help                # List all available commands\n</code></pre>"},{"location":"mlops/automation/#1-pre-commit-hooks-auto-formatting","title":"\u2705 1. Pre-commit Hooks &amp; Auto-formatting","text":""},{"location":"mlops/automation/#features","title":"Features","text":"<ul> <li>\ud83c\udfa8 Auto-format code with Black</li> <li>\ud83d\udce6 Auto-sort imports with isort</li> <li>\ud83d\udd0d Linting with flake8</li> <li>\ud83d\udd12 Security scan with Bandit</li> <li>\ud83d\udcdd Type checking with mypy</li> <li>\ud83d\udd10 Secret detection</li> <li>\ud83d\udee1\ufe0f Dependency safety checks</li> <li>\ud83d\udc33 Dockerfile linting with hadolint</li> </ul>"},{"location":"mlops/automation/#installation","title":"Installation","text":"<pre><code># Install pre-commit\npip install pre-commit\n\n# Install hooks\npre-commit install\n\n# Run on all files\npre-commit run --all-files\n</code></pre>"},{"location":"mlops/automation/#files","title":"Files","text":"<ul> <li><code>.pre-commit-config.yaml</code> - Hook configuration</li> <li><code>pyproject.toml</code> - Tool configurations</li> <li><code>.secrets.baseline</code> - Baseline for secret detection</li> </ul>"},{"location":"mlops/automation/#hooks-included","title":"Hooks Included","text":"<ul> <li>Black - Code formatting</li> <li>isort - Import sorting</li> <li>flake8 - Linting with docstring checks</li> <li>Bandit - Security scanning</li> <li>mypy - Type checking</li> <li>detect-secrets - Secret detection</li> <li>Safety - Dependency security</li> <li>hadolint - Dockerfile linting</li> <li>YAML formatter - YAML file formatting</li> </ul>"},{"location":"mlops/automation/#2-security-scanning-with-bandit","title":"\ud83d\udd12 2. Security Scanning with Bandit","text":""},{"location":"mlops/automation/#features_1","title":"Features","text":"<ul> <li>Automatic code security vulnerability scanning</li> <li>JSON and text report generation</li> <li>Integrated into pre-commit hooks</li> <li>Integrated into CI/CD pipeline</li> </ul>"},{"location":"mlops/automation/#usage","title":"Usage","text":"<pre><code># Run security scan\nmake security-scan\n\n# Or run script directly\n./scripts/security_scan.sh\n</code></pre>"},{"location":"mlops/automation/#configuration","title":"Configuration","text":"<p>Security configuration is in <code>pyproject.toml</code>:</p> <pre><code>[tool.bandit]\nexclude_dirs = [\"tests\", \"venv\", \".venv\"]\ntests = [\"B201\", \"B301\"]\nskips = [\"B101\", \"B601\"]\n</code></pre>"},{"location":"mlops/automation/#reports","title":"Reports","text":"<ul> <li><code>reports/bandit-report.json</code> - JSON format</li> <li>Console output with detailed findings</li> </ul>"},{"location":"mlops/automation/#3-dependency-vulnerability-checking","title":"\ud83d\udee1\ufe0f 3. Dependency Vulnerability Checking","text":""},{"location":"mlops/automation/#tools","title":"Tools","text":"<ul> <li>Safety - Check Python packages for known vulnerabilities</li> <li>pip-audit - Comprehensive dependency scanning</li> <li>Trivy - Docker image scanning (optional)</li> </ul>"},{"location":"mlops/automation/#usage_1","title":"Usage","text":"<pre><code># Check all vulnerabilities\nmake vulnerability-check\n\n# Or run script directly\n./scripts/check_vulnerabilities.sh\n</code></pre>"},{"location":"mlops/automation/#reports_1","title":"Reports","text":"<ul> <li><code>reports/safety-report.json</code> - Safety scan results</li> <li><code>reports/pip-audit-report.json</code> - pip-audit results</li> </ul>"},{"location":"mlops/automation/#4-docker-optimization","title":"\ud83d\udc33 4. Docker Optimization","text":""},{"location":"mlops/automation/#features_2","title":"Features","text":"<ul> <li>Multi-stage build (reduced image size)</li> <li>Non-root user (enhanced security)</li> <li>Health checks</li> <li>Complete stack with monitoring</li> </ul>"},{"location":"mlops/automation/#build-optimized-image","title":"Build Optimized Image","text":"<pre><code># Build optimized Docker image\nmake docker-build-optimized\n\n# Compare with standard image\nmake docker-size-compare\n</code></pre>"},{"location":"mlops/automation/#docker-compose-stack","title":"Docker Compose Stack","text":"<p>The <code>docker-compose.yml</code> includes: - App - Main application (port 8000) - MLflow - Experiment tracking (port 5000) - Prometheus - Metrics collection (port 9090) - Grafana - Monitoring dashboard (port 3000)</p> <pre><code># Start full stack\nmake docker-compose-up\n\n# Stop stack\nmake docker-compose-down\n</code></pre>"},{"location":"mlops/automation/#access-services","title":"Access Services","text":"<ul> <li>App: http://localhost:8000</li> <li>MLflow: http://localhost:5000</li> <li>Prometheus: http://localhost:9090</li> <li>Grafana: http://localhost:3000 (admin/admin)</li> </ul>"},{"location":"mlops/automation/#5-kubernetes-deployment","title":"\u2638\ufe0f 5. Kubernetes Deployment","text":""},{"location":"mlops/automation/#production-ready-manifests","title":"Production-Ready Manifests","text":"<ul> <li><code>k8s/namespace.yaml</code> - Isolated namespace</li> <li><code>k8s/deployment.yaml</code> - Application deployment</li> <li><code>k8s/service.yaml</code> - LoadBalancer service</li> <li><code>k8s/hpa.yaml</code> - Horizontal auto-scaling (2-10 pods)</li> <li><code>k8s/ingress.yaml</code> - HTTPS ingress</li> <li><code>k8s/configmap.yaml</code> - Configuration</li> <li><code>k8s/secrets.yaml</code> - Sensitive data</li> <li><code>k8s/pvc.yaml</code> - Persistent storage</li> <li><code>k8s/monitoring/prometheus.yml</code> - Monitoring config</li> </ul>"},{"location":"mlops/automation/#deploy-to-kubernetes","title":"Deploy to Kubernetes","text":"<pre><code># Deploy all manifests\nmake k8s-deploy\n\n# Or use script\n./scripts/deploy_k8s.sh\n</code></pre>"},{"location":"mlops/automation/#manage-deployment","title":"Manage Deployment","text":"<pre><code># Check status\nmake k8s-status\n\n# View logs\nmake k8s-logs\n\n# Port forward for local access\nmake k8s-port-forward\n\n# Delete deployment\nmake k8s-delete\n</code></pre>"},{"location":"mlops/automation/#features_3","title":"Features","text":"<ul> <li>Auto-scaling: 2-10 pods based on CPU/memory</li> <li>Health checks: Liveness and readiness probes</li> <li>Resource limits: CPU and memory constraints</li> <li>Persistent storage: 10Gi volume</li> <li>HTTPS: Ingress with TLS support</li> <li>Monitoring: Prometheus metrics</li> </ul>"},{"location":"mlops/automation/#6-weights-biases-tracking","title":"\ud83d\udcca 6. Weights &amp; Biases Tracking","text":""},{"location":"mlops/automation/#features_4","title":"Features","text":"<ul> <li>Experiment tracking</li> <li>Model versioning</li> <li>Weight and bias histogram logging</li> <li>Gradient tracking</li> <li>Model artifact storage</li> <li>Hyperparameter tuning with sweeps</li> </ul>"},{"location":"mlops/automation/#setup","title":"Setup","text":"<pre><code># Install W&amp;B\npip install wandb\n\n# Login\nwandb login\n\n# Or use setup script\n./scripts/setup_wandb.sh\n</code></pre>"},{"location":"mlops/automation/#training-with-wb","title":"Training with W&amp;B","text":"<pre><code># Basic training\npython src/training/train.py tracking=wandb\n\n# With both MLflow and W&amp;B\npython src/training/train.py tracking=wandb tracking.backend=both\n\n# Offline mode (no internet required)\nWANDB_MODE=offline python src/training/train.py tracking=wandb\n</code></pre>"},{"location":"mlops/automation/#configuration_1","title":"Configuration","text":"<p>Edit <code>configs/tracking/wandb.yaml</code>:</p> <pre><code>project: image-classifier\nentity: your-username\nlog_model: true\nlog_code: true\nlog_weights_frequency: 10  # Log weights every 10 epochs\nlog_gradients: true\n</code></pre>"},{"location":"mlops/automation/#hyperparameter-sweeps","title":"Hyperparameter Sweeps","text":"<pre><code># Run hyperparameter sweep\npython scripts/wandb_sweep.py\n\n# Run multiple agents\nwandb agent SWEEP_ID --count 5\n</code></pre>"},{"location":"mlops/automation/#what-gets-logged","title":"What Gets Logged","text":"<ul> <li>Training/validation metrics</li> <li>Model architecture</li> <li>Hyperparameters</li> <li>Weight histograms (every 10 epochs)</li> <li>Bias histograms</li> <li>Gradient distributions</li> <li>Model checkpoints</li> <li>Code version</li> <li>System metrics</li> </ul>"},{"location":"mlops/automation/#cicd-pipeline","title":"\ud83d\udd04 CI/CD Pipeline","text":""},{"location":"mlops/automation/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p>File: <code>.github/workflows/code-quality.yml</code></p>"},{"location":"mlops/automation/#jobs","title":"Jobs","text":"<ol> <li>Code Quality - Black, isort, flake8</li> <li>Security - Bandit, Safety</li> <li>Type Checking - mypy</li> <li>Testing - pytest with coverage</li> </ol>"},{"location":"mlops/automation/#triggers","title":"Triggers","text":"<ul> <li>Push to main branch</li> <li>Pull requests</li> <li>Manual dispatch</li> </ul>"},{"location":"mlops/automation/#configuration_2","title":"Configuration","text":"<pre><code>on:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n  workflow_dispatch:\n</code></pre>"},{"location":"mlops/automation/#additional-documentation","title":"\ud83d\udcda Additional Documentation","text":""},{"location":"mlops/automation/#files-created","title":"Files Created","text":"<ul> <li><code>START_HERE.md</code> - Quick start guide</li> <li><code>AUTOMATION_GUIDE.md</code> - Detailed automation guide</li> <li><code>QUICK_REFERENCE.md</code> - Command reference</li> <li><code>CHECKLIST.md</code> - Feature checklist</li> <li><code>IMPLEMENTATION_SUMMARY.md</code> - Implementation details</li> <li><code>MLOPS_AUTOMATION_GUIDE.md</code> - Comprehensive merged guide</li> </ul>"},{"location":"mlops/automation/#reference-order","title":"Reference Order","text":"<ol> <li>START_HERE.md - Begin here for quick start</li> <li>QUICK_REFERENCE.md - Commands and usage</li> <li>AUTOMATION_GUIDE.md - Detailed instructions</li> <li>IMPLEMENTATION_SUMMARY.md - What was implemented</li> </ol>"},{"location":"mlops/automation/#troubleshooting","title":"\ud83d\udee0\ufe0f Troubleshooting","text":""},{"location":"mlops/automation/#pre-commit-hooks-failing","title":"Pre-commit Hooks Failing","text":"<pre><code># Update hooks\npre-commit autoupdate\n\n# Clear cache\npre-commit clean\n\n# Reinstall\npre-commit uninstall\npre-commit install\n</code></pre>"},{"location":"mlops/automation/#wb-login-issues","title":"W&amp;B Login Issues","text":"<pre><code># Login with API key\nwandb login YOUR_API_KEY\n\n# Or set environment variable\nexport WANDB_API_KEY=YOUR_API_KEY\n</code></pre>"},{"location":"mlops/automation/#kubernetes-issues","title":"Kubernetes Issues","text":"<pre><code># Check pods\nkubectl get pods -n mlops-image-classifier\n\n# Describe pod\nkubectl describe pod POD_NAME -n mlops-image-classifier\n\n# View logs\nkubectl logs POD_NAME -n mlops-image-classifier\n</code></pre>"},{"location":"mlops/automation/#docker-build-issues","title":"Docker Build Issues","text":"<pre><code># Clear cache\ndocker builder prune\n\n# Build without cache\ndocker build --no-cache -f Dockerfile.optimized -t image-classifier:latest .\n</code></pre>"},{"location":"mlops/automation/#best-practices","title":"\ud83d\udcc8 Best Practices","text":""},{"location":"mlops/automation/#code-quality","title":"Code Quality","text":"<ul> <li>Run <code>make format</code> before committing</li> <li>Fix all linting errors shown by <code>make lint</code></li> <li>Keep test coverage above 80%</li> </ul>"},{"location":"mlops/automation/#security_1","title":"Security","text":"<ul> <li>Run security scans regularly</li> <li>Update dependencies frequently</li> <li>Never commit secrets (use .env files)</li> </ul>"},{"location":"mlops/automation/#training_1","title":"Training","text":"<ul> <li>Always use experiment tracking</li> <li>Log hyperparameters and metrics</li> <li>Save model artifacts</li> </ul>"},{"location":"mlops/automation/#deployment","title":"Deployment","text":"<ul> <li>Test locally with Docker first</li> <li>Use environment-specific configs</li> <li>Monitor resource usage</li> </ul>"},{"location":"mlops/automation/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<p>After completing the quick start:</p> <ol> <li>Explore the codebase - Understand project structure</li> <li>Run experiments - Try different configurations</li> <li>Monitor metrics - Use W&amp;B and MLflow</li> <li>Deploy to production - Use Kubernetes manifests</li> <li>Set up CI/CD - Configure GitHub Actions</li> <li>Contribute - Follow development guidelines</li> </ol>"},{"location":"mlops/automation/#support","title":"\ud83d\udcde Support","text":"<p>For issues and questions: - Check documentation files - Review error messages carefully - Use <code>make help</code> for available commands - Check logs in <code>reports/</code> directory</p>"},{"location":"mlops/automation/#useful-links","title":"\ud83d\udd17 Useful Links","text":"<ul> <li>Weights &amp; Biases Documentation</li> <li>MLflow Documentation</li> <li>Kubernetes Documentation</li> <li>Docker Best Practices</li> <li>Pre-commit Documentation</li> </ul>"},{"location":"mlops/cicd/","title":"CI/CD Pipeline","text":"<p>This project uses GitHub Actions for Continuous Integration and Continuous Deployment (CI/CD).</p>"},{"location":"mlops/cicd/#overview","title":"Overview","text":"<p>The CI/CD pipeline consists of two main workflows:</p> <ol> <li>Tests &amp; Linting (<code>tests.yaml</code>) - Runs on every push and PR</li> <li>Continuous ML (<code>cml.yaml</code>) - Trains models and generates reports</li> </ol>"},{"location":"mlops/cicd/#workflow-1-tests-and-linting","title":"Workflow 1: Tests and Linting","text":""},{"location":"mlops/cicd/#triggered-on","title":"Triggered On","text":"<ul> <li>Push to <code>main</code> or <code>develop</code> branches</li> <li>Pull requests to <code>main</code> or <code>develop</code></li> </ul>"},{"location":"mlops/cicd/#what-it-does","title":"What It Does","text":"<pre><code>graph LR\n    A[Push/PR] --&gt; B[Setup Python]\n    B --&gt; C[Install Dependencies]\n    C --&gt; D[Lint Code]\n    D --&gt; E[Run Tests]\n    E --&gt; F[Coverage Report]</code></pre>"},{"location":"mlops/cicd/#jobs","title":"Jobs","text":""},{"location":"mlops/cicd/#1-code-quality-checks","title":"1. Code Quality Checks","text":"<p>Flake8 - Syntax and style checking: <pre><code>flake8 src tests --max-line-length=135\n</code></pre></p> <p>Black - Code formatting: <pre><code>black --check src tests\n</code></pre></p> <p>isort - Import sorting: <pre><code>isort --check-only src tests\n</code></pre></p> <p>mypy - Type checking: <pre><code>mypy src --ignore-missing-imports\n</code></pre></p>"},{"location":"mlops/cicd/#2-unit-tests","title":"2. Unit Tests","text":"<p>Runs all tests across multiple Python versions:</p> <ul> <li>Python 3.8</li> <li>Python 3.9</li> <li>Python 3.10</li> </ul> <pre><code>pytest tests/ -v --tb=short -m \"not slow\"\n</code></pre>"},{"location":"mlops/cicd/#3-coverage-report","title":"3. Coverage Report","text":"<p>Generates code coverage and uploads to Codecov:</p> <pre><code>pytest tests/ --cov=src --cov-report=xml\n</code></pre>"},{"location":"mlops/cicd/#configuration-file","title":"Configuration File","text":"<p><code>.github/workflows/tests.yaml</code>:</p> <pre><code>name: CI - Tests and Linting\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main, develop ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\"]\n    # ... (see full file)\n</code></pre>"},{"location":"mlops/cicd/#workflow-2-continuous-ml-cml","title":"Workflow 2: Continuous ML (CML)","text":""},{"location":"mlops/cicd/#triggered-on_1","title":"Triggered On","text":"<ul> <li>Push to <code>main</code> or <code>develop</code></li> <li>Pull requests to <code>main</code></li> </ul>"},{"location":"mlops/cicd/#what-it-does_1","title":"What It Does","text":"<pre><code>graph TD\n    A[Trigger] --&gt; B[Setup Environment]\n    B --&gt; C[Train Model]\n    C --&gt; D[Generate Metrics]\n    D --&gt; E[Create Confusion Matrix]\n    E --&gt; F[Generate Report]\n    F --&gt; G[Post PR Comment]</code></pre>"},{"location":"mlops/cicd/#steps","title":"Steps","text":"<ol> <li>Setup Environment</li> <li>Install Python and dependencies</li> <li> <p>Pull data with DVC (if configured)</p> </li> <li> <p>Train Model <pre><code>python src/training/train.py \\\n  hyperparameters=fast \\\n  hyperparameters.num_epochs=3\n</code></pre></p> </li> <li> <p>Generate Metrics</p> </li> <li>Classification report</li> <li>Confusion matrix</li> <li> <p>Training history plots</p> </li> <li> <p>Create CML Report</p> </li> <li>Compiles all metrics into markdown</li> <li>Posts as PR comment</li> <li>Uploads artifacts</li> </ol>"},{"location":"mlops/cicd/#example-report","title":"Example Report","text":"<p>When you create a PR, CML automatically posts:</p> <p><pre><code># Model Training Report\n\n## Training Configuration\n- Epochs: 3 (fast config for CI)\n- Batch Size: 128\n- Dataset: CIFAR-10\n\n## Metrics\n</code></pre> Classification Report ================================================================================</p> <pre><code>          precision    recall  f1-score   support\n\nairplane     0.7234    0.6821    0.7021      1000\n</code></pre> <p>automobile     0.8123    0.7912    0.8016      1000         bird     0.6234    0.5821    0.6019      1000 ...</p> <pre><code>accuracy                         0.7234     10000\n</code></pre> <p>``` </p>"},{"location":"mlops/cicd/#confusion-matrix","title":"Confusion Matrix","text":""},{"location":"mlops/cicd/#training-history","title":"Training History","text":"<p> ```</p>"},{"location":"mlops/cicd/#configuration-file_1","title":"Configuration File","text":"<p><code>.github/workflows/cml.yaml</code>:</p> <pre><code>name: CML - Continuous Machine Learning\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  train-and-report:\n    runs-on: ubuntu-latest\n    steps:\n    # ... (see full file)\n</code></pre>"},{"location":"mlops/cicd/#setting-up-cicd","title":"Setting Up CI/CD","text":""},{"location":"mlops/cicd/#1-enable-github-actions","title":"1. Enable GitHub Actions","text":"<p>GitHub Actions is enabled by default for public repositories.</p> <p>For private repos: 1. Go to Settings \u2192 Actions \u2192 General 2. Enable \"Allow all actions and reusable workflows\"</p>"},{"location":"mlops/cicd/#2-add-secrets-if-needed","title":"2. Add Secrets (if needed)","text":"<p>For DVC remote storage or other services:</p> <ol> <li>Go to Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Add secrets:</li> <li><code>DVC_REMOTE_URL</code> (if using DVC)</li> <li><code>AWS_ACCESS_KEY_ID</code> (if using S3)</li> <li>etc.</li> </ol>"},{"location":"mlops/cicd/#3-configure-dvc-remote-optional","title":"3. Configure DVC Remote (Optional)","text":"<p>If using DVC:</p> <pre><code># Local setup\ndvc remote add -d storage s3://mybucket/path\n\n# In GitHub Actions, add secret\n# Then modify workflow:\n- name: Configure DVC\n  env:\n    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n  run: dvc pull\n</code></pre>"},{"location":"mlops/cicd/#customization","title":"Customization","text":""},{"location":"mlops/cicd/#modify-test-configuration","title":"Modify Test Configuration","text":"<p>Edit <code>.github/workflows/tests.yaml</code>:</p> <pre><code># Change Python versions\nstrategy:\n  matrix:\n    python-version: [\"3.9\", \"3.10\", \"3.11\"]\n\n# Add more lint checks\n- name: Run pylint\n  run: pylint src\n</code></pre>"},{"location":"mlops/cicd/#modify-cml-configuration","title":"Modify CML Configuration","text":"<p>Edit <code>.github/workflows/cml.yaml</code>:</p> <pre><code># Train for more epochs\n- name: Train model\n  run: |\n    python src/training/train.py \\\n      hyperparameters.num_epochs=10\n</code></pre>"},{"location":"mlops/cicd/#add-more-jobs","title":"Add More Jobs","text":"<pre><code>deploy:\n  needs: test\n  runs-on: ubuntu-latest\n  if: github.ref == 'refs/heads/main'\n  steps:\n    - name: Deploy model\n      run: |\n        # Your deployment script\n</code></pre>"},{"location":"mlops/cicd/#badges","title":"Badges","text":"<p>Add status badges to your README:</p> <pre><code>![Tests](https://github.com/username/repo/workflows/CI%20-%20Tests%20and%20Linting/badge.svg)\n![CML](https://github.com/username/repo/workflows/CML%20-%20Continuous%20Machine%20Learning/badge.svg)\n[![codecov](https://codecov.io/gh/username/repo/branch/main/graph/badge.svg)](https://codecov.io/gh/username/repo)\n</code></pre>"},{"location":"mlops/cicd/#monitoring","title":"Monitoring","text":""},{"location":"mlops/cicd/#view-workflow-runs","title":"View Workflow Runs","text":"<ol> <li>Go to the \"Actions\" tab in your repository</li> <li>Click on a workflow run to see details</li> <li>View logs for each job</li> </ol>"},{"location":"mlops/cicd/#check-coverage","title":"Check Coverage","text":"<p>If you set up Codecov: - Visit <code>https://codecov.io/gh/username/repo</code> - View coverage trends and reports</p>"},{"location":"mlops/cicd/#best-practices","title":"Best Practices","text":"<ol> <li>Keep CI Fast</li> <li>Use fast config for CI</li> <li>Cache dependencies</li> <li> <p>Run slow tests separately</p> </li> <li> <p>Fail Fast</p> </li> <li>Run linting before tests</li> <li> <p>Use <code>continue-on-error: false</code> for critical jobs</p> </li> <li> <p>Artifact Management</p> </li> <li>Upload trained models</li> <li>Save reports for later review</li> <li> <p>Use appropriate retention days</p> </li> <li> <p>Security</p> </li> <li>Never commit secrets</li> <li>Use GitHub Secrets</li> <li>Limit workflow permissions</li> </ol>"},{"location":"mlops/cicd/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mlops/cicd/#tests-failing-locally-but-passing-in-ci","title":"Tests Failing Locally But Passing in CI","text":"<ul> <li>Check Python version</li> <li>Clear cache: <code>pip cache purge</code></li> <li>Check for environment-specific code</li> </ul>"},{"location":"mlops/cicd/#cml-report-not-appearing","title":"CML Report Not Appearing","text":"<ul> <li>Check GitHub token permissions</li> <li>Verify CML installation</li> <li>Check workflow logs</li> </ul>"},{"location":"mlops/cicd/#out-of-memory-in-ci","title":"Out of Memory in CI","text":"<ul> <li>Reduce batch size in fast config</li> <li>Use smaller model</li> <li>Request more resources</li> </ul>"},{"location":"mlops/cicd/#advanced-topics","title":"Advanced Topics","text":""},{"location":"mlops/cicd/#matrix-strategy","title":"Matrix Strategy","text":"<p>Test multiple configurations:</p> <pre><code>strategy:\n  matrix:\n    python-version: [3.8, 3.9, 3.10]\n    os: [ubuntu-latest, windows-latest, macos-latest]\n</code></pre>"},{"location":"mlops/cicd/#caching","title":"Caching","text":"<p>Speed up workflows with caching:</p> <pre><code>- uses: actions/cache@v3\n  with:\n    path: ~/.cache/pip\n    key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}\n</code></pre>"},{"location":"mlops/cicd/#scheduled-runs","title":"Scheduled Runs","text":"<p>Run tests nightly:</p> <pre><code>on:\n  schedule:\n    - cron: '0 0 * * *'  # Every day at midnight\n</code></pre>"},{"location":"mlops/cicd/#learn-more","title":"Learn More","text":"<ul> <li>GitHub Actions Documentation</li> <li>CML Documentation</li> <li>Codecov Documentation</li> </ul>"},{"location":"mlops/docker/","title":"Docker Containerization","text":"<p>Package and deploy the application using Docker.</p>"},{"location":"mlops/docker/#overview","title":"Overview","text":"<p>Docker containerizes the application with all dependencies for consistent deployment across environments.</p>"},{"location":"mlops/docker/#dockerfile","title":"Dockerfile","text":"<p>Our multi-stage Dockerfile optimizes image size:</p> <pre><code># Build stage\nFROM python:3.11-slim as builder\n\nWORKDIR /app\n\n# Install build dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\n\n# Create wheels\nRUN pip install --no-cache-dir wheel &amp;&amp; \\\n    pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt\n\n# Runtime stage\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Install runtime dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    libsm6 libxext6 libxrender-dev \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy wheels from builder\nCOPY --from=builder /app/wheels /wheels\nCOPY --from=builder /app/requirements.txt .\n\n# Install Python packages\nRUN pip install --no-cache /wheels/*\n\n# Copy application\nCOPY . .\n\n# Install package\nRUN pip install -e .\n\n# Create necessary directories\nRUN mkdir -p data models reports outputs\n\n# Set entrypoint\nENTRYPOINT [\"python\", \"src/training/train.py\"]\n</code></pre>"},{"location":"mlops/docker/#building-images","title":"Building Images","text":""},{"location":"mlops/docker/#build-image","title":"Build Image","text":"<pre><code># Build with default tag\ndocker build -t image-classifier .\n\n# Build with custom tag\ndocker build -t image-classifier:v1.0 .\n\n# Build with build args\ndocker build --build-arg PYTHON_VERSION=3.10 -t image-classifier .\n</code></pre>"},{"location":"mlops/docker/#view-images","title":"View Images","text":"<pre><code># List images\ndocker images | grep image-classifier\n\n# View image info\ndocker inspect image-classifier\n</code></pre>"},{"location":"mlops/docker/#running-containers","title":"Running Containers","text":""},{"location":"mlops/docker/#basic-training","title":"Basic Training","text":"<pre><code># Run default training (CIFAR-10)\ndocker run --rm image-classifier\n\n# Run with GPU support\ndocker run --rm --gpus all image-classifier\n\n# Run on MNIST\ndocker run --rm image-classifier data=mnist\n</code></pre>"},{"location":"mlops/docker/#with-volume-mounts","title":"With Volume Mounts","text":"<pre><code># Mount data directory\ndocker run --rm \\\n    -v /home/user/data:/app/data \\\n    image-classifier\n\n# Mount models directory\ndocker run --rm \\\n    -v /home/user/models:/app/models \\\n    image-classifier\n\n# Mount all project directories\ndocker run --rm \\\n    -v /home/user/project:/app \\\n    image-classifier\n</code></pre>"},{"location":"mlops/docker/#interactive-mode","title":"Interactive Mode","text":"<pre><code># Open bash shell\ndocker run --rm -it \\\n    --entrypoint /bin/bash \\\n    image-classifier\n\n# Run custom command\ndocker run --rm -it \\\n    --entrypoint python \\\n    image-classifier \\\n    -c \"from src.models.model import get_model; print(get_model('simple_cnn'))\"\n</code></pre>"},{"location":"mlops/docker/#environmental-variables","title":"Environmental Variables","text":"<pre><code># Pass configuration via environment\ndocker run --rm \\\n    -e CUDA_VISIBLE_DEVICES=0 \\\n    -e PYTHONUNBUFFERED=1 \\\n    image-classifier\n</code></pre>"},{"location":"mlops/docker/#environment-variables","title":"Environment Variables","text":""},{"location":"mlops/docker/#useful-variables","title":"Useful Variables","text":"<pre><code># Python settings\nPYTHONUNBUFFERED=1        # Unbuffered output\nPYTHONDONTWRITEBYTECODE=1 # Don't create .pyc files\n\n# GPU settings\nCUDA_VISIBLE_DEVICES=0    # Use specific GPU\n\n# Application settings\nLOG_LEVEL=INFO            # Logging level\n</code></pre>"},{"location":"mlops/docker/#multi-gpu-training","title":"Multi-GPU Training","text":""},{"location":"mlops/docker/#single-gpu","title":"Single GPU","text":"<pre><code>docker run --rm --gpus device=0 image-classifier\n</code></pre>"},{"location":"mlops/docker/#multiple-gpus","title":"Multiple GPUs","text":"<pre><code># GPU 0 and 1\ndocker run --rm --gpus '\"device=0,1\"' image-classifier\n</code></pre>"},{"location":"mlops/docker/#all-gpus","title":"All GPUs","text":"<pre><code>docker run --rm --gpus all image-classifier\n</code></pre>"},{"location":"mlops/docker/#docker-compose","title":"Docker Compose","text":""},{"location":"mlops/docker/#multi-service-setup","title":"Multi-service Setup","text":"<p>Create <code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\n\nservices:\n  training:\n    build: .\n    image: image-classifier:latest\n    container_name: image-classifier-train\n    volumes:\n      - ./data:/app/data\n      - ./models:/app/models\n      - ./reports:/app/reports\n    environment:\n      - CUDA_VISIBLE_DEVICES=0\n    command: data=cifar10 hyperparameters.num_epochs=50\n\n  inference:\n    build: .\n    image: image-classifier:latest\n    container_name: image-classifier-inference\n    volumes:\n      - ./models:/app/models\n    ports:\n      - \"8000:8000\"\n    command: python src/api/serve.py\n    depends_on:\n      - training\n</code></pre>"},{"location":"mlops/docker/#run-with-docker-compose","title":"Run with Docker Compose","text":"<pre><code># Start services\ndocker-compose up\n\n# Run specific service\ndocker-compose up training\n\n# Background mode\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f training\n\n# Stop services\ndocker-compose down\n</code></pre>"},{"location":"mlops/docker/#publishing-images","title":"Publishing Images","text":""},{"location":"mlops/docker/#docker-hub","title":"Docker Hub","text":"<pre><code># Tag image\ndocker tag image-classifier:latest yourusername/image-classifier:latest\n\n# Login to Docker Hub\ndocker login\n\n# Push image\ndocker push yourusername/image-classifier:latest\n\n# Pull image\ndocker pull yourusername/image-classifier:latest\n</code></pre>"},{"location":"mlops/docker/#performance-optimization","title":"Performance Optimization","text":""},{"location":"mlops/docker/#reduce-image-size","title":"Reduce Image Size","text":"<p>Current optimizations: - Multi-stage build (~500MB final image) - Python slim base image - Remove build dependencies - Minimal runtime dependencies</p>"},{"location":"mlops/docker/#speed-up-builds","title":"Speed Up Builds","text":"<pre><code># Use BuildKit\nDOCKER_BUILDKIT=1 docker build -t image-classifier .\n\n# Cache layers efficiently\n# Put dependencies early in Dockerfile\n# Order: base image \u2192 system deps \u2192 python deps \u2192 app code\n</code></pre>"},{"location":"mlops/docker/#debugging","title":"Debugging","text":""},{"location":"mlops/docker/#view-image-layers","title":"View Image Layers","text":"<pre><code># List image history\ndocker history image-classifier\n\n# Inspect layer details\ndocker inspect image-classifier\n</code></pre>"},{"location":"mlops/docker/#run-with-debugging","title":"Run with Debugging","text":"<pre><code># Keep container running\ndocker run --rm -it \\\n    --entrypoint /bin/bash \\\n    image-classifier\n\n# Inside container\npython -m pdb src/training/train.py\n</code></pre>"},{"location":"mlops/docker/#view-container-logs","title":"View Container Logs","text":"<pre><code># Run and see logs\ndocker run --rm image-classifier\n\n# View logs of stopped container\ndocker logs container-id\n</code></pre>"},{"location":"mlops/docker/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"mlops/docker/#github-actions","title":"GitHub Actions","text":"<pre><code>name: Build Docker Image\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Build image\n        run: docker build -t image-classifier .\n\n      - name: Run tests in container\n        run: docker run --rm image-classifier pytest\n</code></pre>"},{"location":"mlops/docker/#cleanup","title":"Cleanup","text":""},{"location":"mlops/docker/#remove-images-and-containers","title":"Remove Images and Containers","text":"<pre><code># Remove container\ndocker rm container-id\n\n# Remove image\ndocker rmi image-classifier\n\n# Remove unused images\ndocker image prune\n\n# Remove everything\ndocker system prune -a\n</code></pre>"},{"location":"mlops/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mlops/docker/#out-of-memory","title":"Out of Memory","text":"<pre><code># Limit memory usage\ndocker run --rm -m 4g image-classifier\n\n# Check memory usage\ndocker stats\n</code></pre>"},{"location":"mlops/docker/#gpu-not-available","title":"GPU Not Available","text":"<pre><code># Check GPU access\ndocker run --rm --gpus all nvidia-smi\n\n# Verify nvidia-docker installation\nnvidia-docker --version\n</code></pre>"},{"location":"mlops/docker/#large-build-sizes","title":"Large Build Sizes","text":"<pre><code># Use .dockerignore to exclude files\n# Similar to .gitignore\n\n# Check what's included\ndocker build --progress=plain .\n</code></pre>"},{"location":"mlops/docker/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use specific base image versions <pre><code>FROM python:3.11-slim  # \u2713 Good (specific)\nFROM python:3.11      # \u2717 Could be unstable\n</code></pre></p> </li> <li> <p>Minimize layers <pre><code># \u2713 Good - fewer layers\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y curl &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# \u2717 Bad - more layers\nRUN apt-get update\nRUN apt-get install -y curl\nRUN rm -rf /var/lib/apt/lists/*\n</code></pre></p> </li> <li> <p>Use .dockerignore <pre><code>__pycache__\n*.pyc\n.git\n.gitignore\nREADME.md\n.venv\n</code></pre></p> </li> <li> <p>Set proper entrypoint <pre><code># \u2713 Good - exec form (PID 1)\nENTRYPOINT [\"python\", \"src/training/train.py\"]\n\n# \u2717 Bad - shell form\nENTRYPOINT python src/training/train.py\n</code></pre></p> </li> </ol>"},{"location":"mlops/docker/#references","title":"References","text":"<ul> <li>Docker Documentation</li> <li>Docker Best Practices</li> <li>Python Docker Best Practices</li> <li>nvidia-docker</li> </ul>"},{"location":"mlops/dvc/","title":"Data Versioning with DVC","text":"<p>Learn how to use Data Version Control (DVC) to track datasets, models, and experiments.</p>"},{"location":"mlops/dvc/#why-dvc","title":"Why DVC?","text":"<ul> <li>Version large files - Git can't handle large datasets efficiently</li> <li>Share data - Use remote storage (S3, Google Drive, etc.)</li> <li>Reproduce experiments - Track exact data versions</li> <li>Save storage - Don't duplicate data across branches</li> </ul>"},{"location":"mlops/dvc/#installation","title":"Installation","text":"<p>DVC is already included in <code>requirements.txt</code>. If not installed:</p> <pre><code>pip install dvc dvc-gdrive  # Or dvc-s3, dvc-azure, etc.\n</code></pre>"},{"location":"mlops/dvc/#quick-start","title":"Quick Start","text":""},{"location":"mlops/dvc/#1-initialize-dvc","title":"1. Initialize DVC","text":"<pre><code>dvc init\ngit add .dvc .dvcignore\ngit commit -m \"Initialize DVC\"\n</code></pre>"},{"location":"mlops/dvc/#2-track-data","title":"2. Track Data","text":"<pre><code># Track the data directory\ndvc add data/raw\n\n# This creates data/raw.dvc\ngit add data/raw.dvc data/.gitignore\ngit commit -m \"Track data with DVC\"\n</code></pre>"},{"location":"mlops/dvc/#3-setup-remote-storage","title":"3. Setup Remote Storage","text":"<p>Choose a remote storage backend:</p> Google DriveAWS S3Local Storage <pre><code>dvc remote add -d storage gdrive://FOLDER_ID\ngit add .dvc/config\ngit commit -m \"Configure remote storage\"\n</code></pre> <pre><code>dvc remote add -d storage s3://mybucket/path\ndvc remote modify storage region us-west-2\n</code></pre> <pre><code>dvc remote add -d storage /tmp/dvc-storage\n# Or network drive\ndvc remote add -d storage /mnt/shared/dvc-storage\n</code></pre>"},{"location":"mlops/dvc/#4-push-data","title":"4. Push Data","text":"<pre><code>dvc push\n</code></pre>"},{"location":"mlops/dvc/#common-workflows","title":"Common Workflows","text":""},{"location":"mlops/dvc/#clone-repository-and-get-data","title":"Clone Repository and Get Data","text":"<pre><code># Clone repo\ngit clone https://github.com/username/repo.git\ncd repo\n\n# Install dependencies\npip install -r requirements.txt\n\n# Pull data\ndvc pull\n</code></pre>"},{"location":"mlops/dvc/#update-dataset","title":"Update Dataset","text":"<pre><code># 1. Modify your data\ncp new_data/* data/raw/\n\n# 2. Update DVC tracking\ndvc add data/raw\n\n# 3. Commit changes\ngit add data/raw.dvc\ngit commit -m \"Update dataset with new images\"\n\n# 4. Push to remote\ndvc push\ngit push\n</code></pre>"},{"location":"mlops/dvc/#switch-between-versions","title":"Switch Between Versions","text":"<pre><code># Go to a specific commit\ngit checkout &lt;commit-hash&gt;\n\n# Pull the corresponding data\ndvc pull\n</code></pre>"},{"location":"mlops/dvc/#check-status","title":"Check Status","text":"<pre><code># Check what's changed\ndvc status\n\n# Check DVC cache\ndvc cache dir\n</code></pre>"},{"location":"mlops/dvc/#track-models","title":"Track Models","text":""},{"location":"mlops/dvc/#track-trained-models","title":"Track Trained Models","text":"<pre><code># Add models directory\ndvc add models/\n\ngit add models.dvc\ngit commit -m \"Track trained models\"\ndvc push\n</code></pre>"},{"location":"mlops/dvc/#version-models-with-experiments","title":"Version Models with Experiments","text":"<p>Create a pipeline:</p> <pre><code># dvc.yaml\nstages:\n  train:\n    cmd: python src/training/train.py\n    deps:\n      - src/training/train.py\n      - data/raw\n    params:\n      - config.yaml:hyperparameters.learning_rate\n      - config.yaml:hyperparameters.num_epochs\n    outs:\n      - models/simple_cnn_best.pth\n    metrics:\n      - reports/classification_report.txt:\n          cache: false\n</code></pre> <p>Run pipeline:</p> <pre><code>dvc repro\n</code></pre>"},{"location":"mlops/dvc/#remote-storage-options","title":"Remote Storage Options","text":""},{"location":"mlops/dvc/#google-drive","title":"Google Drive","text":"<ol> <li>Create a folder in Google Drive</li> <li>Get the folder ID from the URL</li> <li>Configure DVC:</li> </ol> <pre><code>dvc remote add -d storage gdrive://FOLDER_ID\n</code></pre> <p>First push will open browser for authentication.</p>"},{"location":"mlops/dvc/#aws-s3","title":"AWS S3","text":"<pre><code># Add remote\ndvc remote add -d storage s3://mybucket/path\n\n# Configure credentials\ndvc remote modify storage access_key_id YOUR_KEY\ndvc remote modify storage secret_access_key YOUR_SECRET\n\n# Or use AWS CLI credentials\nexport AWS_ACCESS_KEY_ID=xxx\nexport AWS_SECRET_ACCESS_KEY=xxx\n</code></pre>"},{"location":"mlops/dvc/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code>dvc remote add -d storage azure://container/path\ndvc remote modify storage account_name myaccount\n</code></pre>"},{"location":"mlops/dvc/#sshlocal","title":"SSH/Local","text":"<pre><code># SSH\ndvc remote add -d storage ssh://user@server:/path/to/storage\n\n# Local or network drive\ndvc remote add -d storage /mnt/shared/dvc-storage\n</code></pre>"},{"location":"mlops/dvc/#dvc-with-github-actions","title":"DVC with GitHub Actions","text":""},{"location":"mlops/dvc/#setup","title":"Setup","text":"<p>Add secrets to GitHub:</p> <ol> <li>Go to Settings \u2192 Secrets</li> <li>Add necessary credentials (AWS keys, etc.)</li> </ol>"},{"location":"mlops/dvc/#workflow-example","title":"Workflow Example","text":"<pre><code>name: DVC Pipeline\n\non: [push]\n\njobs:\n  train:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n\n      - name: Pull data\n        env:\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        run: |\n          dvc pull\n\n      - name: Train model\n        run: |\n          dvc repro\n\n      - name: Push results\n        run: |\n          dvc push\n</code></pre>"},{"location":"mlops/dvc/#best-practices","title":"Best Practices","text":""},{"location":"mlops/dvc/#1-organize-data","title":"1. Organize Data","text":"<pre><code>data/\n\u251c\u2500\u2500 raw/           # Original immutable data - track with DVC\n\u251c\u2500\u2500 processed/     # Processed data - track with DVC\n\u2514\u2500\u2500 interim/       # Temporary data - don't track\n</code></pre>"},{"location":"mlops/dvc/#2-use-dvcignore","title":"2. Use .dvcignore","text":"<p>Exclude files you don't want to track:</p> <pre><code># .dvcignore\n*.log\n*.tmp\n__pycache__/\n.ipynb_checkpoints/\n</code></pre>"},{"location":"mlops/dvc/#3-track-different-versions","title":"3. Track Different Versions","text":"<pre><code># Tag important versions\ngit tag -a v1.0 -m \"Dataset version 1.0\"\ngit push --tags\ndvc push\n</code></pre>"},{"location":"mlops/dvc/#4-share-specific-versions","title":"4. Share Specific Versions","text":"<pre><code># Create branch for experiment\ngit checkout -b experiment/new_data\ndvc add data/raw\ngit add data/raw.dvc\ngit commit -m \"Add new dataset\"\ndvc push\ngit push -u origin experiment/new_data\n</code></pre>"},{"location":"mlops/dvc/#advanced-features","title":"Advanced Features","text":""},{"location":"mlops/dvc/#dvc-pipelines","title":"DVC Pipelines","text":"<p>Define reproducible workflows:</p> <pre><code># dvc.yaml\nstages:\n  preprocess:\n    cmd: python src/data/preprocess.py\n    deps:\n      - data/raw\n    outs:\n      - data/processed\n\n  train:\n    cmd: python src/training/train.py\n    deps:\n      - data/processed\n      - src/training/train.py\n    params:\n      - configs/config.yaml:hyperparameters\n    outs:\n      - models/model.pth\n    metrics:\n      - reports/metrics.json:\n          cache: false\n</code></pre> <p>Run entire pipeline:</p> <pre><code>dvc repro\n</code></pre>"},{"location":"mlops/dvc/#experiments","title":"Experiments","text":"<p>Track experiments:</p> <pre><code># Run experiment\ndvc exp run\n\n# Compare experiments\ndvc exp show\n\n# Apply best experiment\ndvc exp apply &lt;exp-name&gt;\n</code></pre>"},{"location":"mlops/dvc/#metrics-tracking","title":"Metrics Tracking","text":"<pre><code># dvc.yaml\nstages:\n  train:\n    metrics:\n      - reports/metrics.json:\n          cache: false\n    plots:\n      - reports/confusion_matrix.png:\n          cache: false\n</code></pre> <p>Compare metrics:</p> <pre><code>dvc metrics show\ndvc metrics diff\n</code></pre>"},{"location":"mlops/dvc/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mlops/dvc/#large-files-not-tracked","title":"Large Files Not Tracked","text":"<p>Check <code>.gitignore</code>:</p> <pre><code>cat data/.gitignore\n# Should contain /raw (added by dvc add)\n</code></pre>"},{"location":"mlops/dvc/#permission-denied","title":"Permission Denied","text":"<p>For remote storage, check credentials:</p> <pre><code>dvc remote list\ndvc remote modify storage --local access_key_id YOUR_KEY\n</code></pre>"},{"location":"mlops/dvc/#cache-issues","title":"Cache Issues","text":"<p>Clear and rebuild cache:</p> <pre><code>dvc cache dir  # Show cache location\nrm -rf .dvc/cache\ndvc pull\n</code></pre>"},{"location":"mlops/dvc/#slow-pushpull","title":"Slow Push/Pull","text":"<p>Use parallel transfers:</p> <pre><code>dvc config cache.type symlink\ndvc push --jobs 4\n</code></pre>"},{"location":"mlops/dvc/#dvc-vs-git-lfs","title":"DVC vs Git LFS","text":"Feature DVC Git LFS Storage backends Many (S3, Azure, GDrive, SSH, etc.) Git LFS server Versioning Full versioning Full versioning Pipeline support Yes No Metrics tracking Yes No Free tier Depends on storage GitHub: 1GB"},{"location":"mlops/dvc/#learn-more","title":"Learn More","text":"<ul> <li>DVC Documentation</li> <li>DVC Tutorial</li> <li>DVC with CI/CD</li> <li>Example Projects</li> </ul>"},{"location":"mlops/dvc/#summary","title":"Summary","text":"<pre><code># Essential commands\ndvc init              # Initialize DVC\ndvc add data/         # Track data\ndvc push              # Upload to remote\ndvc pull              # Download from remote\ndvc status            # Check status\ndvc repro             # Run pipeline\n</code></pre>"},{"location":"mlops/experiment-tracking/","title":"Experiment Tracking","text":"<p>This project supports experiment tracking with both MLflow and Weights &amp; Biases (W&amp;B). This allows you to track, compare, and visualize your machine learning experiments.</p>"},{"location":"mlops/experiment-tracking/#features","title":"Features","text":"<ul> <li>\ud83d\udcca Metric Tracking: Automatically log training/validation metrics</li> <li>\ud83d\udd27 Hyperparameter Logging: Track all configuration parameters</li> <li>\ud83d\udcc8 Visualization: Interactive plots and dashboards</li> <li>\ud83c\udfaf Model Versioning: Save and version your trained models</li> <li>\ud83d\udcc1 Artifact Management: Store confusion matrices, plots, and reports</li> <li>\ud83d\udd04 Comparison: Compare multiple experiments side-by-side</li> <li>\ud83c\udf10 Remote Tracking: Support for both local and remote tracking servers</li> </ul>"},{"location":"mlops/experiment-tracking/#setup","title":"Setup","text":""},{"location":"mlops/experiment-tracking/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install mlflow wandb\n</code></pre> <p>Or install from requirements:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"mlops/experiment-tracking/#mlflow-setup","title":"MLflow Setup","text":"<p>MLflow works out of the box with local tracking. The tracking data is stored in <code>./mlruns</code> by default.</p>"},{"location":"mlops/experiment-tracking/#remote-mlflow-server-optional","title":"Remote MLflow Server (Optional)","text":"<p>To use a remote MLflow server:</p> <ol> <li> <p>Start MLflow server: <pre><code>mlflow server --host 0.0.0.0 --port 5000\n</code></pre></p> </li> <li> <p>Update <code>configs/tracking/mlflow.yaml</code>: <pre><code>tracking_uri: http://localhost:5000\n</code></pre></p> </li> </ol>"},{"location":"mlops/experiment-tracking/#weights-biases-setup","title":"Weights &amp; Biases Setup","text":"<ol> <li> <p>Create a W&amp;B account: Sign up at wandb.ai</p> </li> <li> <p>Login to W&amp;B: <pre><code>wandb login\n</code></pre></p> </li> <li> <p>Update configuration in <code>configs/tracking/wandb.yaml</code>: <pre><code>project: your-project-name\nentity: your-username-or-team\n</code></pre></p> </li> </ol>"},{"location":"mlops/experiment-tracking/#configuration","title":"Configuration","text":""},{"location":"mlops/experiment-tracking/#choose-tracking-backend","title":"Choose Tracking Backend","text":"<p>In <code>configs/config.yaml</code>, set the tracking backend:</p> <pre><code>defaults:\n  - tracking: mlflow  # Options: mlflow, wandb, or null (to disable)\n</code></pre>"},{"location":"mlops/experiment-tracking/#mlflow-configuration","title":"MLflow Configuration","text":"<p>Edit <code>configs/tracking/mlflow.yaml</code>:</p> <pre><code># MLflow Configuration\nenabled: true\ntracking_uri: ./mlruns  # Local tracking\nexperiment_name: ${experiment_name}\nrun_name: ${run_name}\n\n# What to log\nlog_params: true\nlog_metrics: true\nlog_models: true\nlog_artifacts: true\n\n# Model Registry\nregister_model: false\nregistered_model_name: ${model.name}_${data.name}\n\n# Autologging (logs everything automatically)\nautolog: false\n</code></pre>"},{"location":"mlops/experiment-tracking/#wb-configuration","title":"W&amp;B Configuration","text":"<p>Edit <code>configs/tracking/wandb.yaml</code>:</p> <pre><code># Weights &amp; Biases Configuration\nenabled: true\nproject: image-classifier\nentity: null  # Your username/team\n\n# Mode: online, offline, or disabled\nmode: online\n\n# What to log\nlog_params: true\nlog_metrics: true\nlog_gradients: false\nlog_model: true\nlog_artifacts: true\n\n# Model watching (log gradients/parameters)\nwatch_model: false\nwatch_log: \"gradients\"  # \"gradients\", \"parameters\", or \"all\"\nwatch_freq: 100\n</code></pre>"},{"location":"mlops/experiment-tracking/#usage","title":"Usage","text":""},{"location":"mlops/experiment-tracking/#basic-training-with-tracking","title":"Basic Training with Tracking","text":"<pre><code># Train with MLflow\npython src/training/train.py tracking=mlflow\n\n# Train with W&amp;B\npython src/training/train.py tracking=wandb\n\n# Train without tracking\npython src/training/train.py tracking=null\n</code></pre>"},{"location":"mlops/experiment-tracking/#view-mlflow-ui","title":"View MLflow UI","text":"<pre><code>mlflow ui\n</code></pre> <p>Then open http://localhost:5000 in your browser.</p>"},{"location":"mlops/experiment-tracking/#view-wb-dashboard","title":"View W&amp;B Dashboard","text":"<p>After training, W&amp;B will print a URL to your run. You can also visit your project page at: <pre><code>https://wandb.ai/your-username/your-project\n</code></pre></p>"},{"location":"mlops/experiment-tracking/#what-gets-logged","title":"What Gets Logged","text":""},{"location":"mlops/experiment-tracking/#metrics","title":"Metrics","text":"<ul> <li><code>train_loss</code>, <code>train_acc</code> (per epoch)</li> <li><code>val_loss</code>, <code>val_acc</code> (per epoch)</li> <li><code>test_loss</code>, <code>test_acc</code> (final)</li> <li><code>learning_rate</code> (per epoch)</li> <li>Test metrics: <code>precision</code>, <code>recall</code>, <code>f1_score</code>, <code>accuracy</code></li> </ul>"},{"location":"mlops/experiment-tracking/#parameters","title":"Parameters","text":"<ul> <li>Model architecture</li> <li>Dataset name</li> <li>Hyperparameters (learning rate, batch size, optimizer, etc.)</li> <li>Number of parameters (total and trainable)</li> <li>Device (CPU/GPU)</li> </ul>"},{"location":"mlops/experiment-tracking/#artifacts","title":"Artifacts","text":"<ul> <li>\ud83d\udcca Confusion matrix</li> <li>\ud83d\udcc8 Training history plots</li> <li>\ud83d\udcdd Classification report</li> <li>\ud83c\udfaf Best model checkpoint</li> </ul>"},{"location":"mlops/experiment-tracking/#models","title":"Models","text":"<ul> <li>Best model (based on validation accuracy)</li> <li>Model architecture and state dict</li> <li>Optimizer state</li> </ul>"},{"location":"mlops/experiment-tracking/#advanced-usage","title":"Advanced Usage","text":""},{"location":"mlops/experiment-tracking/#custom-experiment-names","title":"Custom Experiment Names","text":"<pre><code>python src/training/train.py \\\n  experiment_name=cifar10_resnet \\\n  run_name=resnet50_lr0.001\n</code></pre>"},{"location":"mlops/experiment-tracking/#enable-wb-model-watching","title":"Enable W&amp;B Model Watching","text":"<p>To log gradients and parameters during training:</p> <pre><code># configs/tracking/wandb.yaml\nwatch_model: true\nwatch_log: \"all\"  # Log both gradients and parameters\nwatch_freq: 100   # Log every 100 batches\n</code></pre> <p>\u26a0\ufe0f Warning: This can significantly slow down training and use more storage.</p>"},{"location":"mlops/experiment-tracking/#mlflow-model-registry","title":"MLflow Model Registry","text":"<p>To register your model in MLflow Model Registry:</p> <pre><code># configs/tracking/mlflow.yaml\nregister_model: true\nregistered_model_name: resnet_cifar10\n</code></pre>"},{"location":"mlops/experiment-tracking/#offline-mode-wb","title":"Offline Mode (W&amp;B)","text":"<p>Train without internet connection:</p> <pre><code># configs/tracking/wandb.yaml\nmode: offline\n</code></pre> <p>Then sync later: <pre><code>wandb sync ./wandb/offline-run-*\n</code></pre></p>"},{"location":"mlops/experiment-tracking/#comparing-experiments","title":"Comparing Experiments","text":""},{"location":"mlops/experiment-tracking/#mlflow","title":"MLflow","text":"<ol> <li>Open MLflow UI: <code>mlflow ui</code></li> <li>Select multiple runs using checkboxes</li> <li>Click \"Compare\" to see side-by-side comparison</li> <li>Use the chart view for metric comparisons</li> </ol>"},{"location":"mlops/experiment-tracking/#wb","title":"W&amp;B","text":"<ol> <li>Go to your project dashboard</li> <li>All runs are automatically tracked and displayed</li> <li>Use the table view to compare metrics</li> <li>Create custom panels and reports</li> </ol>"},{"location":"mlops/experiment-tracking/#examples","title":"Examples","text":""},{"location":"mlops/experiment-tracking/#example-1-compare-different-models","title":"Example 1: Compare Different Models","text":"<pre><code># Train Simple CNN\npython src/training/train.py model=simple_cnn experiment_name=model_comparison\n\n# Train ResNet\npython src/training/train.py model=resnet experiment_name=model_comparison\n</code></pre> <p>All runs will be grouped under the same experiment for easy comparison.</p>"},{"location":"mlops/experiment-tracking/#example-2-hyperparameter-sweep","title":"Example 2: Hyperparameter Sweep","text":"<pre><code># Try different learning rates\npython src/training/train.py hyperparameters.learning_rate=0.001\npython src/training/train.py hyperparameters.learning_rate=0.01\npython src/training/train.py hyperparameters.learning_rate=0.1\n</code></pre>"},{"location":"mlops/experiment-tracking/#example-3-grid-search-with-wb-sweeps","title":"Example 3: Grid Search with W&amp;B Sweeps","text":"<p>Create <code>sweep_config.yaml</code>:</p> <pre><code>program: src/training/train.py\nmethod: grid\nparameters:\n  hyperparameters.learning_rate:\n    values: [0.001, 0.01, 0.1]\n  hyperparameters.batch_size:\n    values: [32, 64, 128]\n  model.name:\n    values: [simple_cnn, resnet]\n</code></pre> <p>Run sweep: <pre><code>wandb sweep sweep_config.yaml\nwandb agent &lt;sweep-id&gt;\n</code></pre></p>"},{"location":"mlops/experiment-tracking/#best-practices","title":"Best Practices","text":"<ol> <li>Use Descriptive Names: Give your experiments and runs meaningful names</li> <li>Tag Your Runs: Use tags to organize experiments (W&amp;B)</li> <li>Document Changes: Add notes about what you're testing (W&amp;B)</li> <li>Regular Cleanup: Archive old experiments you don't need</li> <li>Save Important Models: Use model registry for production models</li> <li>Track Everything: Log all hyperparameters and configuration</li> </ol>"},{"location":"mlops/experiment-tracking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mlops/experiment-tracking/#mlflow-connection-issues","title":"MLflow Connection Issues","text":"<pre><code># Check if MLflow server is running\nmlflow server --help\n\n# Test connection\nmlflow runs list --experiment-id 0\n</code></pre>"},{"location":"mlops/experiment-tracking/#wb-login-issues","title":"W&amp;B Login Issues","text":"<pre><code># Re-login\nwandb login --relogin\n\n# Check status\nwandb status\n</code></pre>"},{"location":"mlops/experiment-tracking/#disk-space-issues","title":"Disk Space Issues","text":"<p>MLflow and W&amp;B can use significant storage. To clean up:</p> <pre><code># MLflow (be careful!)\nrm -rf mlruns/\n\n# W&amp;B\nwandb artifact cache cleanup\n</code></pre>"},{"location":"mlops/experiment-tracking/#integration-with-code","title":"Integration with Code","text":"<p>The experiment tracking is integrated through the <code>ExperimentTracker</code> class:</p> <pre><code>from utils.experiment_tracking import ExperimentTracker\n\n# Initialize tracker\ntracker = ExperimentTracker(cfg, tracking_backend=\"mlflow\")\n\n# Log parameters\ntracker.log_params({\"learning_rate\": 0.001, \"batch_size\": 32})\n\n# Log metrics\ntracker.log_metrics({\"train_loss\": 0.5, \"val_acc\": 0.95}, step=epoch)\n\n# Log artifacts\ntracker.log_artifact(\"reports/confusion_matrix.png\")\n\n# Log model\ntracker.log_model(model, model_path=\"models/best.pth\")\n\n# Finish tracking\ntracker.finish()\n</code></pre>"},{"location":"mlops/experiment-tracking/#resources","title":"Resources","text":"<ul> <li>MLflow Documentation</li> <li>W&amp;B Documentation</li> <li>MLflow Model Registry</li> <li>W&amp;B Sweeps</li> </ul>"},{"location":"mlops/experiment-tracking/#next-steps","title":"Next Steps","text":"<ul> <li>Set up CI/CD pipeline with experiment tracking</li> <li>Implement automated hyperparameter tuning</li> <li>Create custom dashboards for model monitoring</li> <li>Integrate with model deployment pipelines</li> </ul>"},{"location":"mlops/model-versioning/","title":"Model Versioning","text":"<p>Manage and version trained models for reproducibility and deployment.</p>"},{"location":"mlops/model-versioning/#model-storage","title":"Model Storage","text":""},{"location":"mlops/model-versioning/#local-storage","title":"Local Storage","text":"<p>Models are saved to <code>models/</code> directory:</p> <pre><code>models/\n\u251c\u2500\u2500 simple_cnn_best.pth      # Best SimpleCNN checkpoint\n\u251c\u2500\u2500 resnet_best.pth          # Best ResNet checkpoint\n\u2514\u2500\u2500 archived/\n    \u251c\u2500\u2500 simple_cnn_v1.pth    # Previous version\n    \u2514\u2500\u2500 resnet_v1.pth\n</code></pre>"},{"location":"mlops/model-versioning/#model-checkpoint-format","title":"Model Checkpoint Format","text":"<p>Each checkpoint contains:</p> <pre><code>{\n    'epoch': int,                       # Training epoch\n    'model_state_dict': dict,          # Model weights\n    'optimizer_state_dict': dict,      # Optimizer state\n    'val_acc': float,                  # Validation accuracy\n    'config': dict                     # Training configuration\n}\n</code></pre>"},{"location":"mlops/model-versioning/#model-versioning-with-git","title":"Model Versioning with Git","text":""},{"location":"mlops/model-versioning/#tag-model-releases","title":"Tag Model Releases","text":"<pre><code># Create tag for model version\ngit tag model-v1.0 -m \"SimpleCNN on CIFAR-10: 85% accuracy\"\n\n# List tags\ngit tag -l\n\n# Push tags\ngit push origin --tags\n</code></pre>"},{"location":"mlops/model-versioning/#store-model-info","title":"Store Model Info","text":"<p>Create <code>MODELS.md</code> to track model versions:</p> <pre><code># Model Registry\n\n## SimpleCNN v1.0\n- Dataset: CIFAR-10\n- Accuracy: 85.2%\n- Parameters: 1.2M\n- Training Time: 2.5 hours\n- Config: learning_rate=0.001, epochs=50\n- Commit: abc123def456\n- Date: 2025-12-10\n\n## ResNet v1.0\n- Dataset: CIFAR-10\n- Accuracy: 91.5%\n- Parameters: 2.1M\n- Training Time: 4 hours\n- Config: learning_rate=0.0001, epochs=100\n</code></pre>"},{"location":"mlops/model-versioning/#version-control-with-dvc","title":"Version Control with DVC","text":""},{"location":"mlops/model-versioning/#track-model-files","title":"Track Model Files","text":"<pre><code># Add model to DVC\ndvc add models/simple_cnn_best.pth\n\n# Commit DVC metadata\ngit add models/simple_cnn_best.pth.dvc\ngit commit -m \"Add SimpleCNN v1.0 model\"\n</code></pre>"},{"location":"mlops/model-versioning/#push-to-remote","title":"Push to Remote","text":"<pre><code># Push model to remote storage\ndvc push models/simple_cnn_best.pth.dvc\n\n# Pull model from remote\ndvc pull models/simple_cnn_best.pth.dvc\n</code></pre>"},{"location":"mlops/model-versioning/#loading-models","title":"Loading Models","text":""},{"location":"mlops/model-versioning/#load-checkpoint","title":"Load Checkpoint","text":"<pre><code>import torch\nfrom src.models.model import get_model\n\n# Create model\nmodel = get_model(\n    model_name=\"simple_cnn\",\n    num_classes=10,\n    image_size=32\n)\n\n# Load checkpoint\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n# Get metadata\nepoch = checkpoint['epoch']\nval_acc = checkpoint['val_acc']\nconfig = checkpoint['config']\n\nprint(f\"Model trained at epoch {epoch}: {val_acc:.2%} accuracy\")\n</code></pre>"},{"location":"mlops/model-versioning/#load-optimizer-state-for-resuming-training","title":"Load Optimizer State (for resuming training)","text":"<pre><code>import torch\nfrom torch.optim import Adam\nfrom src.models.model import get_model\n\n# Create model and optimizer\nmodel = get_model(\"simple_cnn\", num_classes=10, image_size=32)\noptimizer = Adam(model.parameters())\n\n# Load checkpoint\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n# Resume training from epoch\nstart_epoch = checkpoint['epoch'] + 1\n</code></pre>"},{"location":"mlops/model-versioning/#model-comparison","title":"Model Comparison","text":""},{"location":"mlops/model-versioning/#compare-multiple-models","title":"Compare Multiple Models","text":"<pre><code>import torch\nfrom pathlib import Path\nfrom src.models.model import get_model\nimport json\n\ndef compare_models(models_dir=\"models\"):\n    results = []\n\n    for model_file in Path(models_dir).glob(\"*_best.pth\"):\n        checkpoint = torch.load(model_file)\n\n        results.append({\n            'model': model_file.stem,\n            'epoch': checkpoint['epoch'],\n            'val_acc': checkpoint['val_acc'],\n            'config': checkpoint['config']\n        })\n\n    # Sort by accuracy\n    results.sort(key=lambda x: x['val_acc'], reverse=True)\n\n    print(\"Model Comparison:\")\n    for result in results:\n        print(f\"{result['model']}: {result['val_acc']:.2%}\")\n\n    return results\n</code></pre>"},{"location":"mlops/model-versioning/#versioning-best-practices","title":"Versioning Best Practices","text":""},{"location":"mlops/model-versioning/#1-semantic-versioning","title":"1. Semantic Versioning","text":"<p>Follow MAJOR.MINOR.PATCH:</p> <ul> <li>MAJOR: Architecture changes or significant accuracy improvements</li> <li>MINOR: Hyperparameter tuning, dataset version updates</li> <li>PATCH: Bug fixes, minor adjustments</li> </ul> <pre><code># Version naming\nmodels/simple_cnn_v1.0.0.pth   # MAJOR change\nmodels/simple_cnn_v1.1.0.pth   # MINOR improvement\nmodels/simple_cnn_v1.0.1.pth   # PATCH fix\n</code></pre>"},{"location":"mlops/model-versioning/#2-metadata-documentation","title":"2. Metadata Documentation","text":"<p>Store with each model:</p> <pre><code># models/simple_cnn_v1.0.0_metadata.yaml\nname: SimpleCNN\nversion: 1.0.0\ndataset: CIFAR-10\ntask: image_classification\nmetrics:\n  accuracy: 0.852\n  precision: 0.851\n  recall: 0.852\n  f1_score: 0.851\narchitecture:\n  num_classes: 10\n  input_channels: 3\n  image_size: 32\n  total_params: 1200000\ntraining:\n  epochs: 50\n  learning_rate: 0.001\n  optimizer: adam\n  batch_size: 32\n  training_time_hours: 2.5\n  gpu: NVIDIA RTX 3080\ndate: 2025-12-10\ngit_commit: abc123def456\nauthor: Your Name\nnotes: \"Best model on validation set\"\n</code></pre>"},{"location":"mlops/model-versioning/#3-model-registry","title":"3. Model Registry","text":"<p>Maintain central registry:</p> <pre><code># models/registry.json\n{\n  \"models\": [\n    {\n      \"id\": \"simple_cnn_v1.0.0\",\n      \"name\": \"SimpleCNN v1.0.0\",\n      \"path\": \"models/simple_cnn_best.pth\",\n      \"accuracy\": 0.852,\n      \"created\": \"2025-12-10\",\n      \"status\": \"production\"\n    },\n    {\n      \"id\": \"resnet_v1.0.0\",\n      \"name\": \"ResNet v1.0.0\",\n      \"path\": \"models/resnet_best.pth\",\n      \"accuracy\": 0.915,\n      \"created\": \"2025-12-10\",\n      \"status\": \"production\"\n    }\n  ]\n}\n</code></pre>"},{"location":"mlops/model-versioning/#integration-with-mlflow-optional","title":"Integration with MLflow (Optional)","text":"<p>Track models with MLflow:</p> <pre><code>pip install mlflow\n</code></pre> <pre><code>import mlflow\nfrom src.models.model import get_model\n\nmlflow.start_run()\n\nmodel = get_model(\"simple_cnn\", num_classes=10, image_size=32)\n# ... training code ...\n\n# Log metrics\nmlflow.log_metrics({'val_acc': 0.852, 'val_loss': 0.450})\n\n# Log model\nmlflow.pytorch.log_model(model, \"models/simple_cnn\")\n\n# Log artifacts\nmlflow.log_artifact(\"models/simple_cnn_best.pth\")\n\nmlflow.end_run()\n</code></pre> <p>View models:</p> <pre><code>mlflow ui\n</code></pre>"},{"location":"mlops/model-versioning/#deployment","title":"Deployment","text":""},{"location":"mlops/model-versioning/#export-model-for-production","title":"Export Model for Production","text":"<pre><code>import torch\nfrom src.models.model import get_model\n\n# Load trained model\ncheckpoint = torch.load(\"models/simple_cnn_best.pth\")\nmodel = get_model(\"simple_cnn\", num_classes=10, image_size=32)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Export to TorchScript for faster inference\nscripted_model = torch.jit.script(model)\nscripted_model.save(\"models/simple_cnn_scripted.pt\")\n\n# Export to ONNX for cross-platform support\ndummy_input = torch.randn(1, 3, 32, 32)\ntorch.onnx.export(\n    model, dummy_input,\n    \"models/simple_cnn.onnx\",\n    input_names=['input'],\n    output_names=['output'],\n    opset_version=11\n)\n</code></pre>"},{"location":"mlops/model-versioning/#model-serving","title":"Model Serving","text":"<p>See Docker section for containerized serving.</p>"},{"location":"mlops/model-versioning/#cleanup-archival","title":"Cleanup &amp; Archival","text":""},{"location":"mlops/model-versioning/#archive-old-models","title":"Archive Old Models","text":"<pre><code># Create archive directory\nmkdir -p models/archive\nmv models/simple_cnn_old.pth models/archive/\n\n# Compress archives\ntar -czf models/archive_2025_12_01.tar.gz models/archive/\n</code></pre>"},{"location":"mlops/model-versioning/#keep-storage-clean","title":"Keep Storage Clean","text":"<pre><code># Remove intermediate checkpoints (keep only best)\nfind models/ -name \"*_epoch_*.pth\" -delete\n\n# Check disk usage\ndu -sh models/\n</code></pre>"},{"location":"mlops/model-versioning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mlops/model-versioning/#model-loading-error","title":"Model Loading Error","text":"<pre><code># Check file exists and is accessible\nfrom pathlib import Path\nmodel_path = Path(\"models/simple_cnn_best.pth\")\nassert model_path.exists(), \"Model file not found\"\n\n# Try loading with map_location for cross-device compatibility\ndevice = torch.device(\"cpu\")\ncheckpoint = torch.load(\n    model_path,\n    map_location=device\n)\n</code></pre>"},{"location":"mlops/model-versioning/#version-mismatch","title":"Version Mismatch","text":"<p>Ensure PyTorch versions match:</p> <pre><code># Check PyTorch version when saving\npython -c \"import torch; print(torch.__version__)\"\n\n# Specify version in requirements\ntorch==2.0.0\n</code></pre>"},{"location":"mlops/model-versioning/#references","title":"References","text":"<ul> <li>PyTorch Model Saving</li> <li>MLflow Model Registry</li> <li>ONNX Runtime</li> </ul>"},{"location":"mlops/tracking-cheatsheet/","title":"MLflow and Weights &amp; Biases Cheat Sheet","text":""},{"location":"mlops/tracking-cheatsheet/#mlflow-commands","title":"MLflow Commands","text":""},{"location":"mlops/tracking-cheatsheet/#start-mlflow-ui","title":"Start MLflow UI","text":"<pre><code>mlflow ui\nmlflow ui --port 5000\nmlflow ui --host 0.0.0.0  # Make accessible from network\n</code></pre>"},{"location":"mlops/tracking-cheatsheet/#remote-tracking-server","title":"Remote Tracking Server","text":"<pre><code># Start server\nmlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db\n\n# Connect from client\nexport MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre>"},{"location":"mlops/tracking-cheatsheet/#cli-commands","title":"CLI Commands","text":"<pre><code># List experiments\nmlflow experiments list\n\n# Search runs\nmlflow runs list --experiment-id 0\n\n# Delete experiment\nmlflow experiments delete --experiment-id 1\n\n# Restore experiment\nmlflow experiments restore --experiment-id 1\n</code></pre>"},{"location":"mlops/tracking-cheatsheet/#weights-biases-commands","title":"Weights &amp; Biases Commands","text":""},{"location":"mlops/tracking-cheatsheet/#login","title":"Login","text":"<pre><code>wandb login\nwandb login --relogin\nwandb login --host=http://your-server.com  # Self-hosted\n</code></pre>"},{"location":"mlops/tracking-cheatsheet/#offline-mode","title":"Offline Mode","text":"<pre><code># Set offline mode\nexport WANDB_MODE=offline\n\n# Or in Python\nwandb.init(mode=\"offline\")\n\n# Sync later\nwandb sync ./wandb/offline-run-*\nwandb sync --sync-all\n</code></pre>"},{"location":"mlops/tracking-cheatsheet/#project-management","title":"Project Management","text":"<pre><code># List runs\nwandb runs list your-project\n\n# Pull run data\nwandb pull your-run-id\n\n# Restore run\nwandb restore your-run-id\n</code></pre>"},{"location":"mlops/tracking-cheatsheet/#sweeps","title":"Sweeps","text":"<pre><code># Create sweep\nwandb sweep sweep_config.yaml\n\n# Run agent\nwandb agent your-sweep-id\n\n# Run multiple agents\nwandb agent your-sweep-id --count 10\n</code></pre>"},{"location":"mlops/tracking-cheatsheet/#python-api-examples","title":"Python API Examples","text":""},{"location":"mlops/tracking-cheatsheet/#mlflow","title":"MLflow","text":"<pre><code>import mlflow\n\n# Set tracking URI\nmlflow.set_tracking_uri(\"./mlruns\")\n\n# Create experiment\nmlflow.create_experiment(\"my_experiment\")\n\n# Start run\nwith mlflow.start_run():\n    # Log params\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_params({\"batch_size\": 32, \"epochs\": 10})\n\n    # Log metrics\n    mlflow.log_metric(\"accuracy\", 0.95)\n    mlflow.log_metric(\"loss\", 0.05, step=1)\n\n    # Log model\n    mlflow.pytorch.log_model(model, \"model\")\n\n    # Log artifacts\n    mlflow.log_artifact(\"plot.png\")\n    mlflow.log_artifacts(\"./reports\")\n\n# Load model\nmodel = mlflow.pytorch.load_model(\"runs:/run-id/model\")\nmodel = mlflow.pytorch.load_model(\"models:/model-name/production\")\n</code></pre>"},{"location":"mlops/tracking-cheatsheet/#weights-biases","title":"Weights &amp; Biases","text":"<pre><code>import wandb\n\n# Initialize run\nwandb.init(\n    project=\"my-project\",\n    name=\"my-run\",\n    config={\"learning_rate\": 0.01, \"batch_size\": 32}\n)\n\n# Log metrics\nwandb.log({\"accuracy\": 0.95, \"loss\": 0.05})\nwandb.log({\"accuracy\": 0.96, \"loss\": 0.04}, step=1)\n\n# Log images\nwandb.log({\"predictions\": wandb.Image(image_array)})\n\n# Log plots\nwandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(\n    y_true=y_true,\n    preds=preds,\n    class_names=class_names\n)})\n\n# Watch model\nwandb.watch(model, log=\"all\", log_freq=100)\n\n# Save files\nwandb.save(\"model.pth\")\nwandb.save(\"./reports/*\")\n\n# Finish run\nwandb.finish()\n</code></pre>"},{"location":"mlops/tracking-cheatsheet/#configuration-examples","title":"Configuration Examples","text":""},{"location":"mlops/tracking-cheatsheet/#train-with-different-backends","title":"Train with Different Backends","text":"<pre><code># MLflow\npython src/training/train.py tracking=mlflow\n\n# Weights &amp; Biases\npython src/training/train.py tracking=wandb\n\n# Both\npython src/training/train.py tracking=both\n\n# None\npython src/training/train.py tracking=null\n</code></pre>"},{"location":"mlops/tracking-cheatsheet/#override-tracking-settings","title":"Override Tracking Settings","text":"<pre><code># MLflow with custom experiment\npython src/training/train.py \\\n    tracking=mlflow \\\n    tracking.experiment_name=my_experiment \\\n    tracking.run_name=my_run\n\n# W&amp;B with custom project\npython src/training/train.py \\\n    tracking=wandb \\\n    tracking.project=my-project \\\n    tracking.entity=my-team\n</code></pre>"},{"location":"mlops/tracking-cheatsheet/#comparison","title":"Comparison","text":"Feature MLflow Weights &amp; Biases Setup Easy (local) Requires account UI Basic, functional Beautiful, interactive Hosting Self-hosted by default Cloud by default Storage Local files Cloud storage Collaboration Limited Excellent Model Registry Yes Yes Sweeps Basic Advanced System Monitoring Basic Excellent Free Tier Unlimited 100GB, unlimited runs Cost Free (self-hosted) Free tier + paid plans"},{"location":"mlops/tracking-cheatsheet/#best-practices","title":"Best Practices","text":""},{"location":"mlops/tracking-cheatsheet/#mlflow_1","title":"MLflow","text":"<ol> <li>Use consistent experiment names</li> <li>Tag runs with meaningful metadata</li> <li>Use Model Registry for production models</li> <li>Backup mlruns directory regularly</li> <li>Use remote tracking server for teams</li> </ol>"},{"location":"mlops/tracking-cheatsheet/#weights-biases_1","title":"Weights &amp; Biases","text":"<ol> <li>Use groups to organize related runs</li> <li>Add tags for easy filtering</li> <li>Write detailed run notes</li> <li>Use sweeps for hyperparameter tuning</li> <li>Create reports for sharing results</li> </ol>"},{"location":"mlops/tracking-cheatsheet/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mlops/tracking-cheatsheet/#mlflow_2","title":"MLflow","text":"<p>Cannot connect to tracking server <pre><code># Check if server is running\nps aux | grep mlflow\n\n# Check firewall\nsudo ufw status\n\n# Test connection\ncurl http://localhost:5000\n</code></pre></p> <p>Database locked error <pre><code># Stop all MLflow processes\npkill -f mlflow\n\n# If using SQLite, check file permissions\nls -la mlflow.db\n</code></pre></p>"},{"location":"mlops/tracking-cheatsheet/#weights-biases_2","title":"Weights &amp; Biases","text":"<p>Login issues <pre><code># Check API key\ncat ~/.netrc | grep wandb\n\n# Reset login\nrm ~/.netrc\nwandb login\n</code></pre></p> <p>Sync issues <pre><code># Check sync status\nwandb sync --show\n\n# Force sync\nwandb sync --sync-all --include-offline\n</code></pre></p>"},{"location":"mlops/tracking-cheatsheet/#resources","title":"Resources","text":""},{"location":"mlops/tracking-cheatsheet/#mlflow_3","title":"MLflow","text":"<ul> <li>Docs: https://mlflow.org/docs/latest/</li> <li>GitHub: https://github.com/mlflow/mlflow</li> <li>Tracking: https://mlflow.org/docs/latest/tracking.html</li> <li>Models: https://mlflow.org/docs/latest/models.html</li> </ul>"},{"location":"mlops/tracking-cheatsheet/#weights-biases_3","title":"Weights &amp; Biases","text":"<ul> <li>Docs: https://docs.wandb.ai/</li> <li>GitHub: https://github.com/wandb/wandb</li> <li>Quickstart: https://docs.wandb.ai/quickstart</li> <li>Sweeps: https://docs.wandb.ai/guides/sweeps</li> </ul>"}]}